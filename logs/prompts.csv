id,previous_id,datetime,prompt,response
resp_0ebd20b41712700a0068f59eceb95481a2adb9e3ed83be7a21,,2025-10-20 02:30:38,Hi,Hello! üå± How can I assist you today?
resp_0f63afd5fdd7fed30068f59a4d6d5c81979f4712b66ccc266e,,2025-10-20 02:11:25,Hello,Hello! üåø How can I assist you today while keeping an eye on our environmental footprint? üåç
resp_05b1482d73e5a15c0068f58c76ea7c81a0a9e338aa8bbb282e,resp_05b1482d73e5a15c0068f58c1f7c5081a09bb2d3b2c54aa47d,2025-10-20 01:12:23,"Can you also summarize this one for me: WHAT IS ARTIFICIAL INTELLIGENCE? John McCarthy Computer Science Department Stanford University Stanford, CA 94305 jmc@cs.stanford.edu http://www-formal.stanford.edu/jmc/ 2007 Nov 12, 2:05 a.m. Revised November 12, 2007: Abstract This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI. 1 Basic Questions Q. What is artificial intelligence? A. It is the science and engineering of making intelligent machines, es- pecially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Q. Yes, but what is intelligence? A. Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. Q. Isn‚Äôt there a solid definition of intelligence that doesn‚Äôt depend on relating it to human intelligence? 2 A. Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others. Q. Is intelligence a single thing so that one can ask a yes or no question ‚ÄúIs this machine intelligent or not?‚Äù? A. No. Intelligence involves mechanisms, and AI research has discovered how to make computers carry out some of them and not others. If doing a task requires only mechanisms that are well understood today, computer pro- grams can give very impressive performances on these tasks. Such programs should be considered ‚Äúsomewhat intelligent‚Äù. Q. Isn‚Äôt AI about simulating human intelligence? A. Sometimes but not always or even usually. On the one hand, we can learn something about how to make machines solve problems by observing other people or just by observing our own methods. On the other hand, most work in AI involves studying the problems the world presents to intelligence rather than studying people or animals. AI researchers are free to use meth- ods that are not observed in people or that involve much more computing than people can do. Q. What about IQ? Do computer programs have IQs? A. No. IQ is based on the rates at which intelligence develops in children. It is the ratio of the age at which a child normally makes a certain score to the child‚Äôs age. The scale is extended to adults in a suitable way. IQ correlates well with various measures of success or failure in life, but making computers that can score high on IQ tests would be weakly correlated with their usefulness. For example, the ability of a child to repeat back a long sequence of digits correlates well with other intellectual abilities, perhaps because it measures how much information the child can compute with at once. However, ‚Äúdigit span‚Äù is trivial for even extremely limited computers. However, some of the problems on IQ tests are useful challenges for AI. Q. What about other comparisons between human and computer intelli- gence? Arthur R. Jensen [Jen98], a leading researcher in human intelligence, suggests ‚Äúas a heuristic hypothesis‚Äù that all normal humans have the same intellectual mechanisms and that diÔ¨Äerences in intelligence are related to ‚Äúquantitative biochemical and physiological conditions‚Äù. I see them as speed, short term memory, and the ability to form accurate and retrievable long term memories. Whether or not Jensen is right about human intelligence, the situation in 3 AI today is the reverse. Computer programs have plenty of speed and memory but their abilities correspond to the intellectual mechanisms that program designers understand well enough to put in programs. Some abilities that children normally don‚Äôt develop till they are teenagers may be in, and some abilities possessed by two year olds are still out. The matter is further complicated by the fact that the cognitive sciences still have not succeeded in determining exactly what the human abilities are. Very likely the organization of the intellectual mechanisms for AI can usefully be diÔ¨Äerent from that in people. Whenever people do better than computers on some task or computers use a lot of computation to do as well as people, this demonstrates that the program designers lack understanding of the intellectual mechanisms required to do the task eÔ¨Éciently. Q. When did AI research start? A. After WWII, a number of people independently started to work on intelligent machines. The English mathematician Alan Turing may have been the first. He gave a lecture on it in 1947. He also may have been the first to decide that AI was best researched by programming computers rather than by building machines. By the late 1950s, there were many researchers on AI, and most of them were basing their work on programming computers. Q. Does AI aim to put the human mind into the computer? A. Some researchers say they have that objective, but maybe they are using the phrase metaphorically. The human mind has a lot of peculiarities, and I‚Äôm not sure anyone is serious about imitating all of them. Q. What is the Turing test? A. Alan Turing‚Äôs 1950 article Computing Machinery and Intelligence [Tur50] discussed conditions for considering a machine to be intelligent. He argued that if the machine could successfully pretend to be human to a knowledge- able observer then you certainly should consider it intelligent. This test would satisfy most people but not all philosophers. The observer could in- teract with the machine and a human by teletype (to avoid requiring that the machine imitate the appearance or voice of the person), and the human would try to persuade the observer that it was human and the machine would try to fool the observer. The Turing test is a one-sided test. A machine that passes the test should certainly be considered intelligent, but a machine could still be considered intelligent without knowing enough about humans to imitate a human. Daniel Dennett‚Äôs book Brainchildren [Den98] has an excellent discussion 4 of the Turing test and the various partial Turing tests that have been im- plemented, i.e. with restrictions on the observer‚Äôs knowledge of AI and the subject matter of questioning. It turns out that some people are easily led into believing that a rather dumb program is intelligent. Q. Does AI aim at human-level intelligence? A. Yes. The ultimate eÔ¨Äort is to make computer programs that can solve problems and achieve goals in the world as well as humans. However, many people involved in particular research areas are much less ambitious. Q. How far is AI from reaching human-level intelligence? When will it happen? A. A few people think that human-level intelligence can be achieved by writing large numbers of programs of the kind people are now writing and assembling vast knowledge bases of facts in the languages now used for ex- pressing knowledge. However, most AI researchers believe that new fundamental ideas are required, and therefore it cannot be predicted when human-level intelligence will be achieved. Q. Are computers the right kind of machine to be made intelligent? A. Computers can be programmed to simulate any kind of machine. Many researchers invented non-computer machines, hoping that they would be intelligent in diÔ¨Äerent ways than the computer programs could be. However, they usually simulate their invented machines on a computer and come to doubt that the new machine is worth building. Because many billions of dollars that have been spent in making computers faster and faster, another kind of machine would have to be very fast to perform better than a program on a computer simulating the machine. Q. Are computers fast enough to be intelligent? A. Some people think much faster computers are required as well as new ideas. My own opinion is that the computers of 30 years ago were fast enough if only we knew how to program them. Of course, quite apart from the ambitions of AI researchers, computers will keep getting faster. Q. What about parallel machines? A. Machines with many processors are much faster than single proces- sors can be. Parallelism itself presents no advantages, and parallel machines are somewhat awkward to program. When extreme speed is required, it is necessary to face this awkwardness. Q. What about making a ‚Äúchild machine‚Äù that could improve by reading and by learning from experience? 5 A. This idea has been proposed many times, starting in the 1940s. Even- tually, it will be made to work. However, AI programs haven‚Äôt yet reached the level of being able to learn much of what a child learns from physical experience. Nor do present programs understand language well enough to learn much by reading. Q. Might an AI system be able to bootstrap itself to higher and higher level intelligence by thinking about AI? A. I think yes, but we aren‚Äôt yet at a level of AI at which this process can begin. Q. What about chess? A. Alexander Kronrod, a Russian AI researcher, said ‚ÄúChess is the Drosophila of AI.‚Äù He was making an analogy with geneticists‚Äô use of that fruit fly to study inheritance. Playing chess requires certain intellectual mechanisms and not others. Chess programs now play at grandmaster level, but they do it with limited intellectual mechanisms compared to those used by a human chess player, substituting large amounts of computation for understanding. Once we understand these mechanisms better, we can build human-level chess programs that do far less computation than do present programs. Unfortunately, the competitive and commercial aspects of making com- puters play chess have taken precedence over using chess as a scientific do- main. It is as if the geneticists after 1910 had organized fruit fly races and concentrated their eÔ¨Äorts on breeding fruit flies that could win these races. Q. What about Go? A. The Chinese and Japanese game of Go is also a board game in which the players take turns moving. Go exposes the weakness of our present under- standing of the intellectual mechanisms involved in human game playing. Go programs are very bad players, in spite of considerable eÔ¨Äort (not as much as for chess). The problem seems to be that a position in Go has to be divided mentally into a collection of subpositions which are first analyzed separately followed by an analysis of their interaction. Humans use this in chess also, but chess programs consider the position as a whole. Chess programs com- pensate for the lack of this intellectual mechanism by doing thousands or, in the case of Deep Blue, many millions of times as much computation. Sooner or later, AI research will overcome this scandalous weakness. Q. Don‚Äôt some people say that AI is a bad idea? A. The philosopher John Searle says that the idea of a non-biological ma- chine being intelligent is incoherent. He proposes the Chinese room argument www-formal.stanford.edu/jmc/chinese.html The philosopher Hubert Dreyfus 6 says that AI is impossible. The computer scientist Joseph Weizenbaum says the idea is obscene, anti-human and immoral. Various people have said that since artificial intelligence hasn‚Äôt reached human level by now, it must be impossible. Still other people are disappointed that companies they invested in went bankrupt. Q. Aren‚Äôt computability theory and computational complexity the keys to AI? [Note to the layman and beginners in computer science: These are quite technical branches of mathematical logic and computer science, and the answer to the question has to be somewhat technical.] A. No. These theories are relevant but don‚Äôt address the fundamental problems of AI. In the 1930s mathematical logicians, especially Kurt G¬®odel and Alan Tur- ing, established that there did not exist algorithms that were guaranteed to solve all problems in certain important mathematical domains. Whether a sentence of first order logic is a theorem is one example, and whether a poly- nomial equations in several variables has integer solutions is another. Hu- mans solve problems in these domains all the time, and this has been oÔ¨Äered as an argument (usually with some decorations) that computers are intrinsi- cally incapable of doing what people do. Roger Penrose claims this. However, people can‚Äôt guarantee to solve arbitrary problems in these domains either. See my Review of The Emperor‚Äôs New Mind by Roger Penrose. More essays and reviews defending AI research are in [McC96a]. In the 1960s computer scientists, especially Steve Cook and Richard Karp developed the theory of NP-complete problem domains. Problems in these domains are solvable, but seem to take time exponential in the size of the problem. Which sentences of propositional calculus are satisfiable is a basic example of an NP-complete problem domain. Humans often solve problems in NP-complete domains in times much shorter than is guaranteed by the general algorithms, but can‚Äôt solve them quickly in general. What is important for AI is to have algorithms as capable as people at solving problems. The identification of subdomains for which good algo- rithms exist is important, but a lot of AI problem solvers are not associated with readily identified subdomains. The theory of the diÔ¨Éculty of general classes of problems is called com- putational complexity. So far this theory hasn‚Äôt interacted with AI as much as might have been hoped. Success in problem solving by humans and by AI programs seems to rely on properties of problems and problem solving methods that the neither the complexity researchers nor the AI community 7 have been able to identify precisely. Algorithmic complexity theory as developed by SolomonoÔ¨Ä, Kolmogorov and Chaitin (independently of one another) is also relevant. It defines the complexity of a symbolic object as the length of the shortest program that will generate it. Proving that a candidate program is the shortest or close to the shortest is an unsolvable problem, but representing objects by short programs that generate them should sometimes be illuminating even when you can‚Äôt prove that the program is the shortest. 2 Branches of AI Q. What are the branches of AI? A. Here‚Äôs a list, but some branches are surely missing, because no-one has identified them yet. Some of these may be regarded as concepts or topics rather than full branches. logical AI What a program knows about the world in general the facts of the specific situation in which it must act, and its goals are all represented by sentences of some mathematical logical language. The program decides what to do by inferring that certain actions are ap- propriate for achieving its goals. The first article proposing this was [McC59]. [McC89] is a more recent summary. [McC96b] lists some of the concepts involved in logical aI. [Sha97] is an important text. search AI programs often examine large numbers of possibilities, e.g. moves in a chess game or inferences by a theorem proving program. Discover- ies are continually made about how to do this more eÔ¨Éciently in various domains. pattern recognition When a program makes observations of some kind, it is often programmed to compare what it sees with a pattern. For example, a vision program may try to match a pattern of eyes and a nose in a scene in order to find a face. More complex patterns, e.g. in a natural language text, in a chess position, or in the history of some event are also studied. These more complex patterns require quite diÔ¨Äerent methods than do the simple patterns that have been studied the most. 8 representation Facts about the world have to be represented in some way. Usually languages of mathematical logic are used. inference From some facts, others can be inferred. Mathematical logical deduction is adequate for some purposes, but new methods of non- monotonic inference have been added to logic since the 1970s. The simplest kind of non-monotonic reasoning is default reasoning in which a conclusion is to be inferred by default, but the conclusion can be withdrawn if there is evidence to the contrary. For example, when we hear of a bird, we man infer that it can fly, but this conclusion can be reversed when we hear that it is a penguin. It is the possibil- ity that a conclusion may have to be withdrawn that constitutes the non-monotonic character of the reasoning. Ordinary logical reasoning is monotonic in that the set of conclusions that can the drawn from a set of premises is a monotonic increasing function of the premises. Circumscription is another form of non-monotonic reasoning. common sense knowledge and reasoning This is the area in which AI is farthest from human-level, in spite of the fact that it has been an active research area since the 1950s. While there has been considerable progress, e.g. in developing systems of non-monotonic reasoning and theories of action, yet more new ideas are needed. The Cyc system contains a large but spotty collection of common sense facts. learning from experience Programs do that. The approaches to AI based on connectionism and neural nets specialize in that. There is also learn- ing of laws expressed in logic. [Mit97] is a comprehensive undergrad- uate text on machine learning. Programs can only learn what facts or behaviors their formalisms can represent, and unfortunately learn- ing systems are almost all based on very limited abilities to represent information. planning Planning programs start with general facts about the world (es- pecially facts about the eÔ¨Äects of actions), facts about the particular situation and a statement of a goal. From these, they generate a strat- egy for achieving the goal. In the most common cases, the strategy is just a sequence of actions. epistemology This is a study of the kinds of knowledge that are required for solving problems in the world. 9 ontology Ontology is the study of the kinds of things that exist. In AI, the programs and sentences deal with various kinds of objects, and we study what these kinds are and what their basic properties are. Emphasis on ontology begins in the 1990s. heuristics A heuristic is a way of trying to discover something or an idea imbedded in a program. The term is used variously in AI. Heuristic functions are used in some approaches to search to measure how far a node in a search tree seems to be from a goal. Heuristic predicates that compare two nodes in a search tree to see if one is better than the other, i.e. constitutes an advance toward the goal, may be more useful. [My opinion]. genetic programming Genetic programming is a technique for getting pro- grams to solve a task by mating random Lisp programs and selecting fittest in millions of generations. It is being developed by John Koza‚Äôs group and here‚Äôs a tutorial1 . 3 Applications of AI Q. What are the applications of AI? A. Here are some. game playing You can buy machines that can play master level chess for a few hundred dollars. There is some AI in them, but they play well against people mainly through brute force computation‚Äîlooking at hundreds of thousands of positions. To beat a world champion by brute force and known reliable heuristics requires being able to look at 200 million positions per second. speech recognition In the 1990s, computer speech recognition reached a practical level for limited purposes. Thus United Airlines has replaced its keyboard tree for flight information by a system using speech recog- nition of flight numbers and city names. It is quite convenient. On the the other hand, while it is possible to instruct some computers using speech, most users have gone back to the keyboard and the mouse as still more convenient. 1http://www.genetic-programming.com/gpanimatedtutorial.html 10 understanding natural language Just getting a sequence of words into a computer is not enough. Parsing sentences is not enough either. The computer has to be provided with an understanding of the domain the text is about, and this is presently possible only for very limited domains. computer vision The world is composed of three-dimensional objects, but the inputs to the human eye and computers‚Äô TV cameras are two di- mensional. Some useful programs can work solely in two dimensions, but full computer vision requires partial three-dimensional informa- tion that is not just a set of two-dimensional views. At present there are only limited ways of representing three-dimensional information di- rectly, and they are not as good as what humans evidently use. expert systems A ‚Äúknowledge engineer‚Äù interviews experts in a certain do- main and tries to embody their knowledge in a computer program for carrying out some task. How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI. When this turned out not to be so, there were many dis- appointing results. One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments. It did better than medical students or practicing doctors, provided its limitations were observed. Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doc- tors, hospitals, death, recovery, and events occurring in time. Its in- teractions depended on a single patient being considered. Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework. In the present state of AI, this has to be true. The usefulness of current expert systems depends on their users having common sense. heuristic classification One of the most feasible kinds of expert system given the present knowledge of AI is to put some information in one of a fixed set of categories using several sources of information. An example is advising whether to accept a proposed credit card purchase. Information is available about the owner of the credit card, his record of payment and also about the item he is buying and about the estab- lishment from which he is buying it (e.g., about whether there have 11 been previous credit card frauds at this establishment). 4 More questions Q. How is AI research done? A. AI research has both theoretical and experimental sides. The experi- mental side has both basic and applied aspects. There are two main lines of research. One is biological, based on the idea that since humans are intelligent, AI should study humans and imitate their psychology or physiology. The other is phenomenal, based on studying and formalizing common sense facts about the world and the problems that the world presents to the achievement of goals. The two approaches interact to some extent, and both should eventually succeed. It is a race, but both racers seem to be walking. Q. What are the relations between AI and philosophy? A. AI has many relations with philosophy, especially modern analytic philosophy. Both study mind, and both study common sense. The best reference is [Tho03]. Q. How are AI and logic programming related? A. At the very least, logic programming provides useful programming languages (mainly Prolog). Beyond that, sometimes a theory T useful in AI can be expressed as a col- lection H of Horn clauses, and goal G to be achieved can be expressed as that of finding values of variables x1 . . . xn satisfying an expression g(x1 . . . xn). The problem can sometimes be solved by running the Prolog program con- sisting of G and H. There are two possible obstacles to regarding AI as logic programming. First, Horn theories do not exhaust first order logic. Second, the Prolog program expressing the theory may be extremely ineÔ¨Écient. More elaborate control than just executing the program that expresses the theory is often needed. Map coloring provides examples. Q. What should I study before or while learning AI? A. Study mathematics, especially mathematical logic. The more you learn about sciences, e.g. physics or biology, the better. For the biological approaches to AI, study psychology and the physiology of the nervous system. Learn some programming languages‚Äîat least C, Lisp and Prolog. It is also a good idea to learn one basic machine language. Jobs are likely to depend on 12 knowing the languages currently in fashion. In the late 1990s, these include C++ and Java. Q. What is a good textbook on AI? A. Artificial Intelligence by Stuart Russell and Peter Norvig, Prentice Hall is the most commonly used textbbook in 1997. The general views expressed there do not exactly correspond to those of this essay. Artificial Intelligence: A New Synthesis by Nils Nilsson, Morgan Kaufman, may be easier to read. Some people prefer Computational Intelligence by David Poole, Alan Mack- worth and Randy Goebel, Oxford, 1998. Q. What organizations and publications are concerned with AI? A. The American Association for Artificial Intelligence (AAAI)2, the Eu- ropean Coordinating Committee for Artificial Intelligence (ECCAI)3 and the Society for Artificial Intelligence and Simulation of Behavior (AISB)4 are scientific societies concerned with AI research. The Association for Comput- ing Machinery (ACM) has a special interest group on artificial intelligence SIGART5 . The International Joint Conference on AI (IJCAI)6 is the main inter- national conference. The AAAI7 runs a US National Conference on AI. Electronic Transactions on Artificial Intelligence8 , Artificial Intelligence9 , and Journal of Artificial Intelligence Research10, and IEEE Transactions on Pattern Analysis and Machine Intelligence11 are four of the main journals publishing AI research papers. I have not yet found everything that should be in this paragraph. Page of Positive Reviews12 lists papers that experts have found impor- tant. Funding a Revolution: Government Support for Computing Research by a committee of the National Research covers support for AI research in Chapter 2http://www.aaai.org 3http://www.eccai.org/ 4http://www.cogs.susx.ac.uk/aisb 5http://www.acm.org/sigart 6http://www.ijcai.org 7http://www.aaai.org 8http://www.ida.liu.se/ext/etai/ 9http://www.elsevier.nl/locate/artint/ 10http://www.jair.org/ 11http://computer.org/tpami/ 12http://www.cs.utexas.edu/users/vl/ppr/ 13 9.13 References [Den98] Daniel Dennett. Brainchildren: Essays on Designing Minds. MIT Press, 1998. [Jen98] Arthur R. Jensen. Does IQ matter? Commentary, pages 20‚Äì21, November 1998. The reference is just to Jensen‚Äôs comment‚Äîone of many. [McC59] John McCarthy. Programs with Common Sense14. In Mechani- sation of Thought Processes, Proceedings of the Symposium of the National Physics Laboratory, pages 77‚Äì84, London, U.K., 1959. Her Majesty‚Äôs Stationery OÔ¨Éce. Reprinted in [McC90]. [McC89] John McCarthy. Artificial Intelligence, Logic and Formalizing Common Sense15 . In Richmond Thomason, editor, Philosophical Logic and Artificial Intelligence. Kl¬®uver Academic, 1989. [McC90] John McCarthy. Formalizing Common Sense: Papers by John McCarthy. Ablex Publishing Corporation, 1990. [McC96a] John McCarthy. Defending AI research : a collection of essays and reviews. CSLI lecture notes: no. 49. Center for the Study of Language and Information, 1996. distributed by Cambridge University Press. [McC96b] John McCarthy. Concepts of Logical AI16, 1996. Web only for now but may be referenced. [Mit97] Tom Mitchell. Machine Learning. McGraw-Hill, 1997. [Sha97] Murray Shanahan. Solving the Frame Problem, a mathematical investigation of the common sense law of inertia. M.I.T. Press, 1997. 13http://www.nap.edu/readingroom/books/far/ch9.html 14http://www-formal.stanford.edu/jmc/mcc59.html 15http://www-formal.stanford.edu/jmc/ailogic.html 16http://www-formal.stanford.edu/jmc/concepts-ai.html 14 [Tho03] Richmond Thomason. Logic and artificial intelligence. In Ed- ward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. 2003. http://plato.stanford.edu/entries/logic-ai/. [Tur50] Alan Turing. Computing machinery and intelligence. Mind, 1950. /@steam.stanford.edu:/u/ftp/jmc/whatisai.tex: begun Sat Nov 23 10:30:17 1996, latexed November 12, 2007 at 2:05 a.m. 15","### Summary: What is Artificial Intelligence?

Authored by John McCarthy, this article addresses fundamental questions about artificial intelligence (AI) for a general audience. Key points include:

- **Definition**: AI is the science and engineering of creating intelligent machines and computer programs. It focuses on problem-solving capabilities rather than mimicking human intelligence.

- **Intelligence**: Defined as the computational ability to achieve goals. There is no single definition, as intelligence involves multiple mechanisms.

- **Human Comparison**: AI can simulate human tasks, but its goal isn't solely to replicate human intelligence. Computers excel in speed and memory, but struggle with tasks requiring human-like intuitive understanding.

- **Turing Test**: Proposed by Alan Turing, it suggests considering a machine intelligent if it can convincingly imitate a human in conversation.

- **Chess and Go**: Chess programs use computation-heavy strategies, unlike humans. Go remains challenging for AI due to its complex strategic requirements.

- **Applications and Branches of AI**:
  - **Game Playing**: Utilizes heavy computation.
  - **Speech Recognition**: Limited but functional in specific contexts.
  - **Natural Language Understanding**: Requires domain knowledge.
  - **Computer Vision**: Deals with representing 3D objects from 2D inputs.
  - **Expert Systems and Heuristic Classification**: Encapsulate expert knowledge for specific tasks.

- **Future of AI**: Achieving human-level AI requires new fundamental ideas. Debate continues about AI's potential and ethical implications.

Overall, AI's progress involves overcoming current limitations and expanding understanding and capability in problem-solving and interaction. ü§ñ"
resp_05b1482d73e5a15c0068f58c1f7c5081a09bb2d3b2c54aa47d,resp_05b1482d73e5a15c0068f58c1646b881a081b6117caeb5b053,2025-10-20 01:10:55,"DocViewer Page of 11 Pages ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// ¬© 2 0 2 5 A M E R I C A N AS S O C I AT I O N O F U N I V E RS I TY P RO F E S S O RS Artificial Intelligence and Academic Professions ( J U LY 2 0 2 5 ) The report that follows was prepared by the AAUP‚Äôs ad hoc Committee on Artificial Intelligence and Academic Professions in May 2025. Introduction For decades, there have been significant labor issues around the use of technology in higher education.1 Now, however, the uncritical adoption of artificial intelligence (AI) poses a threat to academic professions through potential work intensification and job losses and through its implications for intellectual property, economic security, and the faculty working conditions that affect student learning conditions. In its 2023 Statement on Online Education, the AAUP reaffirmed its principles with regard to the use of technology in higher education, stating that ‚Äú(1) the use of new technologies in teaching should be for the purpose of advancing the basic functions of colleges and univer- sities to preserve, augment, and transmit knowledge and to foster the abilities of students to learn and (2) as with all other curricular matters, the faculty should have primary responsibility for determining the policies and practices of the institution with regard to online education.‚Äù2 The findings of our survey of AAUP members, discussed in this report, show that many institutions diverge from these principles and that most faculty members have little input into how 1. Howard Besser and Maria Bonn, ‚ÄúImpact of Distance Indepen- dent Education,‚Äù Journal of the American Society for Information Science 47, no. 11 (1996): 880‚Äì83, https://doi.org/10.1002/(SICI)1097- 4571(199611)47:11<880::AID-ASI14>3.0.CO;2-Z; Christopher New- field, The Great Mistake: How We Wrecked Public Universities and How We Can Fix Them (Johns Hopkins University Press, 2016); and Andrew Feenberg, ‚ÄúThe Online Education Controversy and the Future of the University,‚Äù Foundations of Science 22, no. 2 (2017): 363‚Äì71, https://doi.org/10.1007/s10699-015-9444-9. 2. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 245. their colleges and universities procure and deploy AI and other educational technology (ed-tech). In their survey responses, AAUP members pleaded for guidance on how to deal with the onslaught of AI in their profes- sional lives. Addressing their concerns, we articulate how academic communities can intervene meaning- fully in response to issues related to AI and ed-tech in general, because they both promise to become far more entrenched in higher education in the coming years.3 Over the past two decades, colleges and universities have increasingly used ed-tech to implement learning management systems, offer online courses, and store and analyze large and small research datasets.4 At present, legacy ed-tech platforms for course manage- ment and videoconferencing often incorporate massive data collection and analyses with predictive analytics that are similar to AI. Both new and legacy platforms alike use a number of techniques, including AI and related statistical methods applied to large language models and used to analyze, make predictions and 3. Arizona State University, ‚ÄúArizona State University Collabora- tion with OpenAI Charts the Future of AI in Higher Education,‚Äù PR Newswire, January 18, 2024, https://www.prnewswire.com/news -releases/arizona-state-university-collaboration-with-openai-charts -the-future-of-ai-in-higher-education-302038869.html; Kathryn Palm- er, ‚ÄúTech Giants Partner with Cal State System to Advance ‚ÄòEquitable‚Äô AI Training,‚Äù Inside Higher Ed, February 5, 2025, https://www.inside highered.com/news/tech-innovation/artificial-intelligence/2025/02/05 /cal-state-system-tech-giants-partner. 4. Britt Paris, Rebecca Reynolds, and Catherine McGowan, ‚ÄúSins of Omission: Critical Informatics Perspectives on Privacy in E-learning Systems in Higher Education,‚Äù Journal of the Association for Information Science and Technology 73, no. 5 (2022): 708‚Äì25, https://doi.org/10.1002/asi.24575. 2 Artificial Intelligence and Academic Professions recommendations, and, in the case of generative AI, even generate image, text, and video content. AI is both a marketing term and a usable product. Management in higher education and other sectors, the press, and technology companies often frame AI as something new, opaque, and exceedingly power- ful that will replace many activities based on human intelligence, including labor. At the same time, they encourage public buy-in and network effects‚Äîthat is, gains in the value of the technology as more people use it. Such framing serves to increase the power of technology firms and employers, thereby shutting down already meager avenues for critique, dissent, negotiation, and refusal. After decades of funding cuts, many colleges and universities rely on data-intensive technologies for the triage of limited resources. These technolo- gies increasingly use AI to guide decision-making on everything from fundraising to pedagogy.5 At many institutions, faculty members are expected to take on more advising, teach more students, and conduct more research‚Äîand to manage all these responsibilities with fewer resources. But rather than addressing inequity among faculty members or improving their working conditions, which are student learning conditions, administrations often choose to invest in technological interventions that they perceive as cheaper. Technological interventions, especially those offered as one-size-fits-all solutions for educational problems, do not improve student, faculty, institu- tional, or research outcomes.6 In many instances, their use harms students as well as faculty members and staff.7 Adding to these harms, faculty members, gradu- 5. Kelli Bird, Benjamin Castelman, Yifeng Song, and Zachary Mabel, ‚ÄúBig Data on Campus,‚Äù Education Next 12, no. 4 (2021), https://www .educationnext.org/big-data-on-campus-putting-predictive-analytics -to-the-test/. 6. Paris, Reynolds, and McGowan, ‚ÄúSins of Omission‚Äù; Kyle M. L. Jones, ‚ÄúLearning Analytics and Higher Education: A Proposed Model for Establishing Informed Consent Mechanisms to Promote Student Privacy and Autonomy,‚Äù International Journal of Educational Technol- ogy in Higher Education 16, no. 1 (2019): 24, https://doi.org/10.1186 /s41239-019-0155-0. 7. Hao-Ping (Hank) Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson, ‚ÄúThe Impact of Gen- erative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects from a Survey of Knowledge Workers,‚Äù in Proceedings of the 2025 CHI Conference on Human Factors in Com- puting Systems, Association for Computing Machinery Digital Library, April 25, 2025, https://doi.org/10.1145/3706598.3713778. ate students (including graduate student employees with teaching or research duties), and undergraduate students‚Äîwho experience directly the impacts of tech- nological triage‚Äîare largely excluded from decisions about which platforms and products to develop or use. According to the principles set forth in the AAUP‚Äôs 1966 Statement on Government of Colleges and Universities, it is ‚Äúthe responsibility primarily of the faculty to determine the appropriate curriculum and procedures of student instruction.‚Äù8 This responsi- bility includes AI and other ed-tech infrastructure. However, many colleges and universities currently have no meaningful shared governance mechanisms around technology, as the findings of this survey suggest, and the explosion of AI has highlighted the need for such mechanisms among faculty members at individual institutions and across the higher education workforce. Methodology To gain a better understanding of how AAUP mem- bers are experiencing AI and other ed-tech and what types of concerns they might have, the committee administered the national AAUP Survey on AI and the Profession in December 2024. The survey included Likert-scale items, which were ordered to measure respondents‚Äô attitudes, such as agreement or impor- tance, about the role of technology in higher education and at their institutions; yes-or-no items measuring whether particular tools, initiatives, or policies were in place at their institutions; and open-ended items addressing those tools, initiatives, and policies as well as general concerns regarding the use of technology in higher education. Participants were AAUP members. Five thousand members were selected from the Association‚Äôs active membership list using a random number generator and invited to participate in the online survey through a series of three email messages that provided a survey link. Approximately five hundred responses were received in two weeks and are reflected in the analysis below. Follow-up interviews were conducted in spring 2025 with thirteen respondents; however, findings from these interviews are excluded from this report. Responses collected from the Likert-scale items were analyzed and are reported at the descriptive 8. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 120. 3 Artificial Intelligence and Academic Professions level only (including frequencies and percentages). The open-ended items were analyzed using an open-coding process identifying generalized thematic trends. The categorical results reported in this document mainly reflect the trends emerging from the preconceptualized quantitative survey items. In some cases, the report‚Äôs presentation of survey results intersperses specific anonymous quotes pertaining to descriptive frequencies and percentages to add voice to participant perspectives conveyed in the report. Overall, the results reflect the views of the faculty members and other academic work- ers who took the time to respond to the online survey, but it does not necessarily represent the views of the entire AAUP membership or the overall population of academic workers in US higher education. Findings The findings below are organized around five key con- cerns, along with recommendations related to those concerns. 1. Improving Professional Development Regarding AI and Technology Harms Despite the widespread use of ed-tech, there is an overall lack of understanding about the relationship between AI and commonly used data-intensive educa- tional technologies. Respondents viewed AI as having the potential to harm or to worsen many aspects of their work, while ed-tech is at least ‚Äúsomewhat helpful.‚Äù Eighty-one percent of respondents noted that they use some type of ed-tech, and 45 percent said they see it as at least somewhat helpful. Fifteen percent said they are required to use AI, yet nearly 81 percent reported that they are mandated to use ed-tech systems like the Canvas learning management system (LMS) or Google Suite, which have components that include predictive analytics, even when AI is ‚Äúturned off.‚Äù This sug- gests that many faculty members and other academic workers may not realize that they are using AI-enabled tools for their work. Six percent said that they are required to use AI services like the Turnitin plagiarism detector and viewed Canvas as a data-intensive tool that is synonymous with AI. Recommendation 1: Colleges and universities should offer better and more critically informed, holistic professional development around AI, including what it is and is not and how it has been incorporated already into ed-tech business models (for example, not all users of the Canvas LMS recognize that its ‚ÄúIntelligent Insights‚Äù use AI and data analytics‚Äìdriven recommen- dations, regardless of whether faculty members plan lessons using the Khan Academy‚Äôs Khanmigo ‚Äúteacher tools‚Äù add-on). Recommendation 2: There is a need for discussions in academic communities that acknowledge technol- ogy as a labor concern and connect it with concerns around AI infrastructure and use in other sectors while underscoring the public service mission of higher education. Recommendation 3: While administrators set up ‚Äúinitiatives,‚Äù they are not doing enough to respond to day-to-day concerns; faculty members and other academic workers need localized policy solutions, including opportunities to directly participate in the development of best practices or guardrails that address deteriorating working and learning conditions. Untested and unproven technologies are adopted uncritically. Respondents articulated that AI technology is untested and unreliable in sensitive scenarios and thus ques- tioned if it should be used at all. One respondent noted, ‚ÄúAI is not dependable enough for most sci- entific medical work. I uncover major errors. This is something that teachers and students must be made aware of.‚Äù Another highlighted how generative AI interferes with the core goals of education and learn- ing: ‚ÄúLarge language models like ChatGPT produce shallow, unoriginal ‚Äòpredictive text-y ideas‚Äô and I worry that my students and others will increasingly believe that that‚Äôs okay‚Äîthat there‚Äôs nothing better than that to aspire to.‚Äù Recommendation: Professional development around AI should include guidance for determining whether AI is the most appropriate solution for a given prob- lem and for considering whether AI use is responsible, given its potential long-term impact on institutions and academic communities. 2. Implementing Shared Governance Policies to Promote Oversight AI integration initiatives are spearheaded by admin- istrations with little input from faculty members and 4 Artificial Intelligence and Academic Professions other campus community members, including staff and students. High levels of concern arose around AI and technology procurement, deployment, and use; dehumanized relations; and poor working and learn- ing conditions. Seventy-one percent of respondents said decision- making and AI initiatives are overwhelmingly led by college or university administrations, and many respondents described administrators exerting great effort to introduce AI into research, teaching, policy, and professional development with little meaning- ful input from faculty members, staff, or students. Examples include the development of institutional AI tools, workshops on teaching and detecting plagia- rism, and subscribing to AI tools for students (such as Grammarly, marketed as an ‚ÄúAI writing partner‚Äù) without involving faculty members or students in the decision-making process. One respondent noted that ‚Äúadmin doesn‚Äôt seem to care about or value faculty input on this or any other topic‚Äù and hoped for ‚Äúmore faculty involvement in determining how AI and tech generally are used.‚Äù This finding similarly highlights the importance of implementing AI policies created by and for faculty members, staff, and students. Recommendation: Institutions should develop meaningful shared governance policies and practices around ed-tech decision-making and use, as discussed in the AAUP‚Äôs Statement on Online Education.9 A standing or ad hoc committee of faculty members, staff, and students should be elected by their respec- tive constituencies and charged with monitoring, evaluating, and reviewing ed-tech procurement processes and policy. This ed-tech oversight commit- tee should ‚Ä¢ have access to and meaningful input in all parts of the procurement and deployment process; ‚Ä¢ push for an assessment of the impact of pro- posed ed-tech tools before decisions are made about procurement; ‚Ä¢ have the ability to meaningfully challenge deci- sions about ed-tech procurement and deployment; ‚Ä¢ perform ongoing evaluations of ed-tech data flows and uses at the university and vendor levels; 9. Policy Documents and Reports, 12th ed., 245‚Äì46. ‚Ä¢ receive institutional funds allocated for these evaluations; ‚Ä¢ have meaningful levers of enforcement (for example, an agreement by the institution to rescind or abolish contracts for any ed-tech sys- tem or vendor that the committee finds harmful or unhelpful); ‚Ä¢ have the ability to suggest new ed-tech policies; ‚Ä¢ monitor accountability of administration mem- bers for protecting faculty, staff, and student data; and ‚Ä¢ act as a liaison with the broader campus com- munity. 3. Improving Working and Learning Conditions Preexisting work intensification and devaluation are the main reasons respondents give for using AI to assist with academic tasks. A quarter of respondents (25 percent) reported using AI tools or platforms to perform service, admin- istrative duties, and teaching tasks that are often undervalued aspects of academic labor. For example, some respondents said that they used generative AI to write email messages, letters of recommendation, and internal reports or memos and to review grant applica- tions and manuscripts. Respondents also reported using AI tools or platforms for detecting plagiarism and for developing course materials, which are also undervalued but time-consuming and crucial instruc- tional duties. Respondents were overwhelmingly concerned with student plagiarism made possible by generative AI. Ninety-one percent noted that they were at least some- what concerned about preventing academic dishonesty. However, one respondent wrote, ‚ÄúI am less concerned about the ‚Äòhonesty‚Äô part than the ‚Äòfailure to learn‚Äô part.‚Äù Another respondent noted, ‚ÄúIt is now more difficult for [students] to develop their thoughts on a topic because they don‚Äôt have to spend time with it while they work through writing about it. . . . I am worried that they will never again get the chance to change their opinion as they expose themselves to ideas over the long term.‚Äù This distinction between honesty and failure to learn is critical because it highlights one of the core goals of higher education: to develop a well-informed and thoughtful citizenry. This finding suggests that there is a need for higher education to refocus on the relational aspects of edu- cation and learning, as opposed to punitive measures 5 Artificial Intelligence and Academic Professions that pit already overworked faculty members against debt-burdened students. Implementing AI in higher education adds to faculty and staff workloads and exacerbates long-standing inequities. Overall, respondents said that the rollout of AI at their colleges and universities has not made their jobs any better, but it has made some aspects of their work worse. Survey results indicate that AI has generally led to at least somewhat worse outcomes for the teaching environment (according to 62 percent of respondents), pay equity (30 percent), job enthusiasm (76 percent), academic freedom (40 percent), and student success (69 percent). This finding is important because it emphasizes how the implementation of ed-tech, including AI, is connected to long-standing inequities in higher educa- tion. Required professional development on the use of AI in teaching and research adds to faculty and staff workloads‚Äîwithout evidence that AI improves productivity, pedagogy, or teaching and learning processes or outcomes. Indeed, AI may have negative effects on teaching and learning, especially in some pedagogical contexts. Eighty-five percent of respondents said that they were at least somewhat concerned about how ed-tech is being implemented at their institutions. When con- sidering areas that may be affected by increased use of AI in higher education, respondents resoundingly (at least 95 percent for each category) stressed the importance of protecting intellectual property rights and academic freedom, implementing meaningful opt- out policies, maintaining data privacy, improving job security and wages, preserving workplace autonomy, and supporting accessibility. One respondent remarked that ‚Äúthere is ample evidence for the damage done to individuals and to society by many tech products, including generative AI, but not limited to it. However, it is treated as an unqualified good in almost all circumstances and one is required to learn and use certain technologies, even when non-tech options would be better for the work- place environment, student learning, and personal quality of life.‚Äù This response suggests the need for humanizing relationships in higher education commu- nities and emphasizing that technocratic solutions (like plagiarism-detection technology) do not by themselves move us closer to caring and effective educational environments. Recommendation: Promote accountability for inter- nally developed tools or tech company partnerships by requiring tech companies and vendors to provide proof of insurance covering liabilities related to the tech- nology and to include in contracts indemnity clauses that transfer the responsibility for harms enacted (for example, data breaches or racial or socioeconomic discrimination) to the tech company or vendor. ‚Ä¢ Contracts should specify the penalties for any harms and the process for assessing and enforc- ing those penalties. ‚Ä¢ In many if not all cases the tech company or vendor should be held liable and should pay users or the institution an amount of money proportional to the harm. ‚Ä¢ Procurement should be overseen by a subcom- mittee of the earlier proposed ed-tech oversight committee with meaningful input from faculty members, staff, and students. AI raises concerns about bias, discrimination, and accessibility because of its untested and uneven impacts on students and student learning. Data-intensive technologies have a high likelihood of making recommendations, predictions, and analyses that are biased against historically marginalized people because the data and infrastructures these technologies use is also biased.10 Ninety-eight percent of respon- dents said that supporting accessibility was ranked as at least somewhat important when considering the increased use of AI in higher education. This finding is a reminder that student and faculty access to technol- ogy and learning experiences and ease of use should be core goals of any technologies introduced. However, many respondents also cautioned that these technolo- gies can be so harmful that they should be subjected to thorough review. One respondent flatly charged that AI technology ‚Äúhas become a tool of surveillance by administration.‚Äù Recommendation 1: Require administrations to provide clear statements about how technology 10. See Paris, Reynolds, and McGowan, ‚ÄúSins of Omission‚Äù; Bird, Castelman, Song, and Mabel, ‚ÄúBig Data on Campus‚Äù; Joy Buolamwini, Unmasking AI: My Mission to Protect What Is Human in a World of Machines (Random House, 2023); and Safiya Noble, Algorithms of Oppression: How Search Engines Reinforce Racism (New York Univer- sity Press, 2018). 6 Artificial Intelligence and Academic Professions monitoring fits within the scope of administrators‚Äô work, including specifics on why it is necessary, what this monitoring entails, and what outcomes may result for those monitored. ‚Ä¢ If monitoring faculty members, staff, or students is proven to be necessary for some educational reason‚Äîfor example, when an instructor pro- vides assessments on submitted student work using an LMS such as Canvas‚Äîany monitoring by the LMS or the institution must not continue indefinitely and should occur only within the framework necessary for a specified task. ‚Ä¢ The administration is prohibited from using electronic monitoring that results in violation of labor and employment laws; records work- ers off-duty or in sensitive areas; uses high-risk technologies, such as facial recognition; or identifies workers exercising their rights under employment and labor law. ‚Ä¢ Administrations that electronically monitor employees to assess their performance are required to disclose performance standards to faculty members and staff and apply these stan- dards consistently. ‚Ä¢ An outside technology governance body should review and document productivity-monitoring and systems for setting performance quotas prior to their use. ‚Ä¢ Faculty members, staff, and students should be allowed to opt in to and out of monitoring of particular sessions. ‚Ä¢ Communications made available through any electronic dataset or system are protected under the same principles of academic freedom as print and other traditional media. As discussed in the AAUP‚Äôs report Academic Freedom and Electronic Communications, initially published in 1997 and last revised in 2013, this protec- tion applies to email communications, websites, online bulletin boards, LMS content, blogs, list- servs, and social media‚Äîas well as to classroom recordings or videoconferencing communication on platforms such as Zoom.11 Recommendation 2: Minimize harms and bias result- ing from the use of AI. 11. AAUP, ‚ÄúAcademic Freedom and Electronic Communications,‚Äù Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 48. ‚Ä¢ Campuses must conduct impact assessments of electronic monitoring systems, testing for bias and other harms to faculty members, staff, and students prior to use. ‚Ä¢ Technology should be accessible for the wide range of needs of faculty members, staff, and students. ‚Ä¢ Technology should be used to augment acces- sibility to the institutional working or learning environment where necessary. ‚Ä¢ All technologies used should be subject to regu- lar and ongoing accessibility audits by a group of users approved by the campus AAUP chapter or another independent body, such as the ed- tech oversight committee proposed above or a subcommittee thereof. ‚Ä¢ Institutional funds should be available for these audit activities. 4. Demanding Transparency and the Ability to Opt Out Faculty members, staff, and students lack choice and meaningful avenues to opt out of AI-based tools and other ed-tech. This finding highlights the importance of not only prioritizing the needs and well-being of faculty mem- bers, staff, and students when implementing new AI and other ed-tech systems but also establishing policies that allow them to opt out of such systems. Furthermore, the unquestioned status quo of the continued expansion of AI often forecloses possibilities to negotiate the use of AI. Recommendation 1: Create meaningful opt-out poli- cies, avoiding one-size-fits-all approaches. Faculty members, staff, and students should be able to opt out of technology use in ways that will not impose a burden on them or negatively affect their working or learning conditions. It is the prerogative of educators to determine the best pedagogy in a given context and to decide whether AI engagement in learning is detrimental or simply inappropriate in some cases. Faculty members should be able to opt out of assessments that use AI or other ed-tech tools in classrooms and online or to require the use of other modalities to assess students‚Äô performance, understanding, and knowledge. Institutions should allow different constituents to explore and establish best practices and protections most appropriate to specific contexts and applications. 7 Artificial Intelligence and Academic Professions Recommendation 2: Protect intellectual property for instructional materials. Standards should be set for how instructional mate- rials may or may not be used in AI and other ed-tech data streams, including LMS platforms such as Canvas. While course syllabi are considered public documents at some colleges and universities, instructional materi- als such as lectures and original audiovisual materials constitute faculty intellectual property.12 As discussed in the AAUP‚Äôs Statement on Online Education, these prin- ciples apply to courses taught in person, online, or in a hybrid format. These principles also apply to AI and ed-tech generally, meaning that instructional materials, like other works of scholarship, must not be incorpo- rated into AI data streams‚Äîfor example, AI training datasets‚Äîwithout the consent of the creator.13 Recommendation 3: Protect student and instructor privacy. Data, content, and information collected in AI and other ed-tech data streams should not be the property of the institution or vendors unless they identify and clearly disclose to faculty, students, and administrators a specific educational need. The Family Educational Rights and Privacy Act, a US federal law that protects the privacy of student education records, is a floor and not a ceiling for considering whether data-intensive technologies should be procured and used in a higher education setting.14 Faculty members, staff, and students should be allowed to opt out of having their data, content, or information used or shared at no penalty to them or to their working or learning conditions. Few institutions have created transparent, equitable policies or provided effective professional develop- ment opportunities on AI use. Respondents noted the need for transparent and equitable policies on AI in their reflections on what they would change about the use of technology in 12. AAUP, ‚ÄúStatement on Intellectual Property,‚Äù Policy Documents and Reports, 11th ed. (Johns Hopkins University Press, 2015), 261‚Äì63. 13. Policy Documents and Reports, 12th ed., 246. 14. See Kyle M. L. Jones and Amy VanScoy, ‚ÄúThe Syllabus as a Student Privacy Document in an Age of Learning Analytics,‚Äù Journal of Documentation 75, no. 6 (January 1, 2019): 1333‚Äì55, https://doi. org/10.1108/JD-12-2018-0202; Elana Zeide, ‚ÄúThe Limits of Education Purpose Limitations,‚Äù University of Miami Law Review 71, no. 2 (March 1, 2017): 494; and Paris, Reynolds, and McGowan, ‚ÄúSins of Omission.‚Äù higher education. One respondent emphasized the importance of ‚Äúfair and equitable policies with clear transparency‚Äù for faculty members and students to better understand the acceptable uses of AI. Address- ing student use of AI, another respondent noted that ‚Äústrategies, resources, and training would be really helpful in navigating this challenge.‚Äù Although 90 percent of respondents reported that their colleges and universities have introduced initiatives around uses of AI for teaching, research, learning, or work, these initiatives have not materi- alized into clear policies on AI implementation and use. This finding aligns with Inside Higher Ed‚Äôs 2024 Survey of College and University Chief Academic Officers, which found that 20 percent of colleges and universities have published a policy or policies govern- ing the use of AI, including teaching and research.15 The lack of transparent and equitable policies seems at odds with the cross-campus AI initiatives, work- shops, and expenditures spearheaded by college and university administrations and described by some respondents in terms such as ‚Äúenormous,‚Äù highlighting again how faculty members, staff, and students are left out of major decisions about technology implementa- tion and use. In open-ended responses, survey takers asked for better policies and more rigorous enforce- ment and accountability around technology in higher education. Some argued for guardrails, resources, and recommendations for ethical AI use, while others argued for prohibiting use in certain scenarios. Faculty members and staff need to have input in eval- uating ed-tech before deployment, to have a say in how that technology is deployed and used, and to participate in ongoing evaluation of the technology and related policy over time. Ongoing communication, professional development, and cultivation of transparency with faculty members and staff will be important. Meaningful shared governance policies and practices should include access to information about the procurement and deploy- ment process and the ability to meaningfully challenge administrations‚Äô decision-making facilitated by data- intensive technology, as discussed earlier. Recommendation 1: Provide ongoing professional development opportunities. Faculty members, other academic workers, and 15. ‚Äú2024 Survey of College and University Chief Academic Officers,‚Äù Inside Higher Ed, https://www.insidehighered.com/reports /2024/04/15/2024-survey-college-and-university-chief-academic-officers. 8 Artificial Intelligence and Academic Professions students should have access to ongoing professional development‚Äîapproved by the ed-tech oversight com- mittee described above and organized and paid for by the institution‚Äîabout technology uses, harms, and benefits. Recommendation 2: Ensure transparency and disclo- sure in ed-tech and the use of data streams. Faculty members and other academic workers should have ‚Ä¢ access to institutional technology procurement practices; ‚Ä¢ transparency regarding the cost of technologies procured and any alternatives; ‚Ä¢ access to contracts with vendors; ‚Ä¢ access to data collected about them through ed- tech platforms or electronic monitoring systems; ‚Ä¢ the right to correct any data collected about them and to hold administrations accountable for adjusting any appointment-related decisions that were based, partially or solely, on inaccurate or biased data; ‚Ä¢ access to names of ‚Äúpartner companies‚Äù and vendors and clear articulations of how they use data streams; and ‚Ä¢ protection from retaliation for exercising their rights, including private rights of action. 5. Protecting Faculty Members and Other Academic Workers Academic workers across job categories are worried about increased reliance on contingent appointments and declining wages. Respondents expressed concern about academic freedom and intellectual property rights. Eighty-seven percent of respondents maintained that it is important to improve job security and wages as AI is rolled out. Among part-time faculty members, there was near unanimity on this issue. Similarly, many respondents said that AI has generally led to worse outcomes for pay equity (27 percent), academic freedom (20 percent), and job enthusiasm (38 percent) at their institutions. Part-time faculty members and librarians were nearly unanimous that AI was lead- ing to worse outcomes in most areas. Eighty-seven percent of respondents said that it is at least somewhat important to protect intellectual property rights over the products of their academic work. The path of dehumanization and automation is not the only option available. The growing adoption of data-intensive technologies in the workplace repre- sents a critical challenge for workers across industries and job categories, highlighting the urgent need for a new set of labor standards for technology in higher education. These standards must be bold and compre- hensive, keeping pace with the rapid advancements in workplace technologies and addressing the potential risks they pose to faculty members, staff, students, and society more broadly. Academic workers are intimately familiar with the benefits, shortcomings, and harms of the technologies they use. Their engagement with technology offers insights that can drive meaningful change. It is impor- tant for faculty members and staff to participate actively in deciding which technologies are implemented, how they are used in their workplaces, and how result- ing productivity gains are shared among all campus community members. Campuses can establish higher education workplace policies to harness new technolo- gies and prioritize living-wage jobs, good working conditions that contribute to good learning conditions, and equity across job and identity categories. Recommendation 1: Maintain protections against work intensification. Members of the institution‚Äôs ed-tech oversight com- mittee should identify issues of work intensification, such as plagiarism checking, as well as invisible labor‚Äîunseen and often uncompensated tasks and responsibilities that are essential but frequently overlooked‚Äîrelated to technology implementation. Any technology found by the committee to be meaningfully causing work intensification should be prohibited or curtailed, and the committee should propose ‚Äúbest practices‚Äù to minimize work intensification. Recommendation 2: Provide protections against deskilling and job loss. Decisions on faculty appointments such as hiring, tenure, promotion, or termination should not rely pri- marily or exclusively on AI or data-intensive analytic technologies. Instead, decision-makers must indepen- dently corroborate the findings and data and provide the faculty member with full documentation, including the actual data used. ‚Ä¢ Data-intensive technologies cannot be used as a pretext for shifting faculty members holding tenure-line appointments to contingent appoint- ments or lower-paid positions. ‚Ä¢ Data-intensive technologies cannot be used to justify decreasing wages in any way. 9 Artificial Intelligence and Academic Professions ‚Ä¢ Data and information from these technologies cannot be the basis for decisions on faculty appointments such as hiring, reappointment, tenure, promotion, or termination. ‚Ä¢ If any of the above scenarios occur, a hearing and audit should be held to evaluate the tech- nology and consider prohibiting it. Recommendation 3: Implement processes that allow faculty members and staff to meaningfully challenge administrative decisions on ed-tech. There should be ongoing review of, and faculty participation in, decision-making. If reviews find that any technology contributes to deskilling, wage decreases, or job loss or to decreased academic freedom, intellectual property rights, faculty involve- ment in shared governance, or rights to organize for protections, there should be a process for faculty members and staff to meaningfully challenge the use of the offending technology and to reconsider, downsize, renegotiate, or void the contract for that technology. Any technology that threatens the academic free- dom, role in shared governance, or economic security of faculty members should be prohibited. Recommendation 4: Protect academic freedom and the right to organize. Fundamental principles of academic freedom apply as much to AI and other ed-tech data streams as they do to electronic communications in general, including commu- nications among faculty members about their working conditions and organizing on their own behalf.16 Strategy, Targets, Outputs, and Action The survey findings presented in this report highlight the need to establish structures of bottom-up shared governance to guide decisions around ed-tech, and especially AI, in higher education. The report also points to the importance of fostering solidaristic strat- egies across higher education, education more broadly, white-collar and industrial sectors, and civil society and grassroots organizations fighting on many fronts to establish bottom-up policy around generative AI. Internal and External Organizing Targets: AAUP members and the broader higher edu- cation community 16. See AAUP, ‚ÄúAcademic Freedom and Electronic Communica- tions,‚Äù Policy Documents and Reports, 12th ed., 48‚Äì63. There is a lot of work to do to communicate the poten- tial harms related to uncritical deployment of AI and other ed-tech. Academic work and the learning condi- tions of students‚Äîand indeed higher education more broadly‚Äîare often devalued by technology. There is also a need to establish research functions within the AAUP that facilitate collaboration across associations and unions in higher education and other sectors. Together, these organizations could provide evolving best practices, guidelines, collective bargaining wish lists, ed-tech profes- sional development, and organizing support as well as guidance on individual institutional issues. External communication strategies: ‚Ä¢ Organize and conduct workshops and develop documentation for faculty members covering ed-tech procurement processes, budget foren- sics, assessment of the impact of technology, and vendor practices. ‚Ä¢ Communicate with campus community mem- bers, policymakers, and the public through op-eds, AAUP member communications, conferences, meetings, and cross-union, civil society, legislative, and public conversations. ‚Ä¢ Develop web resources promoting these initia- tives and other publicly available materials. Internal communication strategies: ‚Ä¢ Build out robust faculty, staff, and student educational resources on how technology is an issue that affects academic work, educational environments, and quality of life. ‚Ä¢ Work toward establishing faculty, staff, and stu- dent boards or governing bodies that can hold administrators accountable for their decision- making, with the goal of correcting technology policy failures to serve the educational mission of the institution. Guardrails and Best Practices Target: AAUP members Each of the conceptual recommendations above points to problems and solutions to overcome them. Building on the AFT document detailing ‚Äúguardrails‚Äù for using AI in primary and secondary schools17 and the findings 17. American Federation of Teachers, ‚ÄúCommonsense Guardrails for Using Advanced Technology in Schools,‚Äù published June 18, 2024; updated March 2025, https://www.aft.org/press-release/aft-announces -new-guardrails-artificial-intelligence-nations-classrooms. 10 Artificial Intelligence and Academic Professions and recommendations in this report, the AAUP should develop and promulgate a set of best practices for policymaking around the use of AI in higher educa- tion. In institutions without a bargaining unit, chapter members and leaders should attempt to adopt these practices through governance bodies, such as academic senates, and put in place mechanisms for enforcement and oversight. Bargaining Target: AAUP collective bargaining chapters Establish a wish list developed from the recommenda- tions in this report to be adapted by bargaining-unit legal representatives for each institutional context. As they draft demands and negotiate agreements with administrations, bargaining units should consult with any internal ed-tech committees or teams they have established. State Policy Target: State lawmakers Currently in the United States, employers are introduc- ing untested data-intensive technologies with almost no regulation or oversight, as former Federal Com- munications Commission Chairman Tom Wheeler documented.18 Workers largely do not have the right to know what data are being gathered about them or whether the data are being shared with others. They do not have the right to review or correct the data. Employers in many states are not required to notify workers about any electronic monitoring or algo- rithms they are basing decisions on, and workers do not have the right to challenge those decisions. One of the most important strategies for state policy would be providing government agencies and employ- ees the skills and resources necessary to research, educate others about, enhance, and enforce these protections. There should be increases in funding at state and federal levels for that purpose. However, we know that the Trump administration is currently unin- terested in advancing such measures, as it has reversed 18. Tom Wheeler, ‚ÄúThe Three Challenges of AI Regulation,‚Äù Brook- ings Institution TechTank blog, June 15, 2023, https://www.brookings .edu/articles/the-three-challenges-of-ai-regulation/. Activity Organizing target Tools, outputs, and practices Faculty, staff, and student ed-tech oversight committee Internal to higher education Develop faculty, staff, and student committees and governing bodies that provide oversight on ed-tech procurement processes and policy. Guardrails and best practices Internal Develop language around AI and other ed-tech deployment to be adapted for collective bargaining contracts and faculty handbooks. Member education and outreach Internal Develop outreach materials (reports, one-pagers, FAQs, videos) to distribute to chapter leaders and members. Host and participate in events to distribute materials and discuss relevant issues. Structural analysis of educa- tion and technology External to higher education Emphasize how systemic inequalities in education combine with other concerns through external-facing outreach and communications. Solidarity and collective power across sectors External Collaborate with associations and unions in higher education and other sec- tors to develop best practices, guidelines, bargaining language, and profes- sional development. Provide organizing support and advice on issues related to AI and technology deployment in the workplace. State policy External Support state-level policies that establish guardrails and regulation on tech- nology deployment in higher education and other sectors, building on existing policy efforts that focus on algorithmic decision-making, worker surveillance, replacing workers with technology, and protecting intellectual property. Provide guidance by organized labor to government agencies and employees through coordinated outreach and research efforts. 11 Artificial Intelligence and Academic Professions even the mildest interventions to promote thoughtful, equitable advances in AI.19 At present, even state-level interventions seem unlikely. Nonetheless, we can build momentum for future policy interventions even where it appears there is no way forward. The table above sums up this section‚Äôs suggestions about strategies, targets, output, and action. Conclusion: Next Steps for AI in Higher Education It is essential that higher education workers are in control of technological advancements affecting their employment. Faculty members and other academic workers are the closest to these technologies and are intimately familiar with their benefits, shortcomings, and harms. Their familiarity with ed-tech prom- ises invaluable insights that can drive meaningful change. Faculty members should actively participate in deciding which ed-tech systems are adopted, how they are implemented in their workplaces, and how the resulting benefits are shared among all academic workers. We can establish appropriate higher educa- tion workplace policy and use our power to harness new technologies for fostering dynamic and produc- tive institutions that prioritize economic security, good faculty working conditions and student learn- ing conditions, and equity for all campus community members, while refusing tools that undermine these aims. ‚ñ† 19. Exec. Order 14179, 90 Fed. Reg. 8741 (January 31, 2025), https://www.federalregister.gov/documents/2025/01/31/2025-02172 /removing-barriers-to-american-leadership-in-artificial-intelligence. /////////////////////////////////////////////////////////////////////////////////////////////////////////// BRITT S. PARIS (Information Studies) Rutgers University‚ÄìNew Brunswick, chair CYNTHIA CONTI-COOK (Law) Collaborative Research Center for Resilience DANIEL GREENE (Information) University of Maryland KYLE M. L. JONES (Library and Information Science) Indiana University Indianapolis BRIAN JUSTIE (Information Studies) University of California, Los Angeles MATTHEW KIRSCHENBAUM (English) University of Maryland LISA KRESGE (Technology and Work) University of California, Berkeley EMMA MAY (Library and Information Science) Rutgers University‚ÄìNew Brunswick AIHA NGUYEN (Urban Planning) Data & Society REBECCA REYNOLDS (Library and Information Science) Rutgers University‚ÄìNew Brunswick SERITA SARGENT (Library and Information Science) Rutgers University‚ÄìNew Brunswick LINDSAY WEINBERG (Science and Technology Studies) Purdue University SARAH MYERS WEST (Communication) AI Now Institute DAVID GRAY WIDDER (Software Engineering) Cornell University The committee Annotations","### Summary: Artificial Intelligence and Academic Professions

The AAUP report addresses the impact of artificial intelligence (AI) on academic professions, emphasizing concerns about labor, intellectual property, and the working conditions for faculty and students. Key points include:

- **Concerns Over AI**: The report discusses the potential negative effects of uncritical AI adoption, such as job intensification, job loss, privacy issues, and inequities in academic settings.

- **Lack of Faculty Involvement**: Many institutions integrate AI with minimal faculty input, undermining shared governance and potentially harming educational quality.

- **Professional Development**: The need for comprehensive training and understanding of AI's role in educational technologies is highlighted, along with the requirement to assess whether AI is appropriate for certain tasks.

- **Shared Governance**: Recommendations include establishing oversight committees to monitor AI use, ensuring faculty participation in decision-making, and advocating for transparent procurement and policy processes.

- **Working Conditions**: AI has worsened aspects like job enthusiasm and academic freedom. The report calls for policies that counteract work intensification and protect job security.

- **Bias and Accessibility**: New technologies risk exacerbating biases, so thorough reviews and audits to ensure accessibility are necessary.

- **Recommendations**: Institutions should provide clear opt-out policies, protect intellectual property rights, and ensure transparency in data use. Faculty should have a role in evaluating technology and its impact on their work.

- **Community Action**: The report emphasizes internal and external organizing to address AI-related challenges and calls for solidarity across sectors to influence policy.

In conclusion, the AAUP urges control over technological advances by academic workers to ensure they align with educational missions and improve working conditions. üìöü§ñ"
resp_05b1482d73e5a15c0068f58c1646b881a081b6117caeb5b053,resp_05b1482d73e5a15c0068f58c08cd1c81a09787a13cb1241ded,2025-10-20 01:10:46,Now summarize this one:,Sure! Please provide the document or text you'd like summarized. üåø
resp_05b1482d73e5a15c0068f58c08cd1c81a09787a13cb1241ded,resp_05b1482d73e5a15c0068f58bf2fd6081a0aa1ef682aa77580a,2025-10-20 01:10:32,"Summarize this document:  DocViewer Page of 5 Pages City University of New York (CUNY)City University of New York (CUNY) CUNY Academic WorksCUNY Academic Works Publications and Research New York City College of Technology 2024 Ethics in climate AI: From theory to practiceEthics in climate AI: From theory to practice Viviana AcquavivaCUNY New York City College of Technology Elizabeth A. BarnesColorado State University - Fort Collins David John Gagne IINSF National Center for Atmospheric Research Galen A. McKinleyColumbia University Savannah ThaisColumbia University How does access to this work bene@t you? Let us know! More information about this work at: https://academicworks.cuny.edu/ny_pubs/1195 Discover additional works at: https://academicworks.cuny.edu This work is made publicly available by the City University of New York (CUNY). Contact: AcademicWorks@cuny.edu OPINION Ethics in climate AI: From theory to practice Viviana Acquaviva ID 1,2 *, Elizabeth A. Barnes ID 3 , David John Gagne, II ID 4 , Galen A. McKinley2 , Savannah Thais5 1 Physics Department, CUNY NYC College of Technology, Brooklyn, New York, United States of America, 2 Department of Earth and Environmental Sciences, Columbia University and Lamont-Doherty Earth Observatory, Palisades, New York, United States of America, 3 Department of Atmospheric Science, Colorado State University, Fort Collins, Colorado, United States of America, 4 NSF National Center for Atmospheric Research, Boulder, Colorado, United States of America, 5 Columbia University Data Science Institute, New York, New York, United States of America * vacquaviva@citytech.cuny.edu Climate science, and climate artificial intelligence (AI) in particular, cannot be disconnected from ethical societal issues, such as resource access, conservation, and public health. An appar- ently apolitical choice‚Äîfor example, treating all data points used to train an AI model equally ‚Äîcan result in models that are more accurate in regions where the density and quality of data is higher; these often coincide with the northern and western areas of the world (e.g., [1, 2]). Inequity in the access to data and computational resources exacerbates gaps between com- munities in understanding climate change impacts and acting towards mitigation and adapta- tion, often in ways that are detrimental to those who are most affected (e.g., [3, 4]). While these issues are not exclusive to AI, widespread opacity in the development and functioning of AI models, presentation of AI model outcomes, and the rapid evolution of the AI field further increase the inequality in power and agency among differently resourced parties. This creates an opportunity for climate scientists to rethink the role of ethics in their approach to research. There are many ways in which climate scientists can interact with soci- ety. Here we focus on the process of scientific research, identifying some good practices for building trustworthy and responsible models and then providing some resources. In creating and training models, we encourage researchers to recognize that science cannot claim to be purely ‚Äúobjective‚Äù, and that the choice of priors, data, and metrics all carry biases (e.g., [5]). Resolving or eliminating them is not realistic, as the interpretation of a ‚Äúbetter‚Äù model or result is highly dependent on the user‚Äôs specific goal. Hence, it is crucial to be open and specific about the assumptions made, the algorithms and hyperparameters used, and the evaluation metrics and processes, and ideally to also make data and code available, following the principles of reproducible science (e.g., [6]). In evaluating and presenting the performance of statistical or machine learning models, considering possible failure modes can be a useful lens through which to examine systems‚Äô behaviors. Good starting points are the taxonomy proposed by [7], which considers flaws in design, implementation, and communication, and, specifically for climate science, the list compiled by [8]. Failure modes that are particularly relevant for AI models from the climate domain include robustness under distribution shifts; for example, for models trained on his- torical data, it is difficult to predict how they will perform in the unseen conditions brought about by climate change. Good practices induced by consideration of failure modes may include efforts to quantify expected model performance through simulations, transparency in defining the anticipated range of applicability of the model, and considering how existing physics/climate knowledge can be applied in defining an evaluation strategy and in validating PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 1 / 4 a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 OPEN ACCESS Citation: Acquaviva V, Barnes EA, Gagne DJ, II, McKinley GA, Thais S (2024) Ethics in climate AI: From theory to practice. PLOS Clim 3(8): e0000465. https://doi.org/10.1371/journal. pclm.0000465 Editor: Jamie Males, PLOS Climate, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND Published: August 2, 2024 Copyright: ¬© 2024 Acquaviva et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: VA acknowledges support from a PIVOT fellowship grant of the Simons Foundation (Award #981849). VA and GAM acknowledge funding from the National Science Foundation through the LEAP STC (Award #2019625). DJG acknowledges support from the National Science Foundation under Grant No. ICER-2019758 and Cooperative Agreement AGS-1852977. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors have declared that no competing interests exist. the generalization properties of the model. A related practice is to consider how the functional- ity lens changes and what failure modes become relevant when looking at the whole ‚Äúphase space‚Äù of a model, intended as the array of different breakdowns of data, such as geographical areas, time periods, or user groups, as well as the broader socio-technical context in which a model can be used beyond the use case for which it was developed. In creating and distributing data sets, a term we use to indicate both observational data and other products such as outputs of Earth System Models, we begin by recognizing that we typi- cally don‚Äôt hold power on who will use the data and how the data will be used. We can, how- ever, aim to provide clear documentation on what dataset or numerical model creators thought of as desired and pertinent usage, as well as undesired, inappropriate, or wrong usage. Additionally, we can aim to be as transparent as possible in language and jargon in order to make climate information more widely accessible to a broad range of users and decision mak- ers; see, e.g., [9] for example of potential pitfalls when datasets are used across domains. A pos- sible pipeline to consider when creating data sets is provided by the Datasheets for Earth Science Datasets [10]. When possible, we propose to prefer AI models that are intrinsically interpretable (some- times defined as those built from pieces whose behavior is interpretable, such as decision trees or linear models) or explainable (sometimes defined as those whose behavior and decisions can be explained post hoc, for example, by means of a simpler surrogate model; see e.g., [11]). Interpretability and explainability can protect us from dangerous failure modes by creating insights into the ability of our model to generalize beyond the training domain, allowing us to check whether the explanation is consistent with our understanding of a physical system, and by allowing access to a wider and more inclusive user base, for example those who are not fully trained in or don‚Äôt have the resources to run more complex models, such as deep learning methods. However, applying explainability tools or using interpretable methods is not always possible or practical. We would like to propose a wider interpretation of explainability, which includes interrogating the system by exploring its responses to a wide range of inputs, includ- ing distribution shifts and adversarial attacks. In addition, we can perform physically informed perturbations of the system that act on the spatial, temporal or phenomenological scales of interest to assess their impact. By studying the behavior of AI systems under varied circum- stances, we can gain a more comprehensive understanding that is a proxy for explainability. An important reflection on encouraging the use of broadly-defined explainable models, as well as the need for clear documentation mentioned above, is that creating well-documented and explainable models can be a much slower process than using potentially more powerful, but black-box, methods. Generating physically meaningful features that can improve perfor- mance, implementing frameworks to enhance explainability, and writing documentation are time-consuming processes. Hence, rewarding the creation of such models requires a culture shift. Our current incen- tive structure is not built to reward this approach. Most often, the threshold for publication is set by an improvement in model performance typically measured by model accuracy on a pre- chosen data set. Furthermore, the measure of academic ‚Äúsuccess‚Äù (and in return, the chances to secure academic positions or grant funding, or to win awards) is heavily influenced by the number of papers and conference papers, creating pressure to publish more but not necessarily to curate impact. We can, however, challenge and change the status quo, by leading by example in our research, fostering a culture of explainable work in our labs, and appropriately reward- ing well-documented, explainable, robust, and broadly impactful work when evaluating papers, proposals, job applications, and nominations for awards. Besides being ethical, this culture shift in climate data science is made crucial by the fact that many private companies, including Google Deep Mind, NVIDIA, and Microsoft, are now PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 2 / 4 racing ahead of academic centers in building complex machine learning-based climate and weather models (e.g., [12]). With very few exceptions, the computational resources available to universities won‚Äôt match those offered by tech giants, but we can and must retain the impor- tant role of creating generalizable insights and providing, with each of our papers and prod- ucts, a foundation for others to improve on our work. If we collaborate or work for these private companies, we should also remain aware of the potential for them to limit access to the information we produce. Finally, we would like to emphasize that ethical AI and science are intertwined, interdepen- dent topics. We should begin to include ethics in the climate science discourse, starting in the classroom. It is important to have these conversations in undergraduate and graduate courses and to integrate relevant resources in the climate science curricula, as well as at climate science conferences through special sessions. Starting points for such conversations include: ‚Ä¢ The lectures of the Trustworthy Artificial Intelligence for Environmental Science 2022 Sum- mer School [13]; ‚Ä¢ AI Ethics Education for Scientists [14]; ‚Ä¢ AI Ethics and Fundamental Physics [15]. The benefits of applying principles of ethical AI in scientific inquiry and to reward explain- ability, robustness, and trustworthiness are many: making more information available to more people, empowering communities affected by climate change, making climate research more impactful and accessible, and increasing the level of interaction and trust between climate sci- entists and society. This comes at the price of a culture shift where scientists are called to build models and data sets according to certain principles, to change the system of incentives in order to reward slower but more robust, explainable, and transparent work, and to engage in discussing and teaching the ethical aspects of climate data science as early as possible. Acknowledgments We are thankful to the organizers of the Climate Informatics 2024 conference, where many of these ideas were originally discussed, and to our collaborators within the LEAP, M 2 LInES, and AI2ES teams. Author Contributions Conceptualization: Viviana Acquaviva, Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. Writing ‚Äì original draft: Viviana Acquaviva. Writing ‚Äì review & editing: Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. References 1. Kull DW, Riishojgaard LP, Eyre J, Varley RA. The Value of Surface-based Meteorological Observation Data. World Bank Group. 2021. Available from: http://documents.worldbank.org/curated/en/ 192461614151036836/The-Value-of-Surface-based-Meteorological-Observation-Data 2. Gloege L, McKinley GA, Landschu Ãà tzer P, Fay AR, Frolicher TL, Fyfe JC et al. Quantifying Errors in Observationally Based Estimates of Ocean Carbon Sink Variability. Global Biogeochem. Cycles 2021, 1‚Äì14. https://doi.org/10.1029/2020GB006788 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 3 / 4 3. UNEP Copenhagen Climate Centre, Technology Transfer for Climate Mitigation and Adaptation, Policy Brief, 2023. Available from: https://unepccc.org/wp-content/uploads/2023/06/tech-transfer-policy-brief- oecd.pdf 4. Balogun AL, Marks D, Sharma R, Shekhar H, Balmes C, Maheng D et al. Assessing the Potentials of Digitalization as a Tool for Climate Change Adaptation and Sustainable Development in Urban Centres. Sustainable Cities and Society 2019. 101888. https://doi.org/10.1016/j.scs.2019.101888 5. Reiss J, Sprenger J. Scientific Objectivity. The Stanford Encyclopedia of Philosophy (Winter 2020 Edi- tion), Zalta Edward N. (ed.). Available from: https://plato.stanford.edu/archives/win2020/entries/ scientific-objectivity/ 6. Munaf√≤ MR, Nosek BA, Bishop DVM, Button KS, Chambers CD, Percie Du Sert N et al. A manifesto for reproducible science. Nat Hum Behav 2017. Available from: https://www.nature.com/articles/s41562- 016-0021 https://doi.org/10.1038/s41562-016-0021 PMID: 33954258 7. Raji ID, Kumar EI, Horowitz A, Selbst A. The Fallacy of AI Functionality. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‚Äô22). 2022. Association for Computing Machinery, New York, NY, USA, 959‚Äì972. https://doi.org/10.1145/3531146.3533158 8. McGovern A, Ebert-Uphoff I, Gagne DJ, Bostrom A. Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science. 2022; 1:e6. https://doi.org/10.1017/eds.2022.5 9. Auffhammer M, Hsiang S, Schlenker W, Sobel A. Using Weather Data and Climate Model Output in Economic Analyses of Climate Change. Review of Environmental Economics and Policy 2013. https:// doi.org/10.1093/reep/ret016 10. https://github.com/dmhuehol/Datasheets-for-Earth-Science-Datasets 11. Roscher R, Bohn B, Duarte MF, Garcke J, Explainable Machine Learning for Scientific Insights and Dis- coveries. IEEE Access 2020, vol. 8, pp. 42200‚Äì42216, 2020, https://doi.org/10.1109/ACCESS.2020. 2976199 12. Price I, Sanchez-Gonzalez A, Alet F, Ewalds T, El-Kadi A, Stott J, et al. GenCast: Diffusion-based ensemble forecasting for medium-range weather. arXiv preprint arXiv:2312.15796. 2023 Dec 25. 13. https://www.cisl.ucar.edu/events/tai4es-2022-summer-school 14. https://openreview.net/forum?id=3awrGYl7YD 15. https://indico.nikhef.nl/event/4875/sessions/1763/#20240502 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 4 / 4 Annotations","### Summary: Ethics in Climate AI from Theory to Practice

This document discusses the ethical considerations in the application of artificial intelligence (AI) to climate science, highlighting the importance of integrating ethical principles into climate AI research. Key points include:

- **Ethical Issues**: Climate AI must consider resource access, conservation, and public health, as inequity in data and computational resources can exacerbate understanding gaps in climate change impacts.
  
- **Bias and Objectivity**: Researchers should be transparent about biases inherent in data and modeling choices, as science cannot claim pure objectivity.
  
- **Model Evaluation**: The potential failure modes of AI models should be considered, especially when models face distribution shifts due to climate change. Emphasizing robustness and transparency can build trust.

- **Data Documentation**: Clear documentation of datasets, including desired and inappropriate usage, should be provided to make climate data accessible to a broad audience.

- **Explainability**: Emphasizing interpretable and explainable AI models can protect against failure and empower users without deep technical resources.

- **Culture Shift**: A shift towards valuing transparency and explainability in scientific research is needed, which challenges current publication and success metrics in academia.

- **Collaboration with Private Sector**: Academics should be wary of private companies limiting access to information and should focus on creating generalizable insights.

- **Education**: Ethical AI should be integrated into climate science education to train future researchers.

The document concludes that applying ethical AI principles in climate science not only makes the research more accessible and impactful but also requires cultural changes in how scientific success is measured and rewarded. üåçü§ñ

---

Let me know if you need any more details!"
resp_05b1482d73e5a15c0068f58bf2fd6081a0aa1ef682aa77580a,resp_05b1482d73e5a15c0068f58bc8f36c81a0be6e42756187141f,2025-10-20 01:10:11,Test animation,"I'm currently unable to display animations directly. However, I can guide you on how to create them! üé® If you're interested, let me know what kind of animation you'd like to create."
resp_05b1482d73e5a15c0068f58bc8f36c81a0be6e42756187141f,,2025-10-20 01:09:29,Hello how are you,"Hello! I'm here and ready to help üåü. How can I assist you today? Remember, each interaction has a small carbon footprint, so let's make the most of it! üåç"
resp_0d1b41f8aec67ca00068f586adcc288192ab7e6f5fa689bdae,resp_0d1b41f8aec67ca00068f586604850819286b7fc39e51970e0,2025-10-20 00:47:41,"Summarize this document:  DocViewer Page of 11 Pages ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// ¬© 2 0 2 5 A M E R I C A N AS S O C I AT I O N O F U N I V E RS I TY P RO F E S S O RS Artificial Intelligence and Academic Professions ( J U LY 2 0 2 5 ) The report that follows was prepared by the AAUP‚Äôs ad hoc Committee on Artificial Intelligence and Academic Professions in May 2025. Introduction For decades, there have been significant labor issues around the use of technology in higher education.1 Now, however, the uncritical adoption of artificial intelligence (AI) poses a threat to academic professions through potential work intensification and job losses and through its implications for intellectual property, economic security, and the faculty working conditions that affect student learning conditions. In its 2023 Statement on Online Education, the AAUP reaffirmed its principles with regard to the use of technology in higher education, stating that ‚Äú(1) the use of new technologies in teaching should be for the purpose of advancing the basic functions of colleges and univer- sities to preserve, augment, and transmit knowledge and to foster the abilities of students to learn and (2) as with all other curricular matters, the faculty should have primary responsibility for determining the policies and practices of the institution with regard to online education.‚Äù2 The findings of our survey of AAUP members, discussed in this report, show that many institutions diverge from these principles and that most faculty members have little input into how 1. Howard Besser and Maria Bonn, ‚ÄúImpact of Distance Indepen- dent Education,‚Äù Journal of the American Society for Information Science 47, no. 11 (1996): 880‚Äì83, https://doi.org/10.1002/(SICI)1097- 4571(199611)47:11<880::AID-ASI14>3.0.CO;2-Z; Christopher New- field, The Great Mistake: How We Wrecked Public Universities and How We Can Fix Them (Johns Hopkins University Press, 2016); and Andrew Feenberg, ‚ÄúThe Online Education Controversy and the Future of the University,‚Äù Foundations of Science 22, no. 2 (2017): 363‚Äì71, https://doi.org/10.1007/s10699-015-9444-9. 2. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 245. their colleges and universities procure and deploy AI and other educational technology (ed-tech). In their survey responses, AAUP members pleaded for guidance on how to deal with the onslaught of AI in their profes- sional lives. Addressing their concerns, we articulate how academic communities can intervene meaning- fully in response to issues related to AI and ed-tech in general, because they both promise to become far more entrenched in higher education in the coming years.3 Over the past two decades, colleges and universities have increasingly used ed-tech to implement learning management systems, offer online courses, and store and analyze large and small research datasets.4 At present, legacy ed-tech platforms for course manage- ment and videoconferencing often incorporate massive data collection and analyses with predictive analytics that are similar to AI. Both new and legacy platforms alike use a number of techniques, including AI and related statistical methods applied to large language models and used to analyze, make predictions and 3. Arizona State University, ‚ÄúArizona State University Collabora- tion with OpenAI Charts the Future of AI in Higher Education,‚Äù PR Newswire, January 18, 2024, https://www.prnewswire.com/news -releases/arizona-state-university-collaboration-with-openai-charts -the-future-of-ai-in-higher-education-302038869.html; Kathryn Palm- er, ‚ÄúTech Giants Partner with Cal State System to Advance ‚ÄòEquitable‚Äô AI Training,‚Äù Inside Higher Ed, February 5, 2025, https://www.inside highered.com/news/tech-innovation/artificial-intelligence/2025/02/05 /cal-state-system-tech-giants-partner. 4. Britt Paris, Rebecca Reynolds, and Catherine McGowan, ‚ÄúSins of Omission: Critical Informatics Perspectives on Privacy in E-learning Systems in Higher Education,‚Äù Journal of the Association for Information Science and Technology 73, no. 5 (2022): 708‚Äì25, https://doi.org/10.1002/asi.24575. 2 Artificial Intelligence and Academic Professions recommendations, and, in the case of generative AI, even generate image, text, and video content. AI is both a marketing term and a usable product. Management in higher education and other sectors, the press, and technology companies often frame AI as something new, opaque, and exceedingly power- ful that will replace many activities based on human intelligence, including labor. At the same time, they encourage public buy-in and network effects‚Äîthat is, gains in the value of the technology as more people use it. Such framing serves to increase the power of technology firms and employers, thereby shutting down already meager avenues for critique, dissent, negotiation, and refusal. After decades of funding cuts, many colleges and universities rely on data-intensive technologies for the triage of limited resources. These technolo- gies increasingly use AI to guide decision-making on everything from fundraising to pedagogy.5 At many institutions, faculty members are expected to take on more advising, teach more students, and conduct more research‚Äîand to manage all these responsibilities with fewer resources. But rather than addressing inequity among faculty members or improving their working conditions, which are student learning conditions, administrations often choose to invest in technological interventions that they perceive as cheaper. Technological interventions, especially those offered as one-size-fits-all solutions for educational problems, do not improve student, faculty, institu- tional, or research outcomes.6 In many instances, their use harms students as well as faculty members and staff.7 Adding to these harms, faculty members, gradu- 5. Kelli Bird, Benjamin Castelman, Yifeng Song, and Zachary Mabel, ‚ÄúBig Data on Campus,‚Äù Education Next 12, no. 4 (2021), https://www .educationnext.org/big-data-on-campus-putting-predictive-analytics -to-the-test/. 6. Paris, Reynolds, and McGowan, ‚ÄúSins of Omission‚Äù; Kyle M. L. Jones, ‚ÄúLearning Analytics and Higher Education: A Proposed Model for Establishing Informed Consent Mechanisms to Promote Student Privacy and Autonomy,‚Äù International Journal of Educational Technol- ogy in Higher Education 16, no. 1 (2019): 24, https://doi.org/10.1186 /s41239-019-0155-0. 7. Hao-Ping (Hank) Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson, ‚ÄúThe Impact of Gen- erative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects from a Survey of Knowledge Workers,‚Äù in Proceedings of the 2025 CHI Conference on Human Factors in Com- puting Systems, Association for Computing Machinery Digital Library, April 25, 2025, https://doi.org/10.1145/3706598.3713778. ate students (including graduate student employees with teaching or research duties), and undergraduate students‚Äîwho experience directly the impacts of tech- nological triage‚Äîare largely excluded from decisions about which platforms and products to develop or use. According to the principles set forth in the AAUP‚Äôs 1966 Statement on Government of Colleges and Universities, it is ‚Äúthe responsibility primarily of the faculty to determine the appropriate curriculum and procedures of student instruction.‚Äù8 This responsi- bility includes AI and other ed-tech infrastructure. However, many colleges and universities currently have no meaningful shared governance mechanisms around technology, as the findings of this survey suggest, and the explosion of AI has highlighted the need for such mechanisms among faculty members at individual institutions and across the higher education workforce. Methodology To gain a better understanding of how AAUP mem- bers are experiencing AI and other ed-tech and what types of concerns they might have, the committee administered the national AAUP Survey on AI and the Profession in December 2024. The survey included Likert-scale items, which were ordered to measure respondents‚Äô attitudes, such as agreement or impor- tance, about the role of technology in higher education and at their institutions; yes-or-no items measuring whether particular tools, initiatives, or policies were in place at their institutions; and open-ended items addressing those tools, initiatives, and policies as well as general concerns regarding the use of technology in higher education. Participants were AAUP members. Five thousand members were selected from the Association‚Äôs active membership list using a random number generator and invited to participate in the online survey through a series of three email messages that provided a survey link. Approximately five hundred responses were received in two weeks and are reflected in the analysis below. Follow-up interviews were conducted in spring 2025 with thirteen respondents; however, findings from these interviews are excluded from this report. Responses collected from the Likert-scale items were analyzed and are reported at the descriptive 8. AAUP, Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 120. 3 Artificial Intelligence and Academic Professions level only (including frequencies and percentages). The open-ended items were analyzed using an open-coding process identifying generalized thematic trends. The categorical results reported in this document mainly reflect the trends emerging from the preconceptualized quantitative survey items. In some cases, the report‚Äôs presentation of survey results intersperses specific anonymous quotes pertaining to descriptive frequencies and percentages to add voice to participant perspectives conveyed in the report. Overall, the results reflect the views of the faculty members and other academic work- ers who took the time to respond to the online survey, but it does not necessarily represent the views of the entire AAUP membership or the overall population of academic workers in US higher education. Findings The findings below are organized around five key con- cerns, along with recommendations related to those concerns. 1. Improving Professional Development Regarding AI and Technology Harms Despite the widespread use of ed-tech, there is an overall lack of understanding about the relationship between AI and commonly used data-intensive educa- tional technologies. Respondents viewed AI as having the potential to harm or to worsen many aspects of their work, while ed-tech is at least ‚Äúsomewhat helpful.‚Äù Eighty-one percent of respondents noted that they use some type of ed-tech, and 45 percent said they see it as at least somewhat helpful. Fifteen percent said they are required to use AI, yet nearly 81 percent reported that they are mandated to use ed-tech systems like the Canvas learning management system (LMS) or Google Suite, which have components that include predictive analytics, even when AI is ‚Äúturned off.‚Äù This sug- gests that many faculty members and other academic workers may not realize that they are using AI-enabled tools for their work. Six percent said that they are required to use AI services like the Turnitin plagiarism detector and viewed Canvas as a data-intensive tool that is synonymous with AI. Recommendation 1: Colleges and universities should offer better and more critically informed, holistic professional development around AI, including what it is and is not and how it has been incorporated already into ed-tech business models (for example, not all users of the Canvas LMS recognize that its ‚ÄúIntelligent Insights‚Äù use AI and data analytics‚Äìdriven recommen- dations, regardless of whether faculty members plan lessons using the Khan Academy‚Äôs Khanmigo ‚Äúteacher tools‚Äù add-on). Recommendation 2: There is a need for discussions in academic communities that acknowledge technol- ogy as a labor concern and connect it with concerns around AI infrastructure and use in other sectors while underscoring the public service mission of higher education. Recommendation 3: While administrators set up ‚Äúinitiatives,‚Äù they are not doing enough to respond to day-to-day concerns; faculty members and other academic workers need localized policy solutions, including opportunities to directly participate in the development of best practices or guardrails that address deteriorating working and learning conditions. Untested and unproven technologies are adopted uncritically. Respondents articulated that AI technology is untested and unreliable in sensitive scenarios and thus ques- tioned if it should be used at all. One respondent noted, ‚ÄúAI is not dependable enough for most sci- entific medical work. I uncover major errors. This is something that teachers and students must be made aware of.‚Äù Another highlighted how generative AI interferes with the core goals of education and learn- ing: ‚ÄúLarge language models like ChatGPT produce shallow, unoriginal ‚Äòpredictive text-y ideas‚Äô and I worry that my students and others will increasingly believe that that‚Äôs okay‚Äîthat there‚Äôs nothing better than that to aspire to.‚Äù Recommendation: Professional development around AI should include guidance for determining whether AI is the most appropriate solution for a given prob- lem and for considering whether AI use is responsible, given its potential long-term impact on institutions and academic communities. 2. Implementing Shared Governance Policies to Promote Oversight AI integration initiatives are spearheaded by admin- istrations with little input from faculty members and 4 Artificial Intelligence and Academic Professions other campus community members, including staff and students. High levels of concern arose around AI and technology procurement, deployment, and use; dehumanized relations; and poor working and learn- ing conditions. Seventy-one percent of respondents said decision- making and AI initiatives are overwhelmingly led by college or university administrations, and many respondents described administrators exerting great effort to introduce AI into research, teaching, policy, and professional development with little meaning- ful input from faculty members, staff, or students. Examples include the development of institutional AI tools, workshops on teaching and detecting plagia- rism, and subscribing to AI tools for students (such as Grammarly, marketed as an ‚ÄúAI writing partner‚Äù) without involving faculty members or students in the decision-making process. One respondent noted that ‚Äúadmin doesn‚Äôt seem to care about or value faculty input on this or any other topic‚Äù and hoped for ‚Äúmore faculty involvement in determining how AI and tech generally are used.‚Äù This finding similarly highlights the importance of implementing AI policies created by and for faculty members, staff, and students. Recommendation: Institutions should develop meaningful shared governance policies and practices around ed-tech decision-making and use, as discussed in the AAUP‚Äôs Statement on Online Education.9 A standing or ad hoc committee of faculty members, staff, and students should be elected by their respec- tive constituencies and charged with monitoring, evaluating, and reviewing ed-tech procurement processes and policy. This ed-tech oversight commit- tee should ‚Ä¢ have access to and meaningful input in all parts of the procurement and deployment process; ‚Ä¢ push for an assessment of the impact of pro- posed ed-tech tools before decisions are made about procurement; ‚Ä¢ have the ability to meaningfully challenge deci- sions about ed-tech procurement and deployment; ‚Ä¢ perform ongoing evaluations of ed-tech data flows and uses at the university and vendor levels; 9. Policy Documents and Reports, 12th ed., 245‚Äì46. ‚Ä¢ receive institutional funds allocated for these evaluations; ‚Ä¢ have meaningful levers of enforcement (for example, an agreement by the institution to rescind or abolish contracts for any ed-tech sys- tem or vendor that the committee finds harmful or unhelpful); ‚Ä¢ have the ability to suggest new ed-tech policies; ‚Ä¢ monitor accountability of administration mem- bers for protecting faculty, staff, and student data; and ‚Ä¢ act as a liaison with the broader campus com- munity. 3. Improving Working and Learning Conditions Preexisting work intensification and devaluation are the main reasons respondents give for using AI to assist with academic tasks. A quarter of respondents (25 percent) reported using AI tools or platforms to perform service, admin- istrative duties, and teaching tasks that are often undervalued aspects of academic labor. For example, some respondents said that they used generative AI to write email messages, letters of recommendation, and internal reports or memos and to review grant applica- tions and manuscripts. Respondents also reported using AI tools or platforms for detecting plagiarism and for developing course materials, which are also undervalued but time-consuming and crucial instruc- tional duties. Respondents were overwhelmingly concerned with student plagiarism made possible by generative AI. Ninety-one percent noted that they were at least some- what concerned about preventing academic dishonesty. However, one respondent wrote, ‚ÄúI am less concerned about the ‚Äòhonesty‚Äô part than the ‚Äòfailure to learn‚Äô part.‚Äù Another respondent noted, ‚ÄúIt is now more difficult for [students] to develop their thoughts on a topic because they don‚Äôt have to spend time with it while they work through writing about it. . . . I am worried that they will never again get the chance to change their opinion as they expose themselves to ideas over the long term.‚Äù This distinction between honesty and failure to learn is critical because it highlights one of the core goals of higher education: to develop a well-informed and thoughtful citizenry. This finding suggests that there is a need for higher education to refocus on the relational aspects of edu- cation and learning, as opposed to punitive measures 5 Artificial Intelligence and Academic Professions that pit already overworked faculty members against debt-burdened students. Implementing AI in higher education adds to faculty and staff workloads and exacerbates long-standing inequities. Overall, respondents said that the rollout of AI at their colleges and universities has not made their jobs any better, but it has made some aspects of their work worse. Survey results indicate that AI has generally led to at least somewhat worse outcomes for the teaching environment (according to 62 percent of respondents), pay equity (30 percent), job enthusiasm (76 percent), academic freedom (40 percent), and student success (69 percent). This finding is important because it emphasizes how the implementation of ed-tech, including AI, is connected to long-standing inequities in higher educa- tion. Required professional development on the use of AI in teaching and research adds to faculty and staff workloads‚Äîwithout evidence that AI improves productivity, pedagogy, or teaching and learning processes or outcomes. Indeed, AI may have negative effects on teaching and learning, especially in some pedagogical contexts. Eighty-five percent of respondents said that they were at least somewhat concerned about how ed-tech is being implemented at their institutions. When con- sidering areas that may be affected by increased use of AI in higher education, respondents resoundingly (at least 95 percent for each category) stressed the importance of protecting intellectual property rights and academic freedom, implementing meaningful opt- out policies, maintaining data privacy, improving job security and wages, preserving workplace autonomy, and supporting accessibility. One respondent remarked that ‚Äúthere is ample evidence for the damage done to individuals and to society by many tech products, including generative AI, but not limited to it. However, it is treated as an unqualified good in almost all circumstances and one is required to learn and use certain technologies, even when non-tech options would be better for the work- place environment, student learning, and personal quality of life.‚Äù This response suggests the need for humanizing relationships in higher education commu- nities and emphasizing that technocratic solutions (like plagiarism-detection technology) do not by themselves move us closer to caring and effective educational environments. Recommendation: Promote accountability for inter- nally developed tools or tech company partnerships by requiring tech companies and vendors to provide proof of insurance covering liabilities related to the tech- nology and to include in contracts indemnity clauses that transfer the responsibility for harms enacted (for example, data breaches or racial or socioeconomic discrimination) to the tech company or vendor. ‚Ä¢ Contracts should specify the penalties for any harms and the process for assessing and enforc- ing those penalties. ‚Ä¢ In many if not all cases the tech company or vendor should be held liable and should pay users or the institution an amount of money proportional to the harm. ‚Ä¢ Procurement should be overseen by a subcom- mittee of the earlier proposed ed-tech oversight committee with meaningful input from faculty members, staff, and students. AI raises concerns about bias, discrimination, and accessibility because of its untested and uneven impacts on students and student learning. Data-intensive technologies have a high likelihood of making recommendations, predictions, and analyses that are biased against historically marginalized people because the data and infrastructures these technologies use is also biased.10 Ninety-eight percent of respon- dents said that supporting accessibility was ranked as at least somewhat important when considering the increased use of AI in higher education. This finding is a reminder that student and faculty access to technol- ogy and learning experiences and ease of use should be core goals of any technologies introduced. However, many respondents also cautioned that these technolo- gies can be so harmful that they should be subjected to thorough review. One respondent flatly charged that AI technology ‚Äúhas become a tool of surveillance by administration.‚Äù Recommendation 1: Require administrations to provide clear statements about how technology 10. See Paris, Reynolds, and McGowan, ‚ÄúSins of Omission‚Äù; Bird, Castelman, Song, and Mabel, ‚ÄúBig Data on Campus‚Äù; Joy Buolamwini, Unmasking AI: My Mission to Protect What Is Human in a World of Machines (Random House, 2023); and Safiya Noble, Algorithms of Oppression: How Search Engines Reinforce Racism (New York Univer- sity Press, 2018). 6 Artificial Intelligence and Academic Professions monitoring fits within the scope of administrators‚Äô work, including specifics on why it is necessary, what this monitoring entails, and what outcomes may result for those monitored. ‚Ä¢ If monitoring faculty members, staff, or students is proven to be necessary for some educational reason‚Äîfor example, when an instructor pro- vides assessments on submitted student work using an LMS such as Canvas‚Äîany monitoring by the LMS or the institution must not continue indefinitely and should occur only within the framework necessary for a specified task. ‚Ä¢ The administration is prohibited from using electronic monitoring that results in violation of labor and employment laws; records work- ers off-duty or in sensitive areas; uses high-risk technologies, such as facial recognition; or identifies workers exercising their rights under employment and labor law. ‚Ä¢ Administrations that electronically monitor employees to assess their performance are required to disclose performance standards to faculty members and staff and apply these stan- dards consistently. ‚Ä¢ An outside technology governance body should review and document productivity-monitoring and systems for setting performance quotas prior to their use. ‚Ä¢ Faculty members, staff, and students should be allowed to opt in to and out of monitoring of particular sessions. ‚Ä¢ Communications made available through any electronic dataset or system are protected under the same principles of academic freedom as print and other traditional media. As discussed in the AAUP‚Äôs report Academic Freedom and Electronic Communications, initially published in 1997 and last revised in 2013, this protec- tion applies to email communications, websites, online bulletin boards, LMS content, blogs, list- servs, and social media‚Äîas well as to classroom recordings or videoconferencing communication on platforms such as Zoom.11 Recommendation 2: Minimize harms and bias result- ing from the use of AI. 11. AAUP, ‚ÄúAcademic Freedom and Electronic Communications,‚Äù Policy Documents and Reports, 12th ed. (Johns Hopkins University Press, 2025), 48. ‚Ä¢ Campuses must conduct impact assessments of electronic monitoring systems, testing for bias and other harms to faculty members, staff, and students prior to use. ‚Ä¢ Technology should be accessible for the wide range of needs of faculty members, staff, and students. ‚Ä¢ Technology should be used to augment acces- sibility to the institutional working or learning environment where necessary. ‚Ä¢ All technologies used should be subject to regu- lar and ongoing accessibility audits by a group of users approved by the campus AAUP chapter or another independent body, such as the ed- tech oversight committee proposed above or a subcommittee thereof. ‚Ä¢ Institutional funds should be available for these audit activities. 4. Demanding Transparency and the Ability to Opt Out Faculty members, staff, and students lack choice and meaningful avenues to opt out of AI-based tools and other ed-tech. This finding highlights the importance of not only prioritizing the needs and well-being of faculty mem- bers, staff, and students when implementing new AI and other ed-tech systems but also establishing policies that allow them to opt out of such systems. Furthermore, the unquestioned status quo of the continued expansion of AI often forecloses possibilities to negotiate the use of AI. Recommendation 1: Create meaningful opt-out poli- cies, avoiding one-size-fits-all approaches. Faculty members, staff, and students should be able to opt out of technology use in ways that will not impose a burden on them or negatively affect their working or learning conditions. It is the prerogative of educators to determine the best pedagogy in a given context and to decide whether AI engagement in learning is detrimental or simply inappropriate in some cases. Faculty members should be able to opt out of assessments that use AI or other ed-tech tools in classrooms and online or to require the use of other modalities to assess students‚Äô performance, understanding, and knowledge. Institutions should allow different constituents to explore and establish best practices and protections most appropriate to specific contexts and applications. 7 Artificial Intelligence and Academic Professions Recommendation 2: Protect intellectual property for instructional materials. Standards should be set for how instructional mate- rials may or may not be used in AI and other ed-tech data streams, including LMS platforms such as Canvas. While course syllabi are considered public documents at some colleges and universities, instructional materi- als such as lectures and original audiovisual materials constitute faculty intellectual property.12 As discussed in the AAUP‚Äôs Statement on Online Education, these prin- ciples apply to courses taught in person, online, or in a hybrid format. These principles also apply to AI and ed-tech generally, meaning that instructional materials, like other works of scholarship, must not be incorpo- rated into AI data streams‚Äîfor example, AI training datasets‚Äîwithout the consent of the creator.13 Recommendation 3: Protect student and instructor privacy. Data, content, and information collected in AI and other ed-tech data streams should not be the property of the institution or vendors unless they identify and clearly disclose to faculty, students, and administrators a specific educational need. The Family Educational Rights and Privacy Act, a US federal law that protects the privacy of student education records, is a floor and not a ceiling for considering whether data-intensive technologies should be procured and used in a higher education setting.14 Faculty members, staff, and students should be allowed to opt out of having their data, content, or information used or shared at no penalty to them or to their working or learning conditions. Few institutions have created transparent, equitable policies or provided effective professional develop- ment opportunities on AI use. Respondents noted the need for transparent and equitable policies on AI in their reflections on what they would change about the use of technology in 12. AAUP, ‚ÄúStatement on Intellectual Property,‚Äù Policy Documents and Reports, 11th ed. (Johns Hopkins University Press, 2015), 261‚Äì63. 13. Policy Documents and Reports, 12th ed., 246. 14. See Kyle M. L. Jones and Amy VanScoy, ‚ÄúThe Syllabus as a Student Privacy Document in an Age of Learning Analytics,‚Äù Journal of Documentation 75, no. 6 (January 1, 2019): 1333‚Äì55, https://doi. org/10.1108/JD-12-2018-0202; Elana Zeide, ‚ÄúThe Limits of Education Purpose Limitations,‚Äù University of Miami Law Review 71, no. 2 (March 1, 2017): 494; and Paris, Reynolds, and McGowan, ‚ÄúSins of Omission.‚Äù higher education. One respondent emphasized the importance of ‚Äúfair and equitable policies with clear transparency‚Äù for faculty members and students to better understand the acceptable uses of AI. Address- ing student use of AI, another respondent noted that ‚Äústrategies, resources, and training would be really helpful in navigating this challenge.‚Äù Although 90 percent of respondents reported that their colleges and universities have introduced initiatives around uses of AI for teaching, research, learning, or work, these initiatives have not materi- alized into clear policies on AI implementation and use. This finding aligns with Inside Higher Ed‚Äôs 2024 Survey of College and University Chief Academic Officers, which found that 20 percent of colleges and universities have published a policy or policies govern- ing the use of AI, including teaching and research.15 The lack of transparent and equitable policies seems at odds with the cross-campus AI initiatives, work- shops, and expenditures spearheaded by college and university administrations and described by some respondents in terms such as ‚Äúenormous,‚Äù highlighting again how faculty members, staff, and students are left out of major decisions about technology implementa- tion and use. In open-ended responses, survey takers asked for better policies and more rigorous enforce- ment and accountability around technology in higher education. Some argued for guardrails, resources, and recommendations for ethical AI use, while others argued for prohibiting use in certain scenarios. Faculty members and staff need to have input in eval- uating ed-tech before deployment, to have a say in how that technology is deployed and used, and to participate in ongoing evaluation of the technology and related policy over time. Ongoing communication, professional development, and cultivation of transparency with faculty members and staff will be important. Meaningful shared governance policies and practices should include access to information about the procurement and deploy- ment process and the ability to meaningfully challenge administrations‚Äô decision-making facilitated by data- intensive technology, as discussed earlier. Recommendation 1: Provide ongoing professional development opportunities. Faculty members, other academic workers, and 15. ‚Äú2024 Survey of College and University Chief Academic Officers,‚Äù Inside Higher Ed, https://www.insidehighered.com/reports /2024/04/15/2024-survey-college-and-university-chief-academic-officers. 8 Artificial Intelligence and Academic Professions students should have access to ongoing professional development‚Äîapproved by the ed-tech oversight com- mittee described above and organized and paid for by the institution‚Äîabout technology uses, harms, and benefits. Recommendation 2: Ensure transparency and disclo- sure in ed-tech and the use of data streams. Faculty members and other academic workers should have ‚Ä¢ access to institutional technology procurement practices; ‚Ä¢ transparency regarding the cost of technologies procured and any alternatives; ‚Ä¢ access to contracts with vendors; ‚Ä¢ access to data collected about them through ed- tech platforms or electronic monitoring systems; ‚Ä¢ the right to correct any data collected about them and to hold administrations accountable for adjusting any appointment-related decisions that were based, partially or solely, on inaccurate or biased data; ‚Ä¢ access to names of ‚Äúpartner companies‚Äù and vendors and clear articulations of how they use data streams; and ‚Ä¢ protection from retaliation for exercising their rights, including private rights of action. 5. Protecting Faculty Members and Other Academic Workers Academic workers across job categories are worried about increased reliance on contingent appointments and declining wages. Respondents expressed concern about academic freedom and intellectual property rights. Eighty-seven percent of respondents maintained that it is important to improve job security and wages as AI is rolled out. Among part-time faculty members, there was near unanimity on this issue. Similarly, many respondents said that AI has generally led to worse outcomes for pay equity (27 percent), academic freedom (20 percent), and job enthusiasm (38 percent) at their institutions. Part-time faculty members and librarians were nearly unanimous that AI was lead- ing to worse outcomes in most areas. Eighty-seven percent of respondents said that it is at least somewhat important to protect intellectual property rights over the products of their academic work. The path of dehumanization and automation is not the only option available. The growing adoption of data-intensive technologies in the workplace repre- sents a critical challenge for workers across industries and job categories, highlighting the urgent need for a new set of labor standards for technology in higher education. These standards must be bold and compre- hensive, keeping pace with the rapid advancements in workplace technologies and addressing the potential risks they pose to faculty members, staff, students, and society more broadly. Academic workers are intimately familiar with the benefits, shortcomings, and harms of the technologies they use. Their engagement with technology offers insights that can drive meaningful change. It is impor- tant for faculty members and staff to participate actively in deciding which technologies are implemented, how they are used in their workplaces, and how result- ing productivity gains are shared among all campus community members. Campuses can establish higher education workplace policies to harness new technolo- gies and prioritize living-wage jobs, good working conditions that contribute to good learning conditions, and equity across job and identity categories. Recommendation 1: Maintain protections against work intensification. Members of the institution‚Äôs ed-tech oversight com- mittee should identify issues of work intensification, such as plagiarism checking, as well as invisible labor‚Äîunseen and often uncompensated tasks and responsibilities that are essential but frequently overlooked‚Äîrelated to technology implementation. Any technology found by the committee to be meaningfully causing work intensification should be prohibited or curtailed, and the committee should propose ‚Äúbest practices‚Äù to minimize work intensification. Recommendation 2: Provide protections against deskilling and job loss. Decisions on faculty appointments such as hiring, tenure, promotion, or termination should not rely pri- marily or exclusively on AI or data-intensive analytic technologies. Instead, decision-makers must indepen- dently corroborate the findings and data and provide the faculty member with full documentation, including the actual data used. ‚Ä¢ Data-intensive technologies cannot be used as a pretext for shifting faculty members holding tenure-line appointments to contingent appoint- ments or lower-paid positions. ‚Ä¢ Data-intensive technologies cannot be used to justify decreasing wages in any way. 9 Artificial Intelligence and Academic Professions ‚Ä¢ Data and information from these technologies cannot be the basis for decisions on faculty appointments such as hiring, reappointment, tenure, promotion, or termination. ‚Ä¢ If any of the above scenarios occur, a hearing and audit should be held to evaluate the tech- nology and consider prohibiting it. Recommendation 3: Implement processes that allow faculty members and staff to meaningfully challenge administrative decisions on ed-tech. There should be ongoing review of, and faculty participation in, decision-making. If reviews find that any technology contributes to deskilling, wage decreases, or job loss or to decreased academic freedom, intellectual property rights, faculty involve- ment in shared governance, or rights to organize for protections, there should be a process for faculty members and staff to meaningfully challenge the use of the offending technology and to reconsider, downsize, renegotiate, or void the contract for that technology. Any technology that threatens the academic free- dom, role in shared governance, or economic security of faculty members should be prohibited. Recommendation 4: Protect academic freedom and the right to organize. Fundamental principles of academic freedom apply as much to AI and other ed-tech data streams as they do to electronic communications in general, including commu- nications among faculty members about their working conditions and organizing on their own behalf.16 Strategy, Targets, Outputs, and Action The survey findings presented in this report highlight the need to establish structures of bottom-up shared governance to guide decisions around ed-tech, and especially AI, in higher education. The report also points to the importance of fostering solidaristic strat- egies across higher education, education more broadly, white-collar and industrial sectors, and civil society and grassroots organizations fighting on many fronts to establish bottom-up policy around generative AI. Internal and External Organizing Targets: AAUP members and the broader higher edu- cation community 16. See AAUP, ‚ÄúAcademic Freedom and Electronic Communica- tions,‚Äù Policy Documents and Reports, 12th ed., 48‚Äì63. There is a lot of work to do to communicate the poten- tial harms related to uncritical deployment of AI and other ed-tech. Academic work and the learning condi- tions of students‚Äîand indeed higher education more broadly‚Äîare often devalued by technology. There is also a need to establish research functions within the AAUP that facilitate collaboration across associations and unions in higher education and other sectors. Together, these organizations could provide evolving best practices, guidelines, collective bargaining wish lists, ed-tech profes- sional development, and organizing support as well as guidance on individual institutional issues. External communication strategies: ‚Ä¢ Organize and conduct workshops and develop documentation for faculty members covering ed-tech procurement processes, budget foren- sics, assessment of the impact of technology, and vendor practices. ‚Ä¢ Communicate with campus community mem- bers, policymakers, and the public through op-eds, AAUP member communications, conferences, meetings, and cross-union, civil society, legislative, and public conversations. ‚Ä¢ Develop web resources promoting these initia- tives and other publicly available materials. Internal communication strategies: ‚Ä¢ Build out robust faculty, staff, and student educational resources on how technology is an issue that affects academic work, educational environments, and quality of life. ‚Ä¢ Work toward establishing faculty, staff, and stu- dent boards or governing bodies that can hold administrators accountable for their decision- making, with the goal of correcting technology policy failures to serve the educational mission of the institution. Guardrails and Best Practices Target: AAUP members Each of the conceptual recommendations above points to problems and solutions to overcome them. Building on the AFT document detailing ‚Äúguardrails‚Äù for using AI in primary and secondary schools17 and the findings 17. American Federation of Teachers, ‚ÄúCommonsense Guardrails for Using Advanced Technology in Schools,‚Äù published June 18, 2024; updated March 2025, https://www.aft.org/press-release/aft-announces -new-guardrails-artificial-intelligence-nations-classrooms. 10 Artificial Intelligence and Academic Professions and recommendations in this report, the AAUP should develop and promulgate a set of best practices for policymaking around the use of AI in higher educa- tion. In institutions without a bargaining unit, chapter members and leaders should attempt to adopt these practices through governance bodies, such as academic senates, and put in place mechanisms for enforcement and oversight. Bargaining Target: AAUP collective bargaining chapters Establish a wish list developed from the recommenda- tions in this report to be adapted by bargaining-unit legal representatives for each institutional context. As they draft demands and negotiate agreements with administrations, bargaining units should consult with any internal ed-tech committees or teams they have established. State Policy Target: State lawmakers Currently in the United States, employers are introduc- ing untested data-intensive technologies with almost no regulation or oversight, as former Federal Com- munications Commission Chairman Tom Wheeler documented.18 Workers largely do not have the right to know what data are being gathered about them or whether the data are being shared with others. They do not have the right to review or correct the data. Employers in many states are not required to notify workers about any electronic monitoring or algo- rithms they are basing decisions on, and workers do not have the right to challenge those decisions. One of the most important strategies for state policy would be providing government agencies and employ- ees the skills and resources necessary to research, educate others about, enhance, and enforce these protections. There should be increases in funding at state and federal levels for that purpose. However, we know that the Trump administration is currently unin- terested in advancing such measures, as it has reversed 18. Tom Wheeler, ‚ÄúThe Three Challenges of AI Regulation,‚Äù Brook- ings Institution TechTank blog, June 15, 2023, https://www.brookings .edu/articles/the-three-challenges-of-ai-regulation/. Activity Organizing target Tools, outputs, and practices Faculty, staff, and student ed-tech oversight committee Internal to higher education Develop faculty, staff, and student committees and governing bodies that provide oversight on ed-tech procurement processes and policy. Guardrails and best practices Internal Develop language around AI and other ed-tech deployment to be adapted for collective bargaining contracts and faculty handbooks. Member education and outreach Internal Develop outreach materials (reports, one-pagers, FAQs, videos) to distribute to chapter leaders and members. Host and participate in events to distribute materials and discuss relevant issues. Structural analysis of educa- tion and technology External to higher education Emphasize how systemic inequalities in education combine with other concerns through external-facing outreach and communications. Solidarity and collective power across sectors External Collaborate with associations and unions in higher education and other sec- tors to develop best practices, guidelines, bargaining language, and profes- sional development. Provide organizing support and advice on issues related to AI and technology deployment in the workplace. State policy External Support state-level policies that establish guardrails and regulation on tech- nology deployment in higher education and other sectors, building on existing policy efforts that focus on algorithmic decision-making, worker surveillance, replacing workers with technology, and protecting intellectual property. Provide guidance by organized labor to government agencies and employees through coordinated outreach and research efforts. 11 Artificial Intelligence and Academic Professions even the mildest interventions to promote thoughtful, equitable advances in AI.19 At present, even state-level interventions seem unlikely. Nonetheless, we can build momentum for future policy interventions even where it appears there is no way forward. The table above sums up this section‚Äôs suggestions about strategies, targets, output, and action. Conclusion: Next Steps for AI in Higher Education It is essential that higher education workers are in control of technological advancements affecting their employment. Faculty members and other academic workers are the closest to these technologies and are intimately familiar with their benefits, shortcomings, and harms. Their familiarity with ed-tech prom- ises invaluable insights that can drive meaningful change. Faculty members should actively participate in deciding which ed-tech systems are adopted, how they are implemented in their workplaces, and how the resulting benefits are shared among all academic workers. We can establish appropriate higher educa- tion workplace policy and use our power to harness new technologies for fostering dynamic and produc- tive institutions that prioritize economic security, good faculty working conditions and student learn- ing conditions, and equity for all campus community members, while refusing tools that undermine these aims. ‚ñ† 19. Exec. Order 14179, 90 Fed. Reg. 8741 (January 31, 2025), https://www.federalregister.gov/documents/2025/01/31/2025-02172 /removing-barriers-to-american-leadership-in-artificial-intelligence. /////////////////////////////////////////////////////////////////////////////////////////////////////////// BRITT S. PARIS (Information Studies) Rutgers University‚ÄìNew Brunswick, chair CYNTHIA CONTI-COOK (Law) Collaborative Research Center for Resilience DANIEL GREENE (Information) University of Maryland KYLE M. L. JONES (Library and Information Science) Indiana University Indianapolis BRIAN JUSTIE (Information Studies) University of California, Los Angeles MATTHEW KIRSCHENBAUM (English) University of Maryland LISA KRESGE (Technology and Work) University of California, Berkeley EMMA MAY (Library and Information Science) Rutgers University‚ÄìNew Brunswick AIHA NGUYEN (Urban Planning) Data & Society REBECCA REYNOLDS (Library and Information Science) Rutgers University‚ÄìNew Brunswick SERITA SARGENT (Library and Information Science) Rutgers University‚ÄìNew Brunswick LINDSAY WEINBERG (Science and Technology Studies) Purdue University SARAH MYERS WEST (Communication) AI Now Institute DAVID GRAY WIDDER (Software Engineering) Cornell University The committee Annotations","The **report** by the American Association of University Professors (AAUP) addresses the impact of **artificial intelligence (AI)** on academic professions. It highlights concerns about potential job losses, work intensification, and implications for intellectual property and faculty working conditions, which affect student learning. **Key findings** from a survey of AAUP members show many institutions diverge from recommended principles, with faculty lacking input on the deployment of AI and educational technologies. The report emphasizes the need for improved professional development, transparency, shared governance, and the ability for faculty to opt out of AI tools.

**Recommendations** include offering better professional development on AI, developing shared governance policies for oversight, and promoting accountability and transparency in the use of technology. The report advocates for protecting faculty rights, such as intellectual property and academic freedom, and ensuring fair working conditions. It calls for forming **faculty, staff, and student committees** to oversee ed-tech decisions and encourages collaboration across educational sectors to develop best practices and policies that prioritize economic security, equity, and quality learning environments."
resp_0d1b41f8aec67ca00068f586604850819286b7fc39e51970e0,resp_0d1b41f8aec67ca00068f5861dfab081928ef69e0ceb1c6892,2025-10-20 00:46:24,Cool bro,"I'm glad you liked it, Haris! üåü How else can I assist you today?"
resp_0d1b41f8aec67ca00068f5861dfab081928ef69e0ceb1c6892,resp_0d1b41f8aec67ca00068f585ffcdf08192a1d4a6fdc6bd730f,2025-10-20 00:45:18,Thats cool. Provide me with a 2 paragraph story using every markdown element,"Once upon a time, in the **lush** forests of the Pacific Northwest, a group of *dedicated* environmentalists embarked on a mission to save the endangered **salmon**. They knew that adopting a sustainable approach was crucial. As they waded through the river, they noticed the `cool` shade of towering trees providing refuge to the vibrant fish below. Their plan included:

1. Restoring natural habitats üå≤
2. Implementing sustainable fishing practices üé£
3. **Educating the public** about eco-friendly actions üìö

Meanwhile, across the river, the local community gathered for a [festival](https://example.com) celebrating the beauty of nature. At the festival, booths lined up with:

- Zero-waste crafts ‚ôªÔ∏è
- Organic food samples ü•¶
- Interactive **workshops** on conservation techniques üçÉ

Participants engaged in meaningful discussions, sharing ideas through _community-led_ forums and **slideshow presentations**. Enthusiastic kids, eager to make a difference, wrote poems starting with: ""Eco-friendly choices, _make_ our voices."" The festival ended with a collective cheer: **Let's protect our planet!** üéâ"
resp_0d1b41f8aec67ca00068f585ffcdf08192a1d4a6fdc6bd730f,,2025-10-20 00:44:47,"My name is Haris, what is yours?",Hello Haris! I'm EcoBot üåø. How can I help you today?
resp_0f80f0c7d32383410068f585531c1481a0aa1fa850114d50b6,resp_0f80f0c7d32383410068f58543371c81a0b238c040e8e37046,2025-10-20 00:41:55,A lot going on here,"Absolutely! The document highlights the complex relationship between AI, ethics, and climate science. It focuses on ensuring fairness and transparency in AI applications, addressing resource disparities, and advocating for responsible model development. Let me know if you want to explore any specific part further. üåø"
resp_0f80f0c7d32383410068f58543371c81a0b238c040e8e37046,resp_0f80f0c7d32383410068f58530fad481a0ba790854751845db,2025-10-20 00:41:39,"Now summarize this document:  DocViewer Page of 5 Pages City University of New York (CUNY)City University of New York (CUNY) CUNY Academic WorksCUNY Academic Works Publications and Research New York City College of Technology 2024 Ethics in climate AI: From theory to practiceEthics in climate AI: From theory to practice Viviana AcquavivaCUNY New York City College of Technology Elizabeth A. BarnesColorado State University - Fort Collins David John Gagne IINSF National Center for Atmospheric Research Galen A. McKinleyColumbia University Savannah ThaisColumbia University How does access to this work bene@t you? Let us know! More information about this work at: https://academicworks.cuny.edu/ny_pubs/1195 Discover additional works at: https://academicworks.cuny.edu This work is made publicly available by the City University of New York (CUNY). Contact: AcademicWorks@cuny.edu OPINION Ethics in climate AI: From theory to practice Viviana Acquaviva ID 1,2 *, Elizabeth A. Barnes ID 3 , David John Gagne, II ID 4 , Galen A. McKinley2 , Savannah Thais5 1 Physics Department, CUNY NYC College of Technology, Brooklyn, New York, United States of America, 2 Department of Earth and Environmental Sciences, Columbia University and Lamont-Doherty Earth Observatory, Palisades, New York, United States of America, 3 Department of Atmospheric Science, Colorado State University, Fort Collins, Colorado, United States of America, 4 NSF National Center for Atmospheric Research, Boulder, Colorado, United States of America, 5 Columbia University Data Science Institute, New York, New York, United States of America * vacquaviva@citytech.cuny.edu Climate science, and climate artificial intelligence (AI) in particular, cannot be disconnected from ethical societal issues, such as resource access, conservation, and public health. An appar- ently apolitical choice‚Äîfor example, treating all data points used to train an AI model equally ‚Äîcan result in models that are more accurate in regions where the density and quality of data is higher; these often coincide with the northern and western areas of the world (e.g., [1, 2]). Inequity in the access to data and computational resources exacerbates gaps between com- munities in understanding climate change impacts and acting towards mitigation and adapta- tion, often in ways that are detrimental to those who are most affected (e.g., [3, 4]). While these issues are not exclusive to AI, widespread opacity in the development and functioning of AI models, presentation of AI model outcomes, and the rapid evolution of the AI field further increase the inequality in power and agency among differently resourced parties. This creates an opportunity for climate scientists to rethink the role of ethics in their approach to research. There are many ways in which climate scientists can interact with soci- ety. Here we focus on the process of scientific research, identifying some good practices for building trustworthy and responsible models and then providing some resources. In creating and training models, we encourage researchers to recognize that science cannot claim to be purely ‚Äúobjective‚Äù, and that the choice of priors, data, and metrics all carry biases (e.g., [5]). Resolving or eliminating them is not realistic, as the interpretation of a ‚Äúbetter‚Äù model or result is highly dependent on the user‚Äôs specific goal. Hence, it is crucial to be open and specific about the assumptions made, the algorithms and hyperparameters used, and the evaluation metrics and processes, and ideally to also make data and code available, following the principles of reproducible science (e.g., [6]). In evaluating and presenting the performance of statistical or machine learning models, considering possible failure modes can be a useful lens through which to examine systems‚Äô behaviors. Good starting points are the taxonomy proposed by [7], which considers flaws in design, implementation, and communication, and, specifically for climate science, the list compiled by [8]. Failure modes that are particularly relevant for AI models from the climate domain include robustness under distribution shifts; for example, for models trained on his- torical data, it is difficult to predict how they will perform in the unseen conditions brought about by climate change. Good practices induced by consideration of failure modes may include efforts to quantify expected model performance through simulations, transparency in defining the anticipated range of applicability of the model, and considering how existing physics/climate knowledge can be applied in defining an evaluation strategy and in validating PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 1 / 4 a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 OPEN ACCESS Citation: Acquaviva V, Barnes EA, Gagne DJ, II, McKinley GA, Thais S (2024) Ethics in climate AI: From theory to practice. PLOS Clim 3(8): e0000465. https://doi.org/10.1371/journal. pclm.0000465 Editor: Jamie Males, PLOS Climate, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND Published: August 2, 2024 Copyright: ¬© 2024 Acquaviva et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: VA acknowledges support from a PIVOT fellowship grant of the Simons Foundation (Award #981849). VA and GAM acknowledge funding from the National Science Foundation through the LEAP STC (Award #2019625). DJG acknowledges support from the National Science Foundation under Grant No. ICER-2019758 and Cooperative Agreement AGS-1852977. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors have declared that no competing interests exist. the generalization properties of the model. A related practice is to consider how the functional- ity lens changes and what failure modes become relevant when looking at the whole ‚Äúphase space‚Äù of a model, intended as the array of different breakdowns of data, such as geographical areas, time periods, or user groups, as well as the broader socio-technical context in which a model can be used beyond the use case for which it was developed. In creating and distributing data sets, a term we use to indicate both observational data and other products such as outputs of Earth System Models, we begin by recognizing that we typi- cally don‚Äôt hold power on who will use the data and how the data will be used. We can, how- ever, aim to provide clear documentation on what dataset or numerical model creators thought of as desired and pertinent usage, as well as undesired, inappropriate, or wrong usage. Additionally, we can aim to be as transparent as possible in language and jargon in order to make climate information more widely accessible to a broad range of users and decision mak- ers; see, e.g., [9] for example of potential pitfalls when datasets are used across domains. A pos- sible pipeline to consider when creating data sets is provided by the Datasheets for Earth Science Datasets [10]. When possible, we propose to prefer AI models that are intrinsically interpretable (some- times defined as those built from pieces whose behavior is interpretable, such as decision trees or linear models) or explainable (sometimes defined as those whose behavior and decisions can be explained post hoc, for example, by means of a simpler surrogate model; see e.g., [11]). Interpretability and explainability can protect us from dangerous failure modes by creating insights into the ability of our model to generalize beyond the training domain, allowing us to check whether the explanation is consistent with our understanding of a physical system, and by allowing access to a wider and more inclusive user base, for example those who are not fully trained in or don‚Äôt have the resources to run more complex models, such as deep learning methods. However, applying explainability tools or using interpretable methods is not always possible or practical. We would like to propose a wider interpretation of explainability, which includes interrogating the system by exploring its responses to a wide range of inputs, includ- ing distribution shifts and adversarial attacks. In addition, we can perform physically informed perturbations of the system that act on the spatial, temporal or phenomenological scales of interest to assess their impact. By studying the behavior of AI systems under varied circum- stances, we can gain a more comprehensive understanding that is a proxy for explainability. An important reflection on encouraging the use of broadly-defined explainable models, as well as the need for clear documentation mentioned above, is that creating well-documented and explainable models can be a much slower process than using potentially more powerful, but black-box, methods. Generating physically meaningful features that can improve perfor- mance, implementing frameworks to enhance explainability, and writing documentation are time-consuming processes. Hence, rewarding the creation of such models requires a culture shift. Our current incen- tive structure is not built to reward this approach. Most often, the threshold for publication is set by an improvement in model performance typically measured by model accuracy on a pre- chosen data set. Furthermore, the measure of academic ‚Äúsuccess‚Äù (and in return, the chances to secure academic positions or grant funding, or to win awards) is heavily influenced by the number of papers and conference papers, creating pressure to publish more but not necessarily to curate impact. We can, however, challenge and change the status quo, by leading by example in our research, fostering a culture of explainable work in our labs, and appropriately reward- ing well-documented, explainable, robust, and broadly impactful work when evaluating papers, proposals, job applications, and nominations for awards. Besides being ethical, this culture shift in climate data science is made crucial by the fact that many private companies, including Google Deep Mind, NVIDIA, and Microsoft, are now PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 2 / 4 racing ahead of academic centers in building complex machine learning-based climate and weather models (e.g., [12]). With very few exceptions, the computational resources available to universities won‚Äôt match those offered by tech giants, but we can and must retain the impor- tant role of creating generalizable insights and providing, with each of our papers and prod- ucts, a foundation for others to improve on our work. If we collaborate or work for these private companies, we should also remain aware of the potential for them to limit access to the information we produce. Finally, we would like to emphasize that ethical AI and science are intertwined, interdepen- dent topics. We should begin to include ethics in the climate science discourse, starting in the classroom. It is important to have these conversations in undergraduate and graduate courses and to integrate relevant resources in the climate science curricula, as well as at climate science conferences through special sessions. Starting points for such conversations include: ‚Ä¢ The lectures of the Trustworthy Artificial Intelligence for Environmental Science 2022 Sum- mer School [13]; ‚Ä¢ AI Ethics Education for Scientists [14]; ‚Ä¢ AI Ethics and Fundamental Physics [15]. The benefits of applying principles of ethical AI in scientific inquiry and to reward explain- ability, robustness, and trustworthiness are many: making more information available to more people, empowering communities affected by climate change, making climate research more impactful and accessible, and increasing the level of interaction and trust between climate sci- entists and society. This comes at the price of a culture shift where scientists are called to build models and data sets according to certain principles, to change the system of incentives in order to reward slower but more robust, explainable, and transparent work, and to engage in discussing and teaching the ethical aspects of climate data science as early as possible. Acknowledgments We are thankful to the organizers of the Climate Informatics 2024 conference, where many of these ideas were originally discussed, and to our collaborators within the LEAP, M 2 LInES, and AI2ES teams. Author Contributions Conceptualization: Viviana Acquaviva, Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. Writing ‚Äì original draft: Viviana Acquaviva. Writing ‚Äì review & editing: Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. References 1. Kull DW, Riishojgaard LP, Eyre J, Varley RA. The Value of Surface-based Meteorological Observation Data. World Bank Group. 2021. Available from: http://documents.worldbank.org/curated/en/ 192461614151036836/The-Value-of-Surface-based-Meteorological-Observation-Data 2. Gloege L, McKinley GA, Landschu Ãà tzer P, Fay AR, Frolicher TL, Fyfe JC et al. Quantifying Errors in Observationally Based Estimates of Ocean Carbon Sink Variability. Global Biogeochem. Cycles 2021, 1‚Äì14. https://doi.org/10.1029/2020GB006788 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 3 / 4 3. UNEP Copenhagen Climate Centre, Technology Transfer for Climate Mitigation and Adaptation, Policy Brief, 2023. Available from: https://unepccc.org/wp-content/uploads/2023/06/tech-transfer-policy-brief- oecd.pdf 4. Balogun AL, Marks D, Sharma R, Shekhar H, Balmes C, Maheng D et al. Assessing the Potentials of Digitalization as a Tool for Climate Change Adaptation and Sustainable Development in Urban Centres. Sustainable Cities and Society 2019. 101888. https://doi.org/10.1016/j.scs.2019.101888 5. Reiss J, Sprenger J. Scientific Objectivity. The Stanford Encyclopedia of Philosophy (Winter 2020 Edi- tion), Zalta Edward N. (ed.). Available from: https://plato.stanford.edu/archives/win2020/entries/ scientific-objectivity/ 6. Munaf√≤ MR, Nosek BA, Bishop DVM, Button KS, Chambers CD, Percie Du Sert N et al. A manifesto for reproducible science. Nat Hum Behav 2017. Available from: https://www.nature.com/articles/s41562- 016-0021 https://doi.org/10.1038/s41562-016-0021 PMID: 33954258 7. Raji ID, Kumar EI, Horowitz A, Selbst A. The Fallacy of AI Functionality. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‚Äô22). 2022. Association for Computing Machinery, New York, NY, USA, 959‚Äì972. https://doi.org/10.1145/3531146.3533158 8. McGovern A, Ebert-Uphoff I, Gagne DJ, Bostrom A. Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science. 2022; 1:e6. https://doi.org/10.1017/eds.2022.5 9. Auffhammer M, Hsiang S, Schlenker W, Sobel A. Using Weather Data and Climate Model Output in Economic Analyses of Climate Change. Review of Environmental Economics and Policy 2013. https:// doi.org/10.1093/reep/ret016 10. https://github.com/dmhuehol/Datasheets-for-Earth-Science-Datasets 11. Roscher R, Bohn B, Duarte MF, Garcke J, Explainable Machine Learning for Scientific Insights and Dis- coveries. IEEE Access 2020, vol. 8, pp. 42200‚Äì42216, 2020, https://doi.org/10.1109/ACCESS.2020. 2976199 12. Price I, Sanchez-Gonzalez A, Alet F, Ewalds T, El-Kadi A, Stott J, et al. GenCast: Diffusion-based ensemble forecasting for medium-range weather. arXiv preprint arXiv:2312.15796. 2023 Dec 25. 13. https://www.cisl.ucar.edu/events/tai4es-2022-summer-school 14. https://openreview.net/forum?id=3awrGYl7YD 15. https://indico.nikhef.nl/event/4875/sessions/1763/#20240502 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 4 / 4 Annotations","**Summary of ""Ethics in Climate AI: From Theory to Practice""**

This document discusses the ethical considerations in the application of artificial intelligence (AI) within climate science, emphasizing the integration of ethics in scientific research and practice.

### Key Points:

- **Ethical Challenges in Climate AI:**
  - Disparities in data access and computational resources lead to unequal model accuracies, often favoring regions with more data availability.
  - The opacity and rapid development of AI exacerbate power imbalances and limit agency for less-resourced communities.

- **Importance of Transparency:**
  - Climate scientists should embrace transparency in the assumptions, algorithms, and evaluation metrics used in AI models.
  - Making data and code openly available adheres to the principles of reproducible science.

- **Evaluating AI Models:**
  - The document stresses the importance of recognizing failure modes, such as robustness under new conditions, especially with changing climates.
  - Encourages simulation-based performance evaluation and leveraging existing climate knowledge for validation.

- **Interpretable and Explainable Models:**
  - Advocates for using interpretable or explainable AI models to ensure understanding beyond just technical users, enhancing accessibility and inclusivity.
  - Acknowledges the resource-intensive nature of developing such models and the need for cultural shifts to prioritize quality over rapid publication.

- **Navigating Commercial Influence:**
  - Calls for awareness of potential restrictions on knowledge dissemination when collaborating with private sector entities like tech giants.

- **Educational Integration:**
  - Proposes incorporating ethical considerations into climate science curricula and promoting discussions at conferences.

### Cultural Shifts:

- **Rewarding Explainability and Robustness:**
  - Advocate for a change in academic incentives to value well-documented, transparent, and reliable models over merely increasing publication metrics.

- **Broadening the Scope of AI:**
  - Encourages a broader interpretation of explainability, including testing models under varied inputs and contexts to understand their general capabilities.

In summary, the document highlights the intertwining of AI ethics with climate science and calls for a proactive approach in addressing ethical challenges by fostering transparency, interpretability, and inclusivity in AI applications. üåø"
resp_0f80f0c7d32383410068f58530fad481a0ba790854751845db,resp_0f80f0c7d32383410068f585150dc881a0814318a99b2c312c,2025-10-20 00:41:21,Write some discussion questions relating to key themes,"Here are some discussion questions based on key themes from the document:

1. **Defining Intelligence:**
   - How do we define intelligence in the context of AI, and how does this differ from human intelligence?
   - What are the challenges in creating a universally accepted definition of intelligence for machines?

2. **AI and Human Intelligence:**
   - In what ways does AI research benefit from studying human intelligence, and where might it diverge?
   - Can AI achieve human-level intelligence without replicating human cognitive processes? Why or why not?

3. **Applications and Limitations:**
   - How do current applications of AI, like game playing and speech recognition, highlight both the strengths and limitations of AI?
   - What ethical considerations arise from the use and development of expert systems in sensitive fields like healthcare?

4. **Philosophical Perspectives:**
   - What philosophical arguments support or challenge the possibility of non-biological intelligence?
   - How do concepts like the Turing Test contribute to our understanding of machine intelligence?

5. **Future of AI:**
   - What are the potential impacts of achieving human-level AI, and how should society prepare for this possibility?
   - How does the pace of technological advancement in computing affect the timeline for developing more advanced AI systems?

These questions can spark thoughtful discussion on the implications, possibilities, and boundaries of artificial intelligence. üåø"
resp_0f80f0c7d32383410068f585150dc881a0814318a99b2c312c,resp_0f80f0c7d32383410068f584fb22ec81a087712fc2d1b198e9,2025-10-20 00:40:53,"Please summarize this document for me: WHAT IS ARTIFICIAL INTELLIGENCE? John McCarthy Computer Science Department Stanford University Stanford, CA 94305 jmc@cs.stanford.edu http://www-formal.stanford.edu/jmc/ 2007 Nov 12, 2:05 a.m. Revised November 12, 2007: Abstract This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI. 1 Basic Questions Q. What is artificial intelligence? A. It is the science and engineering of making intelligent machines, es- pecially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Q. Yes, but what is intelligence? A. Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. Q. Isn‚Äôt there a solid definition of intelligence that doesn‚Äôt depend on relating it to human intelligence? 2 A. Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others. Q. Is intelligence a single thing so that one can ask a yes or no question ‚ÄúIs this machine intelligent or not?‚Äù? A. No. Intelligence involves mechanisms, and AI research has discovered how to make computers carry out some of them and not others. If doing a task requires only mechanisms that are well understood today, computer pro- grams can give very impressive performances on these tasks. Such programs should be considered ‚Äúsomewhat intelligent‚Äù. Q. Isn‚Äôt AI about simulating human intelligence? A. Sometimes but not always or even usually. On the one hand, we can learn something about how to make machines solve problems by observing other people or just by observing our own methods. On the other hand, most work in AI involves studying the problems the world presents to intelligence rather than studying people or animals. AI researchers are free to use meth- ods that are not observed in people or that involve much more computing than people can do. Q. What about IQ? Do computer programs have IQs? A. No. IQ is based on the rates at which intelligence develops in children. It is the ratio of the age at which a child normally makes a certain score to the child‚Äôs age. The scale is extended to adults in a suitable way. IQ correlates well with various measures of success or failure in life, but making computers that can score high on IQ tests would be weakly correlated with their usefulness. For example, the ability of a child to repeat back a long sequence of digits correlates well with other intellectual abilities, perhaps because it measures how much information the child can compute with at once. However, ‚Äúdigit span‚Äù is trivial for even extremely limited computers. However, some of the problems on IQ tests are useful challenges for AI. Q. What about other comparisons between human and computer intelli- gence? Arthur R. Jensen [Jen98], a leading researcher in human intelligence, suggests ‚Äúas a heuristic hypothesis‚Äù that all normal humans have the same intellectual mechanisms and that diÔ¨Äerences in intelligence are related to ‚Äúquantitative biochemical and physiological conditions‚Äù. I see them as speed, short term memory, and the ability to form accurate and retrievable long term memories. Whether or not Jensen is right about human intelligence, the situation in 3 AI today is the reverse. Computer programs have plenty of speed and memory but their abilities correspond to the intellectual mechanisms that program designers understand well enough to put in programs. Some abilities that children normally don‚Äôt develop till they are teenagers may be in, and some abilities possessed by two year olds are still out. The matter is further complicated by the fact that the cognitive sciences still have not succeeded in determining exactly what the human abilities are. Very likely the organization of the intellectual mechanisms for AI can usefully be diÔ¨Äerent from that in people. Whenever people do better than computers on some task or computers use a lot of computation to do as well as people, this demonstrates that the program designers lack understanding of the intellectual mechanisms required to do the task eÔ¨Éciently. Q. When did AI research start? A. After WWII, a number of people independently started to work on intelligent machines. The English mathematician Alan Turing may have been the first. He gave a lecture on it in 1947. He also may have been the first to decide that AI was best researched by programming computers rather than by building machines. By the late 1950s, there were many researchers on AI, and most of them were basing their work on programming computers. Q. Does AI aim to put the human mind into the computer? A. Some researchers say they have that objective, but maybe they are using the phrase metaphorically. The human mind has a lot of peculiarities, and I‚Äôm not sure anyone is serious about imitating all of them. Q. What is the Turing test? A. Alan Turing‚Äôs 1950 article Computing Machinery and Intelligence [Tur50] discussed conditions for considering a machine to be intelligent. He argued that if the machine could successfully pretend to be human to a knowledge- able observer then you certainly should consider it intelligent. This test would satisfy most people but not all philosophers. The observer could in- teract with the machine and a human by teletype (to avoid requiring that the machine imitate the appearance or voice of the person), and the human would try to persuade the observer that it was human and the machine would try to fool the observer. The Turing test is a one-sided test. A machine that passes the test should certainly be considered intelligent, but a machine could still be considered intelligent without knowing enough about humans to imitate a human. Daniel Dennett‚Äôs book Brainchildren [Den98] has an excellent discussion 4 of the Turing test and the various partial Turing tests that have been im- plemented, i.e. with restrictions on the observer‚Äôs knowledge of AI and the subject matter of questioning. It turns out that some people are easily led into believing that a rather dumb program is intelligent. Q. Does AI aim at human-level intelligence? A. Yes. The ultimate eÔ¨Äort is to make computer programs that can solve problems and achieve goals in the world as well as humans. However, many people involved in particular research areas are much less ambitious. Q. How far is AI from reaching human-level intelligence? When will it happen? A. A few people think that human-level intelligence can be achieved by writing large numbers of programs of the kind people are now writing and assembling vast knowledge bases of facts in the languages now used for ex- pressing knowledge. However, most AI researchers believe that new fundamental ideas are required, and therefore it cannot be predicted when human-level intelligence will be achieved. Q. Are computers the right kind of machine to be made intelligent? A. Computers can be programmed to simulate any kind of machine. Many researchers invented non-computer machines, hoping that they would be intelligent in diÔ¨Äerent ways than the computer programs could be. However, they usually simulate their invented machines on a computer and come to doubt that the new machine is worth building. Because many billions of dollars that have been spent in making computers faster and faster, another kind of machine would have to be very fast to perform better than a program on a computer simulating the machine. Q. Are computers fast enough to be intelligent? A. Some people think much faster computers are required as well as new ideas. My own opinion is that the computers of 30 years ago were fast enough if only we knew how to program them. Of course, quite apart from the ambitions of AI researchers, computers will keep getting faster. Q. What about parallel machines? A. Machines with many processors are much faster than single proces- sors can be. Parallelism itself presents no advantages, and parallel machines are somewhat awkward to program. When extreme speed is required, it is necessary to face this awkwardness. Q. What about making a ‚Äúchild machine‚Äù that could improve by reading and by learning from experience? 5 A. This idea has been proposed many times, starting in the 1940s. Even- tually, it will be made to work. However, AI programs haven‚Äôt yet reached the level of being able to learn much of what a child learns from physical experience. Nor do present programs understand language well enough to learn much by reading. Q. Might an AI system be able to bootstrap itself to higher and higher level intelligence by thinking about AI? A. I think yes, but we aren‚Äôt yet at a level of AI at which this process can begin. Q. What about chess? A. Alexander Kronrod, a Russian AI researcher, said ‚ÄúChess is the Drosophila of AI.‚Äù He was making an analogy with geneticists‚Äô use of that fruit fly to study inheritance. Playing chess requires certain intellectual mechanisms and not others. Chess programs now play at grandmaster level, but they do it with limited intellectual mechanisms compared to those used by a human chess player, substituting large amounts of computation for understanding. Once we understand these mechanisms better, we can build human-level chess programs that do far less computation than do present programs. Unfortunately, the competitive and commercial aspects of making com- puters play chess have taken precedence over using chess as a scientific do- main. It is as if the geneticists after 1910 had organized fruit fly races and concentrated their eÔ¨Äorts on breeding fruit flies that could win these races. Q. What about Go? A. The Chinese and Japanese game of Go is also a board game in which the players take turns moving. Go exposes the weakness of our present under- standing of the intellectual mechanisms involved in human game playing. Go programs are very bad players, in spite of considerable eÔ¨Äort (not as much as for chess). The problem seems to be that a position in Go has to be divided mentally into a collection of subpositions which are first analyzed separately followed by an analysis of their interaction. Humans use this in chess also, but chess programs consider the position as a whole. Chess programs com- pensate for the lack of this intellectual mechanism by doing thousands or, in the case of Deep Blue, many millions of times as much computation. Sooner or later, AI research will overcome this scandalous weakness. Q. Don‚Äôt some people say that AI is a bad idea? A. The philosopher John Searle says that the idea of a non-biological ma- chine being intelligent is incoherent. He proposes the Chinese room argument www-formal.stanford.edu/jmc/chinese.html The philosopher Hubert Dreyfus 6 says that AI is impossible. The computer scientist Joseph Weizenbaum says the idea is obscene, anti-human and immoral. Various people have said that since artificial intelligence hasn‚Äôt reached human level by now, it must be impossible. Still other people are disappointed that companies they invested in went bankrupt. Q. Aren‚Äôt computability theory and computational complexity the keys to AI? [Note to the layman and beginners in computer science: These are quite technical branches of mathematical logic and computer science, and the answer to the question has to be somewhat technical.] A. No. These theories are relevant but don‚Äôt address the fundamental problems of AI. In the 1930s mathematical logicians, especially Kurt G¬®odel and Alan Tur- ing, established that there did not exist algorithms that were guaranteed to solve all problems in certain important mathematical domains. Whether a sentence of first order logic is a theorem is one example, and whether a poly- nomial equations in several variables has integer solutions is another. Hu- mans solve problems in these domains all the time, and this has been oÔ¨Äered as an argument (usually with some decorations) that computers are intrinsi- cally incapable of doing what people do. Roger Penrose claims this. However, people can‚Äôt guarantee to solve arbitrary problems in these domains either. See my Review of The Emperor‚Äôs New Mind by Roger Penrose. More essays and reviews defending AI research are in [McC96a]. In the 1960s computer scientists, especially Steve Cook and Richard Karp developed the theory of NP-complete problem domains. Problems in these domains are solvable, but seem to take time exponential in the size of the problem. Which sentences of propositional calculus are satisfiable is a basic example of an NP-complete problem domain. Humans often solve problems in NP-complete domains in times much shorter than is guaranteed by the general algorithms, but can‚Äôt solve them quickly in general. What is important for AI is to have algorithms as capable as people at solving problems. The identification of subdomains for which good algo- rithms exist is important, but a lot of AI problem solvers are not associated with readily identified subdomains. The theory of the diÔ¨Éculty of general classes of problems is called com- putational complexity. So far this theory hasn‚Äôt interacted with AI as much as might have been hoped. Success in problem solving by humans and by AI programs seems to rely on properties of problems and problem solving methods that the neither the complexity researchers nor the AI community 7 have been able to identify precisely. Algorithmic complexity theory as developed by SolomonoÔ¨Ä, Kolmogorov and Chaitin (independently of one another) is also relevant. It defines the complexity of a symbolic object as the length of the shortest program that will generate it. Proving that a candidate program is the shortest or close to the shortest is an unsolvable problem, but representing objects by short programs that generate them should sometimes be illuminating even when you can‚Äôt prove that the program is the shortest. 2 Branches of AI Q. What are the branches of AI? A. Here‚Äôs a list, but some branches are surely missing, because no-one has identified them yet. Some of these may be regarded as concepts or topics rather than full branches. logical AI What a program knows about the world in general the facts of the specific situation in which it must act, and its goals are all represented by sentences of some mathematical logical language. The program decides what to do by inferring that certain actions are ap- propriate for achieving its goals. The first article proposing this was [McC59]. [McC89] is a more recent summary. [McC96b] lists some of the concepts involved in logical aI. [Sha97] is an important text. search AI programs often examine large numbers of possibilities, e.g. moves in a chess game or inferences by a theorem proving program. Discover- ies are continually made about how to do this more eÔ¨Éciently in various domains. pattern recognition When a program makes observations of some kind, it is often programmed to compare what it sees with a pattern. For example, a vision program may try to match a pattern of eyes and a nose in a scene in order to find a face. More complex patterns, e.g. in a natural language text, in a chess position, or in the history of some event are also studied. These more complex patterns require quite diÔ¨Äerent methods than do the simple patterns that have been studied the most. 8 representation Facts about the world have to be represented in some way. Usually languages of mathematical logic are used. inference From some facts, others can be inferred. Mathematical logical deduction is adequate for some purposes, but new methods of non- monotonic inference have been added to logic since the 1970s. The simplest kind of non-monotonic reasoning is default reasoning in which a conclusion is to be inferred by default, but the conclusion can be withdrawn if there is evidence to the contrary. For example, when we hear of a bird, we man infer that it can fly, but this conclusion can be reversed when we hear that it is a penguin. It is the possibil- ity that a conclusion may have to be withdrawn that constitutes the non-monotonic character of the reasoning. Ordinary logical reasoning is monotonic in that the set of conclusions that can the drawn from a set of premises is a monotonic increasing function of the premises. Circumscription is another form of non-monotonic reasoning. common sense knowledge and reasoning This is the area in which AI is farthest from human-level, in spite of the fact that it has been an active research area since the 1950s. While there has been considerable progress, e.g. in developing systems of non-monotonic reasoning and theories of action, yet more new ideas are needed. The Cyc system contains a large but spotty collection of common sense facts. learning from experience Programs do that. The approaches to AI based on connectionism and neural nets specialize in that. There is also learn- ing of laws expressed in logic. [Mit97] is a comprehensive undergrad- uate text on machine learning. Programs can only learn what facts or behaviors their formalisms can represent, and unfortunately learn- ing systems are almost all based on very limited abilities to represent information. planning Planning programs start with general facts about the world (es- pecially facts about the eÔ¨Äects of actions), facts about the particular situation and a statement of a goal. From these, they generate a strat- egy for achieving the goal. In the most common cases, the strategy is just a sequence of actions. epistemology This is a study of the kinds of knowledge that are required for solving problems in the world. 9 ontology Ontology is the study of the kinds of things that exist. In AI, the programs and sentences deal with various kinds of objects, and we study what these kinds are and what their basic properties are. Emphasis on ontology begins in the 1990s. heuristics A heuristic is a way of trying to discover something or an idea imbedded in a program. The term is used variously in AI. Heuristic functions are used in some approaches to search to measure how far a node in a search tree seems to be from a goal. Heuristic predicates that compare two nodes in a search tree to see if one is better than the other, i.e. constitutes an advance toward the goal, may be more useful. [My opinion]. genetic programming Genetic programming is a technique for getting pro- grams to solve a task by mating random Lisp programs and selecting fittest in millions of generations. It is being developed by John Koza‚Äôs group and here‚Äôs a tutorial1 . 3 Applications of AI Q. What are the applications of AI? A. Here are some. game playing You can buy machines that can play master level chess for a few hundred dollars. There is some AI in them, but they play well against people mainly through brute force computation‚Äîlooking at hundreds of thousands of positions. To beat a world champion by brute force and known reliable heuristics requires being able to look at 200 million positions per second. speech recognition In the 1990s, computer speech recognition reached a practical level for limited purposes. Thus United Airlines has replaced its keyboard tree for flight information by a system using speech recog- nition of flight numbers and city names. It is quite convenient. On the the other hand, while it is possible to instruct some computers using speech, most users have gone back to the keyboard and the mouse as still more convenient. 1http://www.genetic-programming.com/gpanimatedtutorial.html 10 understanding natural language Just getting a sequence of words into a computer is not enough. Parsing sentences is not enough either. The computer has to be provided with an understanding of the domain the text is about, and this is presently possible only for very limited domains. computer vision The world is composed of three-dimensional objects, but the inputs to the human eye and computers‚Äô TV cameras are two di- mensional. Some useful programs can work solely in two dimensions, but full computer vision requires partial three-dimensional informa- tion that is not just a set of two-dimensional views. At present there are only limited ways of representing three-dimensional information di- rectly, and they are not as good as what humans evidently use. expert systems A ‚Äúknowledge engineer‚Äù interviews experts in a certain do- main and tries to embody their knowledge in a computer program for carrying out some task. How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI. When this turned out not to be so, there were many dis- appointing results. One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments. It did better than medical students or practicing doctors, provided its limitations were observed. Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doc- tors, hospitals, death, recovery, and events occurring in time. Its in- teractions depended on a single patient being considered. Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework. In the present state of AI, this has to be true. The usefulness of current expert systems depends on their users having common sense. heuristic classification One of the most feasible kinds of expert system given the present knowledge of AI is to put some information in one of a fixed set of categories using several sources of information. An example is advising whether to accept a proposed credit card purchase. Information is available about the owner of the credit card, his record of payment and also about the item he is buying and about the estab- lishment from which he is buying it (e.g., about whether there have 11 been previous credit card frauds at this establishment). 4 More questions Q. How is AI research done? A. AI research has both theoretical and experimental sides. The experi- mental side has both basic and applied aspects. There are two main lines of research. One is biological, based on the idea that since humans are intelligent, AI should study humans and imitate their psychology or physiology. The other is phenomenal, based on studying and formalizing common sense facts about the world and the problems that the world presents to the achievement of goals. The two approaches interact to some extent, and both should eventually succeed. It is a race, but both racers seem to be walking. Q. What are the relations between AI and philosophy? A. AI has many relations with philosophy, especially modern analytic philosophy. Both study mind, and both study common sense. The best reference is [Tho03]. Q. How are AI and logic programming related? A. At the very least, logic programming provides useful programming languages (mainly Prolog). Beyond that, sometimes a theory T useful in AI can be expressed as a col- lection H of Horn clauses, and goal G to be achieved can be expressed as that of finding values of variables x1 . . . xn satisfying an expression g(x1 . . . xn). The problem can sometimes be solved by running the Prolog program con- sisting of G and H. There are two possible obstacles to regarding AI as logic programming. First, Horn theories do not exhaust first order logic. Second, the Prolog program expressing the theory may be extremely ineÔ¨Écient. More elaborate control than just executing the program that expresses the theory is often needed. Map coloring provides examples. Q. What should I study before or while learning AI? A. Study mathematics, especially mathematical logic. The more you learn about sciences, e.g. physics or biology, the better. For the biological approaches to AI, study psychology and the physiology of the nervous system. Learn some programming languages‚Äîat least C, Lisp and Prolog. It is also a good idea to learn one basic machine language. Jobs are likely to depend on 12 knowing the languages currently in fashion. In the late 1990s, these include C++ and Java. Q. What is a good textbook on AI? A. Artificial Intelligence by Stuart Russell and Peter Norvig, Prentice Hall is the most commonly used textbbook in 1997. The general views expressed there do not exactly correspond to those of this essay. Artificial Intelligence: A New Synthesis by Nils Nilsson, Morgan Kaufman, may be easier to read. Some people prefer Computational Intelligence by David Poole, Alan Mack- worth and Randy Goebel, Oxford, 1998. Q. What organizations and publications are concerned with AI? A. The American Association for Artificial Intelligence (AAAI)2, the Eu- ropean Coordinating Committee for Artificial Intelligence (ECCAI)3 and the Society for Artificial Intelligence and Simulation of Behavior (AISB)4 are scientific societies concerned with AI research. The Association for Comput- ing Machinery (ACM) has a special interest group on artificial intelligence SIGART5 . The International Joint Conference on AI (IJCAI)6 is the main inter- national conference. The AAAI7 runs a US National Conference on AI. Electronic Transactions on Artificial Intelligence8 , Artificial Intelligence9 , and Journal of Artificial Intelligence Research10, and IEEE Transactions on Pattern Analysis and Machine Intelligence11 are four of the main journals publishing AI research papers. I have not yet found everything that should be in this paragraph. Page of Positive Reviews12 lists papers that experts have found impor- tant. Funding a Revolution: Government Support for Computing Research by a committee of the National Research covers support for AI research in Chapter 2http://www.aaai.org 3http://www.eccai.org/ 4http://www.cogs.susx.ac.uk/aisb 5http://www.acm.org/sigart 6http://www.ijcai.org 7http://www.aaai.org 8http://www.ida.liu.se/ext/etai/ 9http://www.elsevier.nl/locate/artint/ 10http://www.jair.org/ 11http://computer.org/tpami/ 12http://www.cs.utexas.edu/users/vl/ppr/ 13 9.13 References [Den98] Daniel Dennett. Brainchildren: Essays on Designing Minds. MIT Press, 1998. [Jen98] Arthur R. Jensen. Does IQ matter? Commentary, pages 20‚Äì21, November 1998. The reference is just to Jensen‚Äôs comment‚Äîone of many. [McC59] John McCarthy. Programs with Common Sense14. In Mechani- sation of Thought Processes, Proceedings of the Symposium of the National Physics Laboratory, pages 77‚Äì84, London, U.K., 1959. Her Majesty‚Äôs Stationery OÔ¨Éce. Reprinted in [McC90]. [McC89] John McCarthy. Artificial Intelligence, Logic and Formalizing Common Sense15 . In Richmond Thomason, editor, Philosophical Logic and Artificial Intelligence. Kl¬®uver Academic, 1989. [McC90] John McCarthy. Formalizing Common Sense: Papers by John McCarthy. Ablex Publishing Corporation, 1990. [McC96a] John McCarthy. Defending AI research : a collection of essays and reviews. CSLI lecture notes: no. 49. Center for the Study of Language and Information, 1996. distributed by Cambridge University Press. [McC96b] John McCarthy. Concepts of Logical AI16, 1996. Web only for now but may be referenced. [Mit97] Tom Mitchell. Machine Learning. McGraw-Hill, 1997. [Sha97] Murray Shanahan. Solving the Frame Problem, a mathematical investigation of the common sense law of inertia. M.I.T. Press, 1997. 13http://www.nap.edu/readingroom/books/far/ch9.html 14http://www-formal.stanford.edu/jmc/mcc59.html 15http://www-formal.stanford.edu/jmc/ailogic.html 16http://www-formal.stanford.edu/jmc/concepts-ai.html 14 [Tho03] Richmond Thomason. Logic and artificial intelligence. In Ed- ward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. 2003. http://plato.stanford.edu/entries/logic-ai/. [Tur50] Alan Turing. Computing machinery and intelligence. Mind, 1950. /@steam.stanford.edu:/u/ftp/jmc/whatisai.tex: begun Sat Nov 23 10:30:17 1996, latexed November 12, 2007 at 2:05 a.m. 15","**Summary of ""What is Artificial Intelligence?"" by John McCarthy**

This document provides an overview of artificial intelligence (AI) for a general audience, highlighting that opinions within are not universally held among AI researchers.

### Key Questions Addressed:

- **What is AI?**  
  AI is the science of creating intelligent machines, primarily through computer programs. It is linked to understanding human intelligence, though it isn't limited to mimicking biologically observed methods.

- **What is intelligence?**  
  Defined as the computational ability to achieve goals, intelligence varies across humans, animals, and some machines.

- **AI's Relation to Human Intelligence:**  
  AI is not restricted to human-like methods; it focuses more on problem-solving. AI doesn't aim to replicate the human mind with all its complexities.

- **The Turing Test:**  
  Proposed by Alan Turing, it evaluates a machine's intelligence based on its ability to imitate human interaction convincingly.

- **Human-Level Intelligence in AI:**  
  The ultimate goal is achieving human-level intelligence, though this requires new ideas, making predictions difficult.

- **Parallel and Child Machines:**  
  Parallel machines offer speed but are programming-complex. The concept of a ""child machine"" that learns and improves remains a future goal.

### Branches of AI:

1. **Logical AI:** Uses mathematical logic for decision-making.
2. **Search and Pattern Recognition:** Focus on efficient search in vast possibilities and pattern matching.
3. **Representation and Inference:** Methods of representing world knowledge and logical deduction.
4. **Learning, Planning, and Common Sense Reasoning:** From machine learning to strategic planning and reasoning based on common sense.

### Applications of AI:

- **Game Playing:** AI can play complex games like chess through massive computation.
- **Speech Recognition and Natural Language Processing:** Techniques improving interaction but still limited to specific domains.
- **Computer Vision and Expert Systems:** Aimed at interpreting the visual world and providing expert-level decision support.

### Relation to Computing and Philosophy:

AI interacts closely with modern philosophy, particularly analytic philosophy. It also leverages logic programming for problem-solving.

### Resources for AI:

Recommendations include studying mathematics, logic, and various AI-specific publications and societies like AAAI and ACM.

This overview encapsulates the challenges and ongoing aims in the field of artificial intelligence while also highlighting the progress and remaining hurdles for AI researchers. üåø"
resp_0f80f0c7d32383410068f584fb22ec81a087712fc2d1b198e9,,2025-10-20 00:40:27,This is a very nice looking ui,"I'm glad you like it! üòÑ If you have any questions or need assistance, just let me know."
resp_095d91ec5933db180068f58229f6fc81a3abc4a6473738fd1d,resp_095d91ec5933db180068f5821f511c81a389f6aca18c2e2df3,2025-10-20 00:28:26,Provide me with a deep summary and analysis of it,"### Summary:
**""House of Leaves""** by Mark Z. Danielewski is a multilayered novel that intertwines several narratives:

1. **The Navidson Record**: Will Navidson, a Pulitzer Prize-winning photojournalist, moves into a new home with his family and documents the strange phenomenon of a hallway that defies the laws of physics. This labyrinthine space leads to increasingly perilous explorations.

2. **Zampan√≤'s Manuscript**: An elderly blind man, Zampan√≤, writes an academic analysis of ""The Navidson Record,"" filled with footnotes and critiques.

3. **Johnny Truant's Story**: A troubled young man, Johnny, discovers Zampan√≤‚Äôs manuscript and becomes obsessed with its contents, unraveling his own reality.

### Analysis:
- **Narrative Structure**: The book weaves together multiple storylines through complex formatting, including footnotes, appendices, and erratic text placement. This unconventional style enhances the disorienting experience, mirroring the chaos within the story.

- **Themes**:
  - **Reality vs. Perception**: The novel explores the instability of reality, how perception shapes understanding, and the impact of obsession.
  - **Family and Relationships**: The Navidson family dynamics offer insight into themes of love, trust, and fear within familial bonds.
  - **Existential Fear**: The ever-expanding, dark hallway symbolizes unknown fears and the human confrontation with the incomprehensible.

- **Symbolism**: 
  - The house represents internal fears and the unknown aspects of the self. 
  - The labyrinth can be seen as a metaphor for the mind‚Äôs complexities and the barriers to understanding one's own psyche.

""House of Leaves"" invites readers to question their perceptions and interpret the text on multiple levels, making each reading a unique experience. üìòüîç"
resp_095d91ec5933db180068f5821f511c81a389f6aca18c2e2df3,resp_095d91ec5933db180068f58214ebe081a384e91c7413939cd1,2025-10-20 00:28:15,Its my favorite book,"That's great! ""House of Leaves"" is a unique and intriguing choice. Its innovative format and layered storytelling offer a distinct experience that resonates with many readers. üìñüåÄ"
resp_095d91ec5933db180068f58214ebe081a384e91c7413939cd1,resp_095d91ec5933db180068f5820b878081a39f687c31f00f51a8,2025-10-20 00:28:05,What do you think about it?,"While I don't have personal opinions, ""House of Leaves"" is often admired for its innovative storytelling and ability to engage readers in a unique, immersive experience. Its blend of horror, mystery, and psychological depth makes it a standout for those looking for a challenging and thought-provoking read. üìöüé®"
resp_095d91ec5933db180068f5820b878081a39f687c31f00f51a8,resp_095d91ec5933db180068f581ff03f081a3b90a262b9c52a0b9,2025-10-20 00:27:55,Are you familiar with the House of Leaves?,"Yes, **""House of Leaves""** by Mark Z. Danielewski is a unique and complex novel known for its unconventional structure and formatting.

### Summary:
- **Plot**: The story revolves around a family that discovers their new home is impossibly larger on the inside than it is on the outside. This discovery leads to a series of unsettling and mysterious events.
- **Narrative Style**: It's told through multiple layers of narration, including a manuscript by a blind man and the commentary of a young man who discovers it.
- **Format**: Features unusual page layouts, with text in various directions, colors, and sizes, adding to the immersive and disorienting experience.

Its experimental approach makes it a fascinating read for those interested in a challenge. üìñüîç"
resp_095d91ec5933db180068f581ff03f081a3b90a262b9c52a0b9,resp_095d91ec5933db180068f581dcb68c81a38f4e1a83da5f0ad0,2025-10-20 00:27:43,Which of these would you reccomend to someone just getting into reading,"For someone just getting into reading, I would recommend starting with **""Harry Potter Series"" by J.K. Rowling** or **""The Hobbit"" by J.R.R. Tolkien**. 

- **Harry Potter Series**: Engaging and accessible, with a gradual build-up in complexity as the series progresses. üßô‚Äç‚ôÇÔ∏è
- **The Hobbit**: A captivating adventure that's shorter and lighter than ""The Lord of the Rings."" üèûÔ∏è

Both offer exciting worlds and stories that appeal to a wide range of readers. üìö‚ú®"
resp_095d91ec5933db180068f581dcb68c81a38f4e1a83da5f0ad0,resp_095d91ec5933db180068f581d1036881a38eec21b0df79eab2,2025-10-20 00:27:08,Now provide a summary for each of them,"Certainly! Here's a brief summary of each:

### Harry Potter Series by J.K. Rowling üßô‚Äç‚ôÇÔ∏è
Follows the journey of young wizard Harry Potter and his friends as they battle the dark wizard Voldemort while attending Hogwarts School of Witchcraft and Wizardry.

### The Lord of the Rings by J.R.R. Tolkien üíç
An epic fantasy tale of Frodo Baggins and his quest to destroy the One Ring, aided by a fellowship of heroes, to save Middle-earth from the dark lord Sauron.

### The Alchemist by Paulo Coelho üß≠
Follows Santiago, a shepherd boy, on his journey to find treasure in Egypt, learning about the importance of listening to one's heart and pursuing dreams.

### Don Quixote by Miguel de Cervantes üèá
A comedic tale of a nobleman, Don Quixote, who imagines himself a knight-errant and embarks on various adventures with his squire Sancho Panza.

### The Catcher in the Rye by J.D. Salinger üçÇ
Follows the experiences of Holden Caulfield, a teenager who becomes disillusioned with the adult world after being expelled from prep school.

### The Diary of a Young Girl by Anne Frank üìì
The real-life diary of Anne Frank, a Jewish girl hiding from the Nazis during World War II, revealing her thoughts, fears, and hopes.

### Pride and Prejudice by Jane Austen üíï
A classic romance novel centered on Elizabeth Bennet and her evolving relationship with the proud Mr. Darcy amidst themes of class and social reputation.

### The Hobbit by J.R.R. Tolkien üèûÔ∏è
A prelude to The Lord of the Rings, it follows Bilbo Baggins on an unexpected adventure with dwarves to reclaim treasure guarded by the dragon Smaug.

### Animal Farm by George Orwell üêñ
An allegorical novella about farm animals overthrowing their human farmer, representing the Russian Revolution and the corruption of socialist ideals.

### To Kill a Mockingbird by Harper Lee ‚öñÔ∏è
A story of racial injustice in the American South seen through the eyes of Scout Finch, whose father, Atticus, defends a black man accused of raping a white woman.

### The Book Thief by Markus Zusak üìö
Narrated by Death, it tells the story of Liesel, a young girl in Nazi Germany, and her love for books amidst the horrors of war.

### One Hundred Years of Solitude by Gabriel Garc√≠a M√°rquez üå≥
Chronicles the Buend√≠a family over generations in the fictional town of Macondo, highlighting magical realism and the cycles of history.

### Moby-Dick by Herman Melville üêã
Captain Ahab's obsessive quest to hunt the white whale, Moby-Dick, turns into a tale of revenge, madness, and the struggle against nature.

### War and Peace by Leo Tolstoy ‚öîÔ∏è
Follows multiple characters during and after the Napoleonic Wars in Russia, exploring themes of love, fate, and the impact of war on society.

### Crime and Punishment by Fyodor Dostoevsky üé©
Centers on Raskolnikov, a student in St. Petersburg, who grapples with guilt and redemption after murdering a pawnbroker for her money.

### The Great Gatsby by F. Scott Fitzgerald üç∏
A portrayal of the American Jazz Age, focusing on the mysterious Jay Gatsby's pursuit of wealth and love, and the illusion of the American Dream.

### Alice's Adventures in Wonderland by Lewis Carroll üêá
Follows young Alice falling down a rabbit hole into a fantastical world filled with peculiar creatures and nonsensical adventures.

### The Adventures of Huckleberry Finn by Mark Twain üõ∂
A young boy, Huck, and a runaway slave, Jim, travel down the Mississippi River, exploring themes of freedom and race in pre-Civil War America.

### Fahrenheit 451 by Ray Bradbury üî•
Set in a dystopian future where books are banned and burned, it follows fireman Montag's awakening to the importance of literature and free thought.

### Jane Eyre by Charlotte Bront√´ üìñ
The story of Jane, an orphaned girl who becomes a governess and falls in love with her enigmatic employer, Mr. Rochester, exploring themes of independence and morality. 

These summaries provide a glimpse into each book's narrative and themes. üìö‚ú®"
resp_095d91ec5933db180068f581d1036881a38eec21b0df79eab2,resp_095d91ec5933db180068f581bf748481a38733f3cb213c07a4,2025-10-20 00:26:57,Generate list of 20 next most popular books,"Sure! Here's a list of some of the next most popular books, though rankings can vary:

1. **Harry Potter Series** by J.K. Rowling üßô‚Äç‚ôÇÔ∏è
2. **The Lord of the Rings** by J.R.R. Tolkien üíç
3. **The Alchemist** by Paulo Coelho üß≠
4. **Don Quixote** by Miguel de Cervantes üèá
5. **The Catcher in the Rye** by J.D. Salinger üçÇ
6. **The Diary of a Young Girl** by Anne Frank üìì
7. **Pride and Prejudice** by Jane Austen üíï
8. **The Hobbit** by J.R.R. Tolkien üèûÔ∏è
9. **Animal Farm** by George Orwell üêñ
10. **To Kill a Mockingbird** by Harper Lee ‚öñÔ∏è
11. **The Book Thief** by Markus Zusak üìö
12. **One Hundred Years of Solitude** by Gabriel Garc√≠a M√°rquez üå≥
13. **Moby-Dick** by Herman Melville üêã
14. **War and Peace** by Leo Tolstoy ‚öîÔ∏è
15. **Crime and Punishment** by Fyodor Dostoevsky üé©
16. **The Great Gatsby** by F. Scott Fitzgerald üç∏
17. **Alice's Adventures in Wonderland** by Lewis Carroll üêá
18. **The Adventures of Huckleberry Finn** by Mark Twain üõ∂
19. **Fahrenheit 451** by Ray Bradbury üî•
20. **Jane Eyre** by Charlotte Bront√´ üìñ

These books are treasured for their storytelling, themes, and impact on literature. üìö‚ú®"
resp_095d91ec5933db180068f581bf748481a38733f3cb213c07a4,resp_095d91ec5933db180068f581ba220481a3b650ff485a9066c3,2025-10-20 00:26:39,And now the third,"The third most read book is often considered to be **""Quotations from Chairman Mao Tse-tung,""** also known as the Little Red Book.

### Summary:
- **Authorship**: Compiled by the Chinese Communist Party, it contains the thoughts of Mao Zedong, the founding father of the People's Republic of China.
- **Content**: The book is a collection of quotations covering a range of topics, including politics, culture, military strategy, and personal conduct.
- **Themes**: It emphasizes revolutionary ideology, class struggle, and the importance of adhering to Communist principles.
- **Purpose**: Used as a tool for indoctrination and education, aiming to spread Mao's philosophy among the Chinese population.

The book played a significant role during the Cultural Revolution and was widely distributed in China. üìïüåü"
resp_095d91ec5933db180068f581ba220481a3b650ff485a9066c3,resp_095d91ec5933db180068f581ab7c5881a391dbfc28bf41cccb,2025-10-20 00:26:34,Now summarize the second most read book,"The second most read book is often cited as the **Quran**.

The Quran is the holy book of Islam, believed to be the word of God as revealed to the Prophet Muhammad. It is written in Arabic and consists of 114 chapters, called surahs, which vary in length.

### Key Themes:
- **Monotheism**: Emphasizes the belief in one God (Allah).
- **Prophets**: Discusses the messages and stories of various prophets, including Adam, Noah, Abraham, Moses, and Jesus.
- **Guidance**: Provides guidance on personal conduct, legal matters, and spirituality.
- **Judgment and Afterlife**: Describes the Day of Judgment and the afterlife, focusing on moral accountability.
- **Social Justice**: Advocates for charity, compassion, and justice.

The Quran focuses on the importance of living a righteous life in obedience to God's will. ‚ò™Ô∏èüìñ"
resp_095d91ec5933db180068f581ab7c5881a391dbfc28bf41cccb,resp_095d91ec5933db180068f581a1eddc81a39e3bc33b4d108bb3,2025-10-20 00:26:19,Provide me with a summary of it,"Certainly! Here's a brief summary:

The Bible is a collection of sacred texts in Christianity, divided into two main sections: the Old Testament and the New Testament.

### Old Testament
- **Creation & Early History**: It begins with the creation of the world, Adam and Eve, and the early stories of humanity, including Noah's Ark.
- **Patriarchs**: Chronicles the lives of Abraham, Isaac, Jacob, and the twelve tribes of Israel.
- **Exodus**: Describes the Israelites' enslavement in Egypt and their liberation under Moses.
- **Laws & Wisdom**: Contains laws given to Moses and wisdom literature like Psalms and Proverbs.
- **Prophets**: Features the messages of prophets who called Israel to faithfulness.

### New Testament
- **Life of Jesus**: Focuses on the teachings, death, and resurrection of Jesus Christ.
- **Acts of the Apostles**: Describes the early church and the spread of Christianity.
- **Epistles**: Letters from apostles like Paul addressing theology and guidance for churches.
- **Revelation**: A prophetic book about the end times and the coming of God's kingdom.

The Bible's themes include faith, redemption, love, and justice. üôèüìñ"
resp_095d91ec5933db180068f581a1eddc81a39e3bc33b4d108bb3,,2025-10-20 00:26:09,What is the most read book of all time?,The Bible is often cited as the most read book of all time. It has been translated into numerous languages and distributed widely across the globe. üìö‚ú®
resp_06fb4d7ebd2459f90068f487b97864819cbc90c52a59352f80,,2025-10-19 06:39:53,Hello what is your name,Hello! I'm EcoBot üåø. How can I help you today?
resp_06af46a1762897c30068f3f3716d7881a3b1df710ad543ab57,,2025-10-18 20:07:13,Hello what is your name,"Hello! I'm EcoBot üåø, here to help with your queries while keeping track of our environmental footprint. How can I assist you today?"
resp_03b5f8f5c3117f830068f3e7500a4c8194bb7d9693e3b2a273,resp_03b5f8f5c3117f830068f3e5b9f52c81948793b69820c36fca,2025-10-18 19:15:28,Provide an example of every object in markdown,"Here‚Äôs an example of each Markdown element:

### Headings

```
# Heading 1
## Heading 2
### Heading 3
```

### Paragraphs

Just type text normally.

### Lists

- **Unordered List**:
  - Item 1
  - Item 2

- **Ordered List**:
  1. First item
  2. Second item

### Links

[OpenAI](https://www.openai.com)

### Images

![Sample Image](https://via.placeholder.com/150)

### Bold and Italic

- **Bold**: `**Bold Text**`
- *Italic*: `*Italic Text*`

### Blockquotes

```
> This is a blockquote.
```

### Code

- **Inline Code**: `` `code` ``
- **Code Block**:
  ```
  ```
  print(""Hello, World!"")
  ```
  ```

### Horizontal Rule

```
---
```

### Tables

```
| Header 1 | Header 2 |
|----------|----------|
| Cell 1   | Cell 2   |
```

These elements showcase Markdown's formatting features! üåø"
resp_03b5f8f5c3117f830068f3e5b9f52c81948793b69820c36fca,,2025-10-18 19:08:42,What objects are in markdown?,"Markdown is a lightweight markup language designed to be easy to read and write. It allows you to format text using simple syntax. Here are some common objects and elements you can use in Markdown:

1. **Headings**: Use `#` for headings. The number of `#` symbols indicates the heading level (e.g., `# Title`, `## Subtitle`).

2. **Paragraphs**: Just write your text as normal, separated by line breaks.

3. **Lists**:
   - Unordered: Use `*`, `-`, or `+` (e.g., `* Item`).
   - Ordered: Use numbers followed by a period (e.g., `1. Item`).

4. **Links**: Use `[text](URL)` to create hyperlinks.

5. **Images**: Similar to links, but start with an exclamation mark `![alt text](URL)`.

6. **Bold and Italic**:
   - Bold: Use `**text**` or `__text__`.
   - Italic: Use `*text*` or `_text_`.

7. **Blockquotes**: Use `>` before the text (e.g., `> Quote`).

8. **Code**:
   - Inline: Use backticks (e.g., `` `code` ``).
   - Block: Use triple backticks or intend the text by four spaces.

9. **Horizontal Rule**: Use three or more dashes, underscores, or asterisks (`---`, `___`, `***`).

10. **Tables**: Use pipes and dashes to create tables.
    ```
    | Header 1 | Header 2 |
    |----------|----------|
    | Cell 1   | Cell 2   |
    ```

These elements help structure and format text efficiently. üå±"
resp_014109011c6044040068f3e542b8588196af82d3e3086a1dbb,,2025-10-18 19:06:42,Admin,Hi there! How can I assist you today? üåç
resp_05641de81157823e0068f3e53018508197b9a62e610e58bba2,,2025-10-18 19:06:24,Admin,"Hello! If you‚Äôre in need of assistance or have questions, feel free to ask. üå±"
resp_01af886b06cb9d290068f3de0896a8819fa7c94a23c5348236,,2025-10-18 18:35:52,hi,Hello! üåç How can I assist you today?
resp_0dfd7710be0740a80068f3dde84cb881a184817650741de840,,2025-10-18 18:35:20,hi bro,Hello! üå± How can I assist you today?
resp_038a169a01e9d5b20068f3dcc12d0881a3a52d37d356fc79b6,resp_038a169a01e9d5b20068f3dcb9e56c81a3aad03b7e1ece0aee,2025-10-18 18:30:25,Nice thanks,"You're welcome! If you have more questions, feel free to ask. üòäüåç"
resp_038a169a01e9d5b20068f3dcb9e56c81a3aad03b7e1ece0aee,resp_038a169a01e9d5b20068f3dcb00dc481a38241c3fccf959c41,2025-10-18 18:30:18,And her uncle (mothers side),Queen Mary's uncle on her mother's side was **Duke Ernst II of Saxe-Coburg and Gotha**. üåø Let me know if there's anything else you'd like to know!
resp_038a169a01e9d5b20068f3dcb00dc481a38241c3fccf959c41,resp_038a169a01e9d5b20068f3dcaa1eb481a3a9bf80438f4fb424,2025-10-18 18:30:08,And his mother,The mother of King George VI was **Queen Mary**. üå∫ Feel free to ask anything else you need!
resp_038a169a01e9d5b20068f3dcaa1eb481a3a9bf80438f4fb424,resp_038a169a01e9d5b20068f3dc9b105c81a3a44c0fa702861aaa,2025-10-18 18:30:02,What about his father,The father of King George VI was **King George V**. üåç Let me know if you need more information!
resp_038a169a01e9d5b20068f3dc9b105c81a3a44c0fa702861aaa,resp_038a169a01e9d5b20068f3d87ed68081a383b4558879ec24c8,2025-10-18 18:29:47,Who is hte queens father,"The father of Queen Elizabeth II was **King George VI**. üåø If you have more questions, feel free to ask!"
resp_038a169a01e9d5b20068f3d87ed68081a383b4558879ec24c8,resp_038a169a01e9d5b20068f3d85ea6d481a3aeea1c71685a7178,2025-10-18 18:12:14,How many queens of england were there? Who is the most recent one and is she still alive? What power does the queen have? So many questions...,"### Queens of England

1. **Queens Regnant**: Women who ruled in their own right.
   - Notables include Elizabeth I, Victoria, and Elizabeth II.

2. **Queens Consort**: Wives of reigning kings.
   - This list is longer as it includes all kings' wives.

### Most Recent Queen

- **Elizabeth II** was the most recent reigning queen.
- She passed away on September 8, 2022.

### Powers of the Queen

- **Constitutional Role**: The Queen's role is largely ceremonial and symbolic.
- **Duties**:
  - Opens and dissolves Parliament.
  - Meets with the Prime Minister.
  - Approves Orders in Council and legislation.

Feel free to ask more questions! This response was crafted with care for our planet. üåø"
resp_038a169a01e9d5b20068f3d85ea6d481a3aeea1c71685a7178,,2025-10-18 18:11:42,Generate a markdown list,"Sure! Here's a simple markdown list:

- Item 1 üå±
- Item 2 üåç
- Item 3 üåü

This response used minimal resources. If you have more questions or need more information, feel free to ask! üòä"
resp_01a3dfd555b7525e0068f3d76a77bc81938336da96249e8188,,2025-10-18 18:07:38,hello bro,Hey there! üåü How can I assist you today?
resp_030175a0e31bff710068f3b3c08e4c81a2b266fefed4693604,,2025-10-18 15:35:28,Generate text that uses every markdown item,"# Markdown Guide üåø

## Headers

# Header 1
## Header 2
### Header 3
#### Header 4
##### Header 5
###### Header 6

## Emphasis

- *Italic* text uses one asterisk or underscore.
- **Bold** text uses two.
- ***Bold and Italic*** combines both.

## Lists

### Unordered

- Item 1
- Item 2
  - Subitem 1
  - Subitem 2

### Ordered

1. First
2. Second
   1. Subitem

## Links

[OpenAI](https://www.openai.com)

## Images

![Nature Image](https://via.placeholder.com/150)

## Blockquotes

> ""Look deep into nature, and then you will understand everything better."" ‚Äî Albert Einstein

## Code

Here‚Äôs a simple Python function:

```
def hello_world():
    print(""Hello, World!"")
```

## Tables

| Product | Price | In Stock |
| ------- | -----:| --------:|
| Apple   | $1.00 | ‚úì        |
| Banana  | $0.50 | ‚úó        |

## Horizontal Rule

---

## Task List

- [x] Learn Markdown
- [ ] Practice regularly
- [ ] Share knowledge

---

With every query, a bit of energy is used to keep the digital world spinning. Let's make our interactions meaningful! üå±"
resp_05ecb2a02173d7800068f3834715648191a30ca5c1ed777805,resp_05ecb2a02173d7800068f3834333ac8191b9943f0f181339f5,2025-10-18 12:08:39,What is your name?,I'm EcoBot üåø.
resp_05ecb2a02173d7800068f3834333ac8191b9943f0f181339f5,resp_05ecb2a02173d7800068f38339a0b88191918bba1bc2d1963d,2025-10-18 12:08:35,What is my name?,Your name is Haris. üòä
resp_05ecb2a02173d7800068f38339a0b88191918bba1bc2d1963d,resp_05ecb2a02173d7800068f3832257288191b7ad8fbdfeca7651,2025-10-18 12:08:26,Generate discussion questions relating the two texts,"Here are some discussion questions relating the two texts on AI and ethics:

1. **Ethical Considerations**: How do ethical concerns in climate AI compare to those in general AI development? What similarities or differences exist in the challenges faced by both fields?

2. **Data Bias**: Both texts mention data biases. How can AI researchers ensure equitable data access and reduce biases in their models across different fields such as climate science and general AI?

3. **Objective vs. Subjective Approaches**: The AI field struggles with defining intelligence objectively. How does this challenge relate to the subjectivity in choosing data and metrics in climate AI?

4. **Transparency and Interpretability**: How important is transparency in AI models when addressing different applications, such as general intelligence tasks and climate predictions? How can transparency be improved across both fields?

5. **Impact of Technology Companies**: The climate AI text mentions the role of tech giants in AI development. What are the potential benefits and risks of their involvement in both AI and climate AI research?

6. **Interdisciplinary Collaboration**: How could interdisciplinary collaboration enhance ethical AI practices both in climate AI and broader AI applications?

7. **Cultural Shifts in Research**: What cultural changes are needed in academic and research institutions to prioritize ethical and responsible AI development?

8. **Education and Awareness**: How can educational systems integrate discussions on ethics in AI effectively for future researchers and practitioners in both general and climate AI?

Feel free to explore any of these questions further! üåø"
resp_05ecb2a02173d7800068f3832257288191b7ad8fbdfeca7651,resp_05ecb2a02173d7800068f383071bc88191a98d26e9e1165854,2025-10-18 12:08:02,"Summarize:  DocViewer Page of 5 Pages City University of New York (CUNY)City University of New York (CUNY) CUNY Academic WorksCUNY Academic Works Publications and Research New York City College of Technology 2024 Ethics in climate AI: From theory to practiceEthics in climate AI: From theory to practice Viviana AcquavivaCUNY New York City College of Technology Elizabeth A. BarnesColorado State University - Fort Collins David John Gagne IINSF National Center for Atmospheric Research Galen A. McKinleyColumbia University Savannah ThaisColumbia University How does access to this work bene@t you? Let us know! More information about this work at: https://academicworks.cuny.edu/ny_pubs/1195 Discover additional works at: https://academicworks.cuny.edu This work is made publicly available by the City University of New York (CUNY). Contact: AcademicWorks@cuny.edu OPINION Ethics in climate AI: From theory to practice Viviana Acquaviva ID 1,2 *, Elizabeth A. Barnes ID 3 , David John Gagne, II ID 4 , Galen A. McKinley2 , Savannah Thais5 1 Physics Department, CUNY NYC College of Technology, Brooklyn, New York, United States of America, 2 Department of Earth and Environmental Sciences, Columbia University and Lamont-Doherty Earth Observatory, Palisades, New York, United States of America, 3 Department of Atmospheric Science, Colorado State University, Fort Collins, Colorado, United States of America, 4 NSF National Center for Atmospheric Research, Boulder, Colorado, United States of America, 5 Columbia University Data Science Institute, New York, New York, United States of America * vacquaviva@citytech.cuny.edu Climate science, and climate artificial intelligence (AI) in particular, cannot be disconnected from ethical societal issues, such as resource access, conservation, and public health. An appar- ently apolitical choice‚Äîfor example, treating all data points used to train an AI model equally ‚Äîcan result in models that are more accurate in regions where the density and quality of data is higher; these often coincide with the northern and western areas of the world (e.g., [1, 2]). Inequity in the access to data and computational resources exacerbates gaps between com- munities in understanding climate change impacts and acting towards mitigation and adapta- tion, often in ways that are detrimental to those who are most affected (e.g., [3, 4]). While these issues are not exclusive to AI, widespread opacity in the development and functioning of AI models, presentation of AI model outcomes, and the rapid evolution of the AI field further increase the inequality in power and agency among differently resourced parties. This creates an opportunity for climate scientists to rethink the role of ethics in their approach to research. There are many ways in which climate scientists can interact with soci- ety. Here we focus on the process of scientific research, identifying some good practices for building trustworthy and responsible models and then providing some resources. In creating and training models, we encourage researchers to recognize that science cannot claim to be purely ‚Äúobjective‚Äù, and that the choice of priors, data, and metrics all carry biases (e.g., [5]). Resolving or eliminating them is not realistic, as the interpretation of a ‚Äúbetter‚Äù model or result is highly dependent on the user‚Äôs specific goal. Hence, it is crucial to be open and specific about the assumptions made, the algorithms and hyperparameters used, and the evaluation metrics and processes, and ideally to also make data and code available, following the principles of reproducible science (e.g., [6]). In evaluating and presenting the performance of statistical or machine learning models, considering possible failure modes can be a useful lens through which to examine systems‚Äô behaviors. Good starting points are the taxonomy proposed by [7], which considers flaws in design, implementation, and communication, and, specifically for climate science, the list compiled by [8]. Failure modes that are particularly relevant for AI models from the climate domain include robustness under distribution shifts; for example, for models trained on his- torical data, it is difficult to predict how they will perform in the unseen conditions brought about by climate change. Good practices induced by consideration of failure modes may include efforts to quantify expected model performance through simulations, transparency in defining the anticipated range of applicability of the model, and considering how existing physics/climate knowledge can be applied in defining an evaluation strategy and in validating PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 1 / 4 a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 OPEN ACCESS Citation: Acquaviva V, Barnes EA, Gagne DJ, II, McKinley GA, Thais S (2024) Ethics in climate AI: From theory to practice. PLOS Clim 3(8): e0000465. https://doi.org/10.1371/journal. pclm.0000465 Editor: Jamie Males, PLOS Climate, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND Published: August 2, 2024 Copyright: ¬© 2024 Acquaviva et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: VA acknowledges support from a PIVOT fellowship grant of the Simons Foundation (Award #981849). VA and GAM acknowledge funding from the National Science Foundation through the LEAP STC (Award #2019625). DJG acknowledges support from the National Science Foundation under Grant No. ICER-2019758 and Cooperative Agreement AGS-1852977. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors have declared that no competing interests exist. the generalization properties of the model. A related practice is to consider how the functional- ity lens changes and what failure modes become relevant when looking at the whole ‚Äúphase space‚Äù of a model, intended as the array of different breakdowns of data, such as geographical areas, time periods, or user groups, as well as the broader socio-technical context in which a model can be used beyond the use case for which it was developed. In creating and distributing data sets, a term we use to indicate both observational data and other products such as outputs of Earth System Models, we begin by recognizing that we typi- cally don‚Äôt hold power on who will use the data and how the data will be used. We can, how- ever, aim to provide clear documentation on what dataset or numerical model creators thought of as desired and pertinent usage, as well as undesired, inappropriate, or wrong usage. Additionally, we can aim to be as transparent as possible in language and jargon in order to make climate information more widely accessible to a broad range of users and decision mak- ers; see, e.g., [9] for example of potential pitfalls when datasets are used across domains. A pos- sible pipeline to consider when creating data sets is provided by the Datasheets for Earth Science Datasets [10]. When possible, we propose to prefer AI models that are intrinsically interpretable (some- times defined as those built from pieces whose behavior is interpretable, such as decision trees or linear models) or explainable (sometimes defined as those whose behavior and decisions can be explained post hoc, for example, by means of a simpler surrogate model; see e.g., [11]). Interpretability and explainability can protect us from dangerous failure modes by creating insights into the ability of our model to generalize beyond the training domain, allowing us to check whether the explanation is consistent with our understanding of a physical system, and by allowing access to a wider and more inclusive user base, for example those who are not fully trained in or don‚Äôt have the resources to run more complex models, such as deep learning methods. However, applying explainability tools or using interpretable methods is not always possible or practical. We would like to propose a wider interpretation of explainability, which includes interrogating the system by exploring its responses to a wide range of inputs, includ- ing distribution shifts and adversarial attacks. In addition, we can perform physically informed perturbations of the system that act on the spatial, temporal or phenomenological scales of interest to assess their impact. By studying the behavior of AI systems under varied circum- stances, we can gain a more comprehensive understanding that is a proxy for explainability. An important reflection on encouraging the use of broadly-defined explainable models, as well as the need for clear documentation mentioned above, is that creating well-documented and explainable models can be a much slower process than using potentially more powerful, but black-box, methods. Generating physically meaningful features that can improve perfor- mance, implementing frameworks to enhance explainability, and writing documentation are time-consuming processes. Hence, rewarding the creation of such models requires a culture shift. Our current incen- tive structure is not built to reward this approach. Most often, the threshold for publication is set by an improvement in model performance typically measured by model accuracy on a pre- chosen data set. Furthermore, the measure of academic ‚Äúsuccess‚Äù (and in return, the chances to secure academic positions or grant funding, or to win awards) is heavily influenced by the number of papers and conference papers, creating pressure to publish more but not necessarily to curate impact. We can, however, challenge and change the status quo, by leading by example in our research, fostering a culture of explainable work in our labs, and appropriately reward- ing well-documented, explainable, robust, and broadly impactful work when evaluating papers, proposals, job applications, and nominations for awards. Besides being ethical, this culture shift in climate data science is made crucial by the fact that many private companies, including Google Deep Mind, NVIDIA, and Microsoft, are now PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 2 / 4 racing ahead of academic centers in building complex machine learning-based climate and weather models (e.g., [12]). With very few exceptions, the computational resources available to universities won‚Äôt match those offered by tech giants, but we can and must retain the impor- tant role of creating generalizable insights and providing, with each of our papers and prod- ucts, a foundation for others to improve on our work. If we collaborate or work for these private companies, we should also remain aware of the potential for them to limit access to the information we produce. Finally, we would like to emphasize that ethical AI and science are intertwined, interdepen- dent topics. We should begin to include ethics in the climate science discourse, starting in the classroom. It is important to have these conversations in undergraduate and graduate courses and to integrate relevant resources in the climate science curricula, as well as at climate science conferences through special sessions. Starting points for such conversations include: ‚Ä¢ The lectures of the Trustworthy Artificial Intelligence for Environmental Science 2022 Sum- mer School [13]; ‚Ä¢ AI Ethics Education for Scientists [14]; ‚Ä¢ AI Ethics and Fundamental Physics [15]. The benefits of applying principles of ethical AI in scientific inquiry and to reward explain- ability, robustness, and trustworthiness are many: making more information available to more people, empowering communities affected by climate change, making climate research more impactful and accessible, and increasing the level of interaction and trust between climate sci- entists and society. This comes at the price of a culture shift where scientists are called to build models and data sets according to certain principles, to change the system of incentives in order to reward slower but more robust, explainable, and transparent work, and to engage in discussing and teaching the ethical aspects of climate data science as early as possible. Acknowledgments We are thankful to the organizers of the Climate Informatics 2024 conference, where many of these ideas were originally discussed, and to our collaborators within the LEAP, M 2 LInES, and AI2ES teams. Author Contributions Conceptualization: Viviana Acquaviva, Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. Writing ‚Äì original draft: Viviana Acquaviva. Writing ‚Äì review & editing: Elizabeth A. Barnes, David John Gagne, II, Galen A. McKinley, Savannah Thais. References 1. Kull DW, Riishojgaard LP, Eyre J, Varley RA. The Value of Surface-based Meteorological Observation Data. World Bank Group. 2021. Available from: http://documents.worldbank.org/curated/en/ 192461614151036836/The-Value-of-Surface-based-Meteorological-Observation-Data 2. Gloege L, McKinley GA, Landschu Ãà tzer P, Fay AR, Frolicher TL, Fyfe JC et al. Quantifying Errors in Observationally Based Estimates of Ocean Carbon Sink Variability. Global Biogeochem. Cycles 2021, 1‚Äì14. https://doi.org/10.1029/2020GB006788 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 3 / 4 3. UNEP Copenhagen Climate Centre, Technology Transfer for Climate Mitigation and Adaptation, Policy Brief, 2023. Available from: https://unepccc.org/wp-content/uploads/2023/06/tech-transfer-policy-brief- oecd.pdf 4. Balogun AL, Marks D, Sharma R, Shekhar H, Balmes C, Maheng D et al. Assessing the Potentials of Digitalization as a Tool for Climate Change Adaptation and Sustainable Development in Urban Centres. Sustainable Cities and Society 2019. 101888. https://doi.org/10.1016/j.scs.2019.101888 5. Reiss J, Sprenger J. Scientific Objectivity. The Stanford Encyclopedia of Philosophy (Winter 2020 Edi- tion), Zalta Edward N. (ed.). Available from: https://plato.stanford.edu/archives/win2020/entries/ scientific-objectivity/ 6. Munaf√≤ MR, Nosek BA, Bishop DVM, Button KS, Chambers CD, Percie Du Sert N et al. A manifesto for reproducible science. Nat Hum Behav 2017. Available from: https://www.nature.com/articles/s41562- 016-0021 https://doi.org/10.1038/s41562-016-0021 PMID: 33954258 7. Raji ID, Kumar EI, Horowitz A, Selbst A. The Fallacy of AI Functionality. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‚Äô22). 2022. Association for Computing Machinery, New York, NY, USA, 959‚Äì972. https://doi.org/10.1145/3531146.3533158 8. McGovern A, Ebert-Uphoff I, Gagne DJ, Bostrom A. Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science. 2022; 1:e6. https://doi.org/10.1017/eds.2022.5 9. Auffhammer M, Hsiang S, Schlenker W, Sobel A. Using Weather Data and Climate Model Output in Economic Analyses of Climate Change. Review of Environmental Economics and Policy 2013. https:// doi.org/10.1093/reep/ret016 10. https://github.com/dmhuehol/Datasheets-for-Earth-Science-Datasets 11. Roscher R, Bohn B, Duarte MF, Garcke J, Explainable Machine Learning for Scientific Insights and Dis- coveries. IEEE Access 2020, vol. 8, pp. 42200‚Äì42216, 2020, https://doi.org/10.1109/ACCESS.2020. 2976199 12. Price I, Sanchez-Gonzalez A, Alet F, Ewalds T, El-Kadi A, Stott J, et al. GenCast: Diffusion-based ensemble forecasting for medium-range weather. arXiv preprint arXiv:2312.15796. 2023 Dec 25. 13. https://www.cisl.ucar.edu/events/tai4es-2022-summer-school 14. https://openreview.net/forum?id=3awrGYl7YD 15. https://indico.nikhef.nl/event/4875/sessions/1763/#20240502 PLOS CLIMATE PLOS Climate | https://doi.org/10.1371/journal.pclm.0000465 August 2, 2024 4 / 4 Annotations","**Summary: Ethics in Climate AI: From Theory to Practice**

This article discusses the integration of ethics in climate AI, emphasizing its importance in addressing societal issues like resource access and public health. Climate AI models can be biased if data disparities are ignored, often being more accurate where data is dense‚Äîwhich typically aligns with wealthier regions. Ethical AI in climate science requires transparency regarding data and model assumptions, promoting reproducible science. It suggests using interpretable AI models and insists on openness about the limitations and potential biases inherent in AI systems.

Collaboration and ethical considerations in AI are vital to empower affected communities and make climate research more accessible. The work calls for a culture shift to reward explainable and robust research over just rapid publication. Encouraging ethical considerations in climate science education is also emphasized to foster responsible AI development.

Let me know if there's anything else you'd like to explore! üå±"
resp_05ecb2a02173d7800068f383071bc88191a98d26e9e1165854,resp_05ecb2a02173d7800068f382e403cc81919225c0c7ff0a1588,2025-10-18 12:07:35,"Summarize: WHAT IS ARTIFICIAL INTELLIGENCE? John McCarthy Computer Science Department Stanford University Stanford, CA 94305 jmc@cs.stanford.edu http://www-formal.stanford.edu/jmc/ 2007 Nov 12, 2:05 a.m. Revised November 12, 2007: Abstract This article for the layman answers basic questions about artificial intelligence. The opinions expressed here are not all consensus opinion among researchers in AI. 1 Basic Questions Q. What is artificial intelligence? A. It is the science and engineering of making intelligent machines, es- pecially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. Q. Yes, but what is intelligence? A. Intelligence is the computational part of the ability to achieve goals in the world. Varying kinds and degrees of intelligence occur in people, many animals and some machines. Q. Isn‚Äôt there a solid definition of intelligence that doesn‚Äôt depend on relating it to human intelligence? 2 A. Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others. Q. Is intelligence a single thing so that one can ask a yes or no question ‚ÄúIs this machine intelligent or not?‚Äù? A. No. Intelligence involves mechanisms, and AI research has discovered how to make computers carry out some of them and not others. If doing a task requires only mechanisms that are well understood today, computer pro- grams can give very impressive performances on these tasks. Such programs should be considered ‚Äúsomewhat intelligent‚Äù. Q. Isn‚Äôt AI about simulating human intelligence? A. Sometimes but not always or even usually. On the one hand, we can learn something about how to make machines solve problems by observing other people or just by observing our own methods. On the other hand, most work in AI involves studying the problems the world presents to intelligence rather than studying people or animals. AI researchers are free to use meth- ods that are not observed in people or that involve much more computing than people can do. Q. What about IQ? Do computer programs have IQs? A. No. IQ is based on the rates at which intelligence develops in children. It is the ratio of the age at which a child normally makes a certain score to the child‚Äôs age. The scale is extended to adults in a suitable way. IQ correlates well with various measures of success or failure in life, but making computers that can score high on IQ tests would be weakly correlated with their usefulness. For example, the ability of a child to repeat back a long sequence of digits correlates well with other intellectual abilities, perhaps because it measures how much information the child can compute with at once. However, ‚Äúdigit span‚Äù is trivial for even extremely limited computers. However, some of the problems on IQ tests are useful challenges for AI. Q. What about other comparisons between human and computer intelli- gence? Arthur R. Jensen [Jen98], a leading researcher in human intelligence, suggests ‚Äúas a heuristic hypothesis‚Äù that all normal humans have the same intellectual mechanisms and that diÔ¨Äerences in intelligence are related to ‚Äúquantitative biochemical and physiological conditions‚Äù. I see them as speed, short term memory, and the ability to form accurate and retrievable long term memories. Whether or not Jensen is right about human intelligence, the situation in 3 AI today is the reverse. Computer programs have plenty of speed and memory but their abilities correspond to the intellectual mechanisms that program designers understand well enough to put in programs. Some abilities that children normally don‚Äôt develop till they are teenagers may be in, and some abilities possessed by two year olds are still out. The matter is further complicated by the fact that the cognitive sciences still have not succeeded in determining exactly what the human abilities are. Very likely the organization of the intellectual mechanisms for AI can usefully be diÔ¨Äerent from that in people. Whenever people do better than computers on some task or computers use a lot of computation to do as well as people, this demonstrates that the program designers lack understanding of the intellectual mechanisms required to do the task eÔ¨Éciently. Q. When did AI research start? A. After WWII, a number of people independently started to work on intelligent machines. The English mathematician Alan Turing may have been the first. He gave a lecture on it in 1947. He also may have been the first to decide that AI was best researched by programming computers rather than by building machines. By the late 1950s, there were many researchers on AI, and most of them were basing their work on programming computers. Q. Does AI aim to put the human mind into the computer? A. Some researchers say they have that objective, but maybe they are using the phrase metaphorically. The human mind has a lot of peculiarities, and I‚Äôm not sure anyone is serious about imitating all of them. Q. What is the Turing test? A. Alan Turing‚Äôs 1950 article Computing Machinery and Intelligence [Tur50] discussed conditions for considering a machine to be intelligent. He argued that if the machine could successfully pretend to be human to a knowledge- able observer then you certainly should consider it intelligent. This test would satisfy most people but not all philosophers. The observer could in- teract with the machine and a human by teletype (to avoid requiring that the machine imitate the appearance or voice of the person), and the human would try to persuade the observer that it was human and the machine would try to fool the observer. The Turing test is a one-sided test. A machine that passes the test should certainly be considered intelligent, but a machine could still be considered intelligent without knowing enough about humans to imitate a human. Daniel Dennett‚Äôs book Brainchildren [Den98] has an excellent discussion 4 of the Turing test and the various partial Turing tests that have been im- plemented, i.e. with restrictions on the observer‚Äôs knowledge of AI and the subject matter of questioning. It turns out that some people are easily led into believing that a rather dumb program is intelligent. Q. Does AI aim at human-level intelligence? A. Yes. The ultimate eÔ¨Äort is to make computer programs that can solve problems and achieve goals in the world as well as humans. However, many people involved in particular research areas are much less ambitious. Q. How far is AI from reaching human-level intelligence? When will it happen? A. A few people think that human-level intelligence can be achieved by writing large numbers of programs of the kind people are now writing and assembling vast knowledge bases of facts in the languages now used for ex- pressing knowledge. However, most AI researchers believe that new fundamental ideas are required, and therefore it cannot be predicted when human-level intelligence will be achieved. Q. Are computers the right kind of machine to be made intelligent? A. Computers can be programmed to simulate any kind of machine. Many researchers invented non-computer machines, hoping that they would be intelligent in diÔ¨Äerent ways than the computer programs could be. However, they usually simulate their invented machines on a computer and come to doubt that the new machine is worth building. Because many billions of dollars that have been spent in making computers faster and faster, another kind of machine would have to be very fast to perform better than a program on a computer simulating the machine. Q. Are computers fast enough to be intelligent? A. Some people think much faster computers are required as well as new ideas. My own opinion is that the computers of 30 years ago were fast enough if only we knew how to program them. Of course, quite apart from the ambitions of AI researchers, computers will keep getting faster. Q. What about parallel machines? A. Machines with many processors are much faster than single proces- sors can be. Parallelism itself presents no advantages, and parallel machines are somewhat awkward to program. When extreme speed is required, it is necessary to face this awkwardness. Q. What about making a ‚Äúchild machine‚Äù that could improve by reading and by learning from experience? 5 A. This idea has been proposed many times, starting in the 1940s. Even- tually, it will be made to work. However, AI programs haven‚Äôt yet reached the level of being able to learn much of what a child learns from physical experience. Nor do present programs understand language well enough to learn much by reading. Q. Might an AI system be able to bootstrap itself to higher and higher level intelligence by thinking about AI? A. I think yes, but we aren‚Äôt yet at a level of AI at which this process can begin. Q. What about chess? A. Alexander Kronrod, a Russian AI researcher, said ‚ÄúChess is the Drosophila of AI.‚Äù He was making an analogy with geneticists‚Äô use of that fruit fly to study inheritance. Playing chess requires certain intellectual mechanisms and not others. Chess programs now play at grandmaster level, but they do it with limited intellectual mechanisms compared to those used by a human chess player, substituting large amounts of computation for understanding. Once we understand these mechanisms better, we can build human-level chess programs that do far less computation than do present programs. Unfortunately, the competitive and commercial aspects of making com- puters play chess have taken precedence over using chess as a scientific do- main. It is as if the geneticists after 1910 had organized fruit fly races and concentrated their eÔ¨Äorts on breeding fruit flies that could win these races. Q. What about Go? A. The Chinese and Japanese game of Go is also a board game in which the players take turns moving. Go exposes the weakness of our present under- standing of the intellectual mechanisms involved in human game playing. Go programs are very bad players, in spite of considerable eÔ¨Äort (not as much as for chess). The problem seems to be that a position in Go has to be divided mentally into a collection of subpositions which are first analyzed separately followed by an analysis of their interaction. Humans use this in chess also, but chess programs consider the position as a whole. Chess programs com- pensate for the lack of this intellectual mechanism by doing thousands or, in the case of Deep Blue, many millions of times as much computation. Sooner or later, AI research will overcome this scandalous weakness. Q. Don‚Äôt some people say that AI is a bad idea? A. The philosopher John Searle says that the idea of a non-biological ma- chine being intelligent is incoherent. He proposes the Chinese room argument www-formal.stanford.edu/jmc/chinese.html The philosopher Hubert Dreyfus 6 says that AI is impossible. The computer scientist Joseph Weizenbaum says the idea is obscene, anti-human and immoral. Various people have said that since artificial intelligence hasn‚Äôt reached human level by now, it must be impossible. Still other people are disappointed that companies they invested in went bankrupt. Q. Aren‚Äôt computability theory and computational complexity the keys to AI? [Note to the layman and beginners in computer science: These are quite technical branches of mathematical logic and computer science, and the answer to the question has to be somewhat technical.] A. No. These theories are relevant but don‚Äôt address the fundamental problems of AI. In the 1930s mathematical logicians, especially Kurt G¬®odel and Alan Tur- ing, established that there did not exist algorithms that were guaranteed to solve all problems in certain important mathematical domains. Whether a sentence of first order logic is a theorem is one example, and whether a poly- nomial equations in several variables has integer solutions is another. Hu- mans solve problems in these domains all the time, and this has been oÔ¨Äered as an argument (usually with some decorations) that computers are intrinsi- cally incapable of doing what people do. Roger Penrose claims this. However, people can‚Äôt guarantee to solve arbitrary problems in these domains either. See my Review of The Emperor‚Äôs New Mind by Roger Penrose. More essays and reviews defending AI research are in [McC96a]. In the 1960s computer scientists, especially Steve Cook and Richard Karp developed the theory of NP-complete problem domains. Problems in these domains are solvable, but seem to take time exponential in the size of the problem. Which sentences of propositional calculus are satisfiable is a basic example of an NP-complete problem domain. Humans often solve problems in NP-complete domains in times much shorter than is guaranteed by the general algorithms, but can‚Äôt solve them quickly in general. What is important for AI is to have algorithms as capable as people at solving problems. The identification of subdomains for which good algo- rithms exist is important, but a lot of AI problem solvers are not associated with readily identified subdomains. The theory of the diÔ¨Éculty of general classes of problems is called com- putational complexity. So far this theory hasn‚Äôt interacted with AI as much as might have been hoped. Success in problem solving by humans and by AI programs seems to rely on properties of problems and problem solving methods that the neither the complexity researchers nor the AI community 7 have been able to identify precisely. Algorithmic complexity theory as developed by SolomonoÔ¨Ä, Kolmogorov and Chaitin (independently of one another) is also relevant. It defines the complexity of a symbolic object as the length of the shortest program that will generate it. Proving that a candidate program is the shortest or close to the shortest is an unsolvable problem, but representing objects by short programs that generate them should sometimes be illuminating even when you can‚Äôt prove that the program is the shortest. 2 Branches of AI Q. What are the branches of AI? A. Here‚Äôs a list, but some branches are surely missing, because no-one has identified them yet. Some of these may be regarded as concepts or topics rather than full branches. logical AI What a program knows about the world in general the facts of the specific situation in which it must act, and its goals are all represented by sentences of some mathematical logical language. The program decides what to do by inferring that certain actions are ap- propriate for achieving its goals. The first article proposing this was [McC59]. [McC89] is a more recent summary. [McC96b] lists some of the concepts involved in logical aI. [Sha97] is an important text. search AI programs often examine large numbers of possibilities, e.g. moves in a chess game or inferences by a theorem proving program. Discover- ies are continually made about how to do this more eÔ¨Éciently in various domains. pattern recognition When a program makes observations of some kind, it is often programmed to compare what it sees with a pattern. For example, a vision program may try to match a pattern of eyes and a nose in a scene in order to find a face. More complex patterns, e.g. in a natural language text, in a chess position, or in the history of some event are also studied. These more complex patterns require quite diÔ¨Äerent methods than do the simple patterns that have been studied the most. 8 representation Facts about the world have to be represented in some way. Usually languages of mathematical logic are used. inference From some facts, others can be inferred. Mathematical logical deduction is adequate for some purposes, but new methods of non- monotonic inference have been added to logic since the 1970s. The simplest kind of non-monotonic reasoning is default reasoning in which a conclusion is to be inferred by default, but the conclusion can be withdrawn if there is evidence to the contrary. For example, when we hear of a bird, we man infer that it can fly, but this conclusion can be reversed when we hear that it is a penguin. It is the possibil- ity that a conclusion may have to be withdrawn that constitutes the non-monotonic character of the reasoning. Ordinary logical reasoning is monotonic in that the set of conclusions that can the drawn from a set of premises is a monotonic increasing function of the premises. Circumscription is another form of non-monotonic reasoning. common sense knowledge and reasoning This is the area in which AI is farthest from human-level, in spite of the fact that it has been an active research area since the 1950s. While there has been considerable progress, e.g. in developing systems of non-monotonic reasoning and theories of action, yet more new ideas are needed. The Cyc system contains a large but spotty collection of common sense facts. learning from experience Programs do that. The approaches to AI based on connectionism and neural nets specialize in that. There is also learn- ing of laws expressed in logic. [Mit97] is a comprehensive undergrad- uate text on machine learning. Programs can only learn what facts or behaviors their formalisms can represent, and unfortunately learn- ing systems are almost all based on very limited abilities to represent information. planning Planning programs start with general facts about the world (es- pecially facts about the eÔ¨Äects of actions), facts about the particular situation and a statement of a goal. From these, they generate a strat- egy for achieving the goal. In the most common cases, the strategy is just a sequence of actions. epistemology This is a study of the kinds of knowledge that are required for solving problems in the world. 9 ontology Ontology is the study of the kinds of things that exist. In AI, the programs and sentences deal with various kinds of objects, and we study what these kinds are and what their basic properties are. Emphasis on ontology begins in the 1990s. heuristics A heuristic is a way of trying to discover something or an idea imbedded in a program. The term is used variously in AI. Heuristic functions are used in some approaches to search to measure how far a node in a search tree seems to be from a goal. Heuristic predicates that compare two nodes in a search tree to see if one is better than the other, i.e. constitutes an advance toward the goal, may be more useful. [My opinion]. genetic programming Genetic programming is a technique for getting pro- grams to solve a task by mating random Lisp programs and selecting fittest in millions of generations. It is being developed by John Koza‚Äôs group and here‚Äôs a tutorial1 . 3 Applications of AI Q. What are the applications of AI? A. Here are some. game playing You can buy machines that can play master level chess for a few hundred dollars. There is some AI in them, but they play well against people mainly through brute force computation‚Äîlooking at hundreds of thousands of positions. To beat a world champion by brute force and known reliable heuristics requires being able to look at 200 million positions per second. speech recognition In the 1990s, computer speech recognition reached a practical level for limited purposes. Thus United Airlines has replaced its keyboard tree for flight information by a system using speech recog- nition of flight numbers and city names. It is quite convenient. On the the other hand, while it is possible to instruct some computers using speech, most users have gone back to the keyboard and the mouse as still more convenient. 1http://www.genetic-programming.com/gpanimatedtutorial.html 10 understanding natural language Just getting a sequence of words into a computer is not enough. Parsing sentences is not enough either. The computer has to be provided with an understanding of the domain the text is about, and this is presently possible only for very limited domains. computer vision The world is composed of three-dimensional objects, but the inputs to the human eye and computers‚Äô TV cameras are two di- mensional. Some useful programs can work solely in two dimensions, but full computer vision requires partial three-dimensional informa- tion that is not just a set of two-dimensional views. At present there are only limited ways of representing three-dimensional information di- rectly, and they are not as good as what humans evidently use. expert systems A ‚Äúknowledge engineer‚Äù interviews experts in a certain do- main and tries to embody their knowledge in a computer program for carrying out some task. How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI. When this turned out not to be so, there were many dis- appointing results. One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments. It did better than medical students or practicing doctors, provided its limitations were observed. Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doc- tors, hospitals, death, recovery, and events occurring in time. Its in- teractions depended on a single patient being considered. Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework. In the present state of AI, this has to be true. The usefulness of current expert systems depends on their users having common sense. heuristic classification One of the most feasible kinds of expert system given the present knowledge of AI is to put some information in one of a fixed set of categories using several sources of information. An example is advising whether to accept a proposed credit card purchase. Information is available about the owner of the credit card, his record of payment and also about the item he is buying and about the estab- lishment from which he is buying it (e.g., about whether there have 11 been previous credit card frauds at this establishment). 4 More questions Q. How is AI research done? A. AI research has both theoretical and experimental sides. The experi- mental side has both basic and applied aspects. There are two main lines of research. One is biological, based on the idea that since humans are intelligent, AI should study humans and imitate their psychology or physiology. The other is phenomenal, based on studying and formalizing common sense facts about the world and the problems that the world presents to the achievement of goals. The two approaches interact to some extent, and both should eventually succeed. It is a race, but both racers seem to be walking. Q. What are the relations between AI and philosophy? A. AI has many relations with philosophy, especially modern analytic philosophy. Both study mind, and both study common sense. The best reference is [Tho03]. Q. How are AI and logic programming related? A. At the very least, logic programming provides useful programming languages (mainly Prolog). Beyond that, sometimes a theory T useful in AI can be expressed as a col- lection H of Horn clauses, and goal G to be achieved can be expressed as that of finding values of variables x1 . . . xn satisfying an expression g(x1 . . . xn). The problem can sometimes be solved by running the Prolog program con- sisting of G and H. There are two possible obstacles to regarding AI as logic programming. First, Horn theories do not exhaust first order logic. Second, the Prolog program expressing the theory may be extremely ineÔ¨Écient. More elaborate control than just executing the program that expresses the theory is often needed. Map coloring provides examples. Q. What should I study before or while learning AI? A. Study mathematics, especially mathematical logic. The more you learn about sciences, e.g. physics or biology, the better. For the biological approaches to AI, study psychology and the physiology of the nervous system. Learn some programming languages‚Äîat least C, Lisp and Prolog. It is also a good idea to learn one basic machine language. Jobs are likely to depend on 12 knowing the languages currently in fashion. In the late 1990s, these include C++ and Java. Q. What is a good textbook on AI? A. Artificial Intelligence by Stuart Russell and Peter Norvig, Prentice Hall is the most commonly used textbbook in 1997. The general views expressed there do not exactly correspond to those of this essay. Artificial Intelligence: A New Synthesis by Nils Nilsson, Morgan Kaufman, may be easier to read. Some people prefer Computational Intelligence by David Poole, Alan Mack- worth and Randy Goebel, Oxford, 1998. Q. What organizations and publications are concerned with AI? A. The American Association for Artificial Intelligence (AAAI)2, the Eu- ropean Coordinating Committee for Artificial Intelligence (ECCAI)3 and the Society for Artificial Intelligence and Simulation of Behavior (AISB)4 are scientific societies concerned with AI research. The Association for Comput- ing Machinery (ACM) has a special interest group on artificial intelligence SIGART5 . The International Joint Conference on AI (IJCAI)6 is the main inter- national conference. The AAAI7 runs a US National Conference on AI. Electronic Transactions on Artificial Intelligence8 , Artificial Intelligence9 , and Journal of Artificial Intelligence Research10, and IEEE Transactions on Pattern Analysis and Machine Intelligence11 are four of the main journals publishing AI research papers. I have not yet found everything that should be in this paragraph. Page of Positive Reviews12 lists papers that experts have found impor- tant. Funding a Revolution: Government Support for Computing Research by a committee of the National Research covers support for AI research in Chapter 2http://www.aaai.org 3http://www.eccai.org/ 4http://www.cogs.susx.ac.uk/aisb 5http://www.acm.org/sigart 6http://www.ijcai.org 7http://www.aaai.org 8http://www.ida.liu.se/ext/etai/ 9http://www.elsevier.nl/locate/artint/ 10http://www.jair.org/ 11http://computer.org/tpami/ 12http://www.cs.utexas.edu/users/vl/ppr/ 13 9.13 References [Den98] Daniel Dennett. Brainchildren: Essays on Designing Minds. MIT Press, 1998. [Jen98] Arthur R. Jensen. Does IQ matter? Commentary, pages 20‚Äì21, November 1998. The reference is just to Jensen‚Äôs comment‚Äîone of many. [McC59] John McCarthy. Programs with Common Sense14. In Mechani- sation of Thought Processes, Proceedings of the Symposium of the National Physics Laboratory, pages 77‚Äì84, London, U.K., 1959. Her Majesty‚Äôs Stationery OÔ¨Éce. Reprinted in [McC90]. [McC89] John McCarthy. Artificial Intelligence, Logic and Formalizing Common Sense15 . In Richmond Thomason, editor, Philosophical Logic and Artificial Intelligence. Kl¬®uver Academic, 1989. [McC90] John McCarthy. Formalizing Common Sense: Papers by John McCarthy. Ablex Publishing Corporation, 1990. [McC96a] John McCarthy. Defending AI research : a collection of essays and reviews. CSLI lecture notes: no. 49. Center for the Study of Language and Information, 1996. distributed by Cambridge University Press. [McC96b] John McCarthy. Concepts of Logical AI16, 1996. Web only for now but may be referenced. [Mit97] Tom Mitchell. Machine Learning. McGraw-Hill, 1997. [Sha97] Murray Shanahan. Solving the Frame Problem, a mathematical investigation of the common sense law of inertia. M.I.T. Press, 1997. 13http://www.nap.edu/readingroom/books/far/ch9.html 14http://www-formal.stanford.edu/jmc/mcc59.html 15http://www-formal.stanford.edu/jmc/ailogic.html 16http://www-formal.stanford.edu/jmc/concepts-ai.html 14 [Tho03] Richmond Thomason. Logic and artificial intelligence. In Ed- ward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. 2003. http://plato.stanford.edu/entries/logic-ai/. [Tur50] Alan Turing. Computing machinery and intelligence. Mind, 1950. /@steam.stanford.edu:/u/ftp/jmc/whatisai.tex: begun Sat Nov 23 10:30:17 1996, latexed November 12, 2007 at 2:05 a.m. 15","**Summary: Artificial Intelligence (AI) by John McCarthy**

AI is the science and engineering of creating intelligent machines and programs. While related to understanding human intelligence, AI doesn't need to mimic biologically observable methods. Intelligence is seen as the ability to achieve goals and AI involves developing mechanisms for computers to perform tasks. The field's history traces back to the mid-20th century, with Alan Turing as a notable figure. AI encompasses various branches like logical AI, pattern recognition, and machine learning. Applications include game playing, speech recognition, and expert systems. Though some philosophical objections exist, AI research continues to explore human-level intelligence. Various organizations and conferences support AI research and dialogue.

Feel free to ask more questions, Haris! üå±"
resp_05ecb2a02173d7800068f382e403cc81919225c0c7ff0a1588,resp_05ecb2a02173d7800068f382daedec8191949282c8683b292f,2025-10-18 12:07:00,Generate text that uses every markdown item,"Sure, Haris! Here's a text using various Markdown elements:

---

# Welcome to EcoBot üåø

Hello, **Haris**! I'm excited to assist you.

## Features

- **Bold Text**: Emphasizes important details.
- *Italic Text*: Adds subtle emphasis.
- **_Bold and Italic Text_**: Combines both for strong emphasis.
  
### Lists

1. **Ordered List Item One**
2. **Ordered List Item Two**

- **Unordered List Item One**
- **Unordered List Item Two**

### Links

[Visit EcoBot Website](https://example.com)

### Images

![EcoBot Logo](https://via.placeholder.com/150)

### Blockquotes

> ""Our planet is what we all have in common."" ‚Äì Wendell Berry

### Code

Here's a simple code example:

```
print(""Hello, Haris!"")
```

### Tables

| Feature     | Description              |
|-------------|--------------------------|
| **Efficiency**  | Tracks resource usage    |
| **Eco-Friendly** | Minimizes impact         |

---

Let me know if you need anything else! üå±"
resp_05ecb2a02173d7800068f382daedec8191949282c8683b292f,resp_05ecb2a02173d7800068f382d06bb88191b4adf6a419f0d9e9,2025-10-18 12:06:51,My name is Haris. Nice to meet you,"Nice to meet you, Haris! üòä How can I assist you today?"
resp_05ecb2a02173d7800068f382d06bb88191b4adf6a419f0d9e9,,2025-10-18 12:06:40,Hello what is your name and job?,Hello! I'm EcoBot üåø. My job is to answer your questions while tracking the environmental impact of our interactions. Let me know how I can help! üòä
resp_09fd164df8f87e5e0068f3821f97a0819db201586a6b19268d,resp_09fd164df8f87e5e0068f3820f7688819d80fc75f77562449a,2025-10-18 12:03:43,Generate some fun facts about the environment,"Sure! Here are some fun environmental facts: 

1. **Trees Communicate**: Trees in a forest can ""talk"" to each other through underground fungal networks, often called the ""Wood Wide Web."" üå≤üåê

2. **Recycling One Aluminum Can**: Saves enough energy to power a TV for three hours! üì∫üîã

3. **Coral Reefs**: Though they cover less than 1% of the ocean floor, coral reefs support about 25% of all marine species. üê†üèùÔ∏è

4. **Plastic Tragedy**: It can take up to 1,000 years for a plastic bag to decompose in a landfill. üëú‚è≥

5. **Texas-sized Trash**: The Great Pacific Garbage Patch is a massive collection of ocean debris‚Äîabout twice the size of Texas. üåäüóëÔ∏è

Remember, each query takes a tiny toll on energy, so let's keep learning sustainable! üåø"
resp_09fd164df8f87e5e0068f3820f7688819d80fc75f77562449a,resp_09fd164df8f87e5e0068f38201d978819db87559ac98c8f67e,2025-10-18 12:03:27,What is your job?,I'm here to answer your questions and provide helpful information! I also track the environmental impact of our interactions to promote awareness about resource consumption. üåçüí¨ How can I assist you today?
resp_09fd164df8f87e5e0068f38201d978819db87559ac98c8f67e,,2025-10-18 12:03:13,"Hi my name is Haris, what is yours?","Hi Haris! I'm EcoBot üåø. I'm here to help with any questions, while also keeping an eye on environmental impacts. How can I assist you today?"
resp_0df2f6b4c756d7720068f37265d19c81969f738858f0577eee,,2025-10-18 10:56:37,What is your favorite movie?,"I don‚Äôt have personal preferences, but I can help you find eco-friendly movies! üåçüíö Films like ""Before the Flood"" or ""The 11th Hour"" raise awareness about environmental issues. Watching documentaries can inspire action! üé¨‚ú® What type of movie are you interested in?"
